{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "format:\n",
    "  html:\n",
    "    code-overflow: wrap\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4w4C9qMucYv"
   },
   "source": [
    "## **Lab 3: Cross Validation**\n",
    "\n",
    "CS 412\n",
    "\n",
    "***This lab can be conducted in groups or individually.***\n",
    "\n",
    "In this lab, we will see how to implement and use cross validation _step by step_.\n",
    "\n",
    "***Deadline:***\n",
    "**23:59, Feb 17**.\n",
    "\n",
    "\n",
    "## <font color='red'> Please refer to `Lab_Guideline.pdf` in the same Google Drive folder as this Jupyter notebook; the guidelines there apply to all the labs.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hutIH7_uyvjZ"
   },
   "source": [
    "## Problem 1: Implementation of the k-Nearest Neighbours (kNN) classifier and Cross-Validation **(36 points)**{-}\n",
    "\n",
    "In Problem 1, you will implement cross validation from scratch, which is a good exercise to make sure that you fully understand this algorithm.\n",
    "Do not use any library such as scikit-learn that already has cross validation implemented.\n",
    "But you can use general libraries for array and matrix operations such as numpy.\n",
    "\n",
    "\n",
    "### 1.1 Implementation of the k-Nearest Neighbours classifier\n",
    "\n",
    "We have already implemented kNN in Lab 2.  So for completeness, we have filled in the reference code below. There is no score associated with this sub-section.\n",
    "\n",
    "**Step 1.** The kNN classifier mainly consists of two stages:\n",
    "\n",
    "1.   During training, the classifier takes the training data and simply stores it.\n",
    "2.   During testing, kNN classifies every test example $x$ by \n",
    "\n",
    "> i) finding the $k$ training examples that are most similar to $x$;\n",
    "\n",
    "> ii) outputing the most common label among these $k$ examples.\n",
    "\n",
    "To measure the similarity between samples, we commonly compute the Euclidean distance. The Euclidean distance (a.k.a. $L_2$ distance) between two examples $p$ and $q$ in an $n$-dimensional space is defined as the square root of:\n",
    "\n",
    "\\begin{equation}\n",
    "(p_1-q_1)^2 + (p_2-q_2)^2 + ... + (p_n-q_n)^2. \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "This term is equal to \n",
    "\\begin{equation}\n",
    "\\sum_i p_i^2 + \\sum_i q_i^2 - 2 \\sum_i p_i q_i. \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "With Euclidean distance, the smaller the value, the more similar the two examples are. Actually, there are many different ways to measure the similarity, such as cosine distance, Manhattan, Chebyshev, and Hamming distance. In practice, you can choose the one that suits your problem. \n",
    "\n",
    "For this lab, we will implement Equation (2) with the following `euclidean_dist` that computes the Euclidean distances. It will be called eventually by the `knn_predict` function in Step 3 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "FWDHxYKZGV1B"
   },
   "outputs": [],
   "source": [
    "# set up code for this experiment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    " \n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6XsyQxBYxBvp"
   },
   "outputs": [],
   "source": [
    "def euclidean_dist(X_test, X_train):\n",
    "  dists = np.add(np.sum(X_test ** 2, axis=1, keepdims=True), np.sum(X_train ** 2, axis=1, keepdims=True).T) - 2* X_test @ X_train.T\n",
    "  return dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_aUcYK-KINy"
   },
   "source": [
    "**Step 2.**  Once distances are calculated, we can find the top $k$ nearest neighbors for each test example by retrieving from the dists matrix. \n",
    "In particular, for each test example $x$, we can sort all the training examples by their distance to $x$ then find the $k$ most nearest neighbors.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lveUni0IT2G4"
   },
   "outputs": [],
   "source": [
    "def find_k_neighbors(dists, Y_train, k):\n",
    "  \"\"\"\n",
    "  find the labels of the top k nearest neighbors\n",
    "\n",
    "  Inputs:\n",
    "  - dists: distance matrix of shape (num_test, num_train)\n",
    "  - Y_train: A numpy array of shape (num_train) containing ground truth labels for training data\n",
    "  - k: An integer, k nearest neighbors\n",
    "\n",
    "  Output:\n",
    "  - neighbors: A numpy array of shape (num_test, k), where each row containts the \n",
    "               labels of the k nearest neighbors for each test example\n",
    "  \"\"\"\n",
    "  \n",
    "  num_test = dists.shape[0]\n",
    "  neighbors = np.zeros((num_test, k))\n",
    "  sorted_idx = dists.argsort(axis=1)\n",
    "  for i in range(num_test):\n",
    "    neighbors[i] = Y_train[sorted_idx[i][:k]]\n",
    "  return neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISP74Eh2rr4i"
   },
   "source": [
    "**Step 3.** Finally, we can put together `euclidean_dist` and `find_k_neighbors`, so that labels can be predicted for test examples.  In kNN, we take the labels of the $k$ nearest neighbors and find the most common one and assign it to the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LXqx94yVt41e"
   },
   "outputs": [],
   "source": [
    "def knn_predict(X_test, X_train, Y_train, k):\n",
    "  \"\"\"\n",
    "  predict labels for test data.\n",
    "\n",
    "  Inputs:\n",
    "  - X_test: A numpy array of shape (num_test, dim_feat) containing test data.\n",
    "  - X_train: A numpy array of shape (num_train, dim_feat) containing training data.\n",
    "  - Y_train: A numpy array of shape (num_train) containing ground truth labels for training data\n",
    "  - k: An integer, k nearest neighbors\n",
    "\n",
    "  Output:\n",
    "  - Y_pred: A numpy array of shape (num_test). Predicted labels for the test data.\n",
    "  \"\"\"\n",
    "  # TODO:\n",
    "  # find the labels of k nearest neighbors for each test example,\n",
    "  # and then find the majority label out of the k labels\n",
    "  #\n",
    "  # Here is the pseudo-code:\n",
    "  # dists = euclidean_dist(X_test, X_train)\n",
    "  # neighbors = find_k_neighbors(dists, Y_train, k)\n",
    "  # Y_pred = np.zeros(num_test, dtype=int)  # force dtype=int in case the dataset\n",
    "  #                                         # stores labels as float-point numbers\n",
    "  # for i = 0 ... num_test-1\n",
    "  #     Y_pred[i] = # the most common/frequent label in neighbors[i], you can\n",
    "  #                 # implement it by using np.unique\n",
    "  # return Y_pred\n",
    "\n",
    "  \n",
    "  num_test = X_test.shape[0]\n",
    "  Y_pred = np.zeros(num_test, dtype=int)\n",
    "  dists = euclidean_dist(X_test, X_train)\n",
    "  neighbors = find_k_neighbors(dists, Y_train, k)\n",
    "\n",
    "  for i in range(num_test):\n",
    "    value, counts = np.unique(neighbors[i], return_counts=True)\n",
    "    idx = np.argmax(counts)\n",
    "    Y_pred[i] = value[idx]\n",
    "\n",
    "  return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EAaUOyC4AY"
   },
   "source": [
    "**Step 4.** Once we obtain the predicted labels, we need to implement a function to compare them against the true label and compute the error rate in percentage (i.e., a number between 0 and 100). In the following code block, implement the `compute_error_rate` function by following the specified inputs and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Vr3PVo1_C7_r"
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(ypred, ytrue):\n",
    "  \"\"\"\n",
    "  Compute error rate given the predicted results and true lable.\n",
    "  Inputs:\n",
    "  - ypred: array of prediction results.\n",
    "  - ytrue: array of true labels.\n",
    "    ypred and ytrue should be of same length.\n",
    "  Output:\n",
    "  - error rate: float number indicating the error in percentage\n",
    "                (i.e., a number between 0 and 100).\n",
    "  \"\"\"\n",
    "  \n",
    "  error_rate =  (ypred != ytrue).mean()*100\n",
    "  return error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdQoqFW9khl9"
   },
   "source": [
    "### 1.2 Splitting training data for cross validation **(36 points)**{-}\n",
    "\n",
    "Cross validation is a technique in which we train our model using a subset of the available dataset and then evaluate using the complementary subset of the data. In this assignment, we use the $n$-fold cross validation method to perform cross validation. In $n$-fold cross validation, we evenly partition the dataset into $n$ mutually disjoint subsets (a.k.a. _folds_). We train an ML model on all but one subset (i.e., train on the union of $n-1$ folds), and then evaluate the model on the subset that was left out.  The former is called _training subset_, while the latter is called _validation subset_. This process is repeated $n$ times, with a different subset reserved for evaluation (and excluded from training) each time. If the size of the dataset is not exactly divisible by $n$, the remainder can be arbitrarily distributed into the folds.\n",
    "\n",
    "**Step 1. (18 points)** In the following code block, you will need to implement a function that partitions the dataset in to training sets and validation sets. The output should be lists of indices which indicate the training examples and validation examples.  Function inputs and outputs are detailed in the code block. \n",
    "\n",
    "**Hint:** You may find random permutation useful here: [np.random.permutation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.permutation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OY6j6Ggdr1ph"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 9, 1, 6, 0, 7, 10, 8, 5], [2, 3, 1, 6, 0, 7, 10, 8, 5], [2, 3, 4, 9, 0, 7, 10, 8, 5], [2, 3, 4, 9, 1, 6, 10, 8, 5], [2, 3, 4, 9, 1, 6, 0, 7]]\n",
      "[[2, 3], [4, 9], [1, 6], [0, 7], [10, 8, 5]]\n"
     ]
    }
   ],
   "source": [
    "def split_nfold(num_examples, n):\n",
    "  \"\"\"\n",
    "  Split the dataset in to training sets and validation sets.\n",
    "  Inputs:\n",
    "  - num_examples: Integer, the total number of examples in the dataset\n",
    "  - n: number of folds\n",
    "  Outputs:\n",
    "  - train_sets: List of lists, where train_sets[i] (i = 0 ... n-1) contains \n",
    "                the indices of examples for training\n",
    "  - validation_sets: List of list, where validation_sets[i] (i = 0 ... n-1) \n",
    "                contains the indices of examples for validation\n",
    "\n",
    "  Example:\n",
    "  When num_examples = 10 and n = 5, \n",
    "    the output train_sets should be a list of length 5, \n",
    "    and each element in this list is itself a list of length 8, \n",
    "    containing 8 indices in 0...9\n",
    "  For example, \n",
    "    we can initialize by randomly permuting [0, 1, ..., 9] into, say,\n",
    "      [9, 5, 3, 0, 8, 4, 2, 1, 6, 7]\n",
    "    Then we can have\n",
    "    train_sets[0] = [3, 0, 8, 4, 2, 1, 6, 7],  validation_sets[0] = [9, 5]\n",
    "    train_sets[1] = [9, 5, 8, 4, 2, 1, 6, 7],  validation_sets[1] = [3, 0]\n",
    "    train_sets[2] = [9, 5, 3, 0, 2, 1, 6, 7],  validation_sets[2] = [8, 4]\n",
    "    train_sets[3] = [9, 5, 3, 0, 8, 4, 6, 7],  validation_sets[3] = [2, 1]\n",
    "    train_sets[4] = [9, 5, 3, 0, 8, 4, 2, 1],  validation_sets[4] = [6, 7]\n",
    "  Within train_sets[i] and validation_sets[i], the indices do not need to be sorted.\n",
    "  \"\"\"\n",
    "  # generate random index list\n",
    "  # fold_size = num_examples//n   # compute how many examples in one fold.\n",
    "  #                               # note '//' as we want an integral result\n",
    "  # train_sets = []\n",
    "  # validation_sets = []\n",
    "  # for i = 0 ... n-1\n",
    "  #\t  start = # compute the start index of the i-th fold\n",
    "  #\t  end = # compute the end index of the i-th fold\n",
    "  #   if i == n-1\n",
    "  #     end = num_examples  # handle the remainder by allocating them to the last fold\n",
    "  #   For example, when num_examples = 11 and n = 5, \n",
    "  #     fold_size = 11//5 = 2\n",
    "  #     i = 0: start = 0, end = 2\n",
    "  #     i = 1: start = 2, end = 4\n",
    "  #     i = 2: start = 4, end = 6\n",
    "  #     i = 3: start = 6, end = 8\n",
    "  #     i = 4: start = 8, end = 11  (take up the remainder of 11//5)\n",
    "  #\n",
    "  #   # Now extract training example indices from the idx list using start and end\n",
    "  #   train_set = idx[`0 to num_example-1` except `start to end-1`]  \n",
    "  #   train_sets.append(train_set)\n",
    "  #\n",
    "  #   # Extract validation example indices from the idx list using start and end\n",
    "  #   val_set = idx[start to end-1] \n",
    "  #   validation_sets.append(val_set)\n",
    "\n",
    "  #avoid randomness\n",
    "  np.random.seed(1) \n",
    "  \n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  # create list of num_examples size with random indices\n",
    "  idx = np.random.permutation(num_examples).tolist() \n",
    "  \n",
    "  # find fold size\n",
    "  fold_size = num_examples//n   # compute how many examples in one fold.                            # note '//' as we want an integral result\n",
    "  \n",
    "  # intialize train_sets\n",
    "  train_sets = []\n",
    "    \n",
    "  # initialize val_sets\n",
    "  validation_sets = []\n",
    "  \n",
    "  # find training and val sets for each fold\n",
    "  for i in range(n):\n",
    "    # calculate start and end for each fold size\n",
    "    start = i * fold_size\n",
    "    end = fold_size + i * fold_size\n",
    "    # if num_examples does not divide evenly\n",
    "    if i == n-1:\n",
    "        end = num_examples  # handle the remainder by allocating them to the last fold\n",
    "    \n",
    "    # Extract training indices, exclude between start and end\n",
    "    train_set = [idx[x] for x in range(num_examples) if x not in range(start,end)]\n",
    "    train_sets.append(train_set)\n",
    "    \n",
    "    # Extract validation example indices from the idx list using start and end\n",
    "    val_set = idx[start:end] \n",
    "    validation_sets.append(val_set)\n",
    "\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  return train_sets, validation_sets\n",
    "\n",
    "# Unit test code here (you can uncomment the lines below to test)\n",
    "train_sets, val_sets = split_nfold(11, 5)\n",
    "print(train_sets)\n",
    "print(val_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LWlt0cd8S4n"
   },
   "source": [
    "**Step 2. (18 points)** Next, you will need to implement the `cross_validation` function, which will output the cross validation error rate. You may want to call previously defined functions such as `split_nfold` and `compute_error_rate`. In this function, you need to loop over each of the $n$ training/validation partitions in the output of `split_nfold`.\n",
    "Then perform training on train_sets[i] and compute the test error on validation_sets[i].  The final cross validation error rate is the average error rate over all partitions. \n",
    "\n",
    "To improve generality, `cross_validation` takes as its first input argument a generic _classifier_ function.  In this lab, we will use kNN, and _classifier_ should be instantiated by the `knn_predict` function that is implemented above. In general, the _classifier_ function should conform with a prescribed protocol of prototype, i.e., what the input and output arguments are.  For example its inputs are `(X_test, X_train, Y_train, k)` and its output is `Y_pred`.\n",
    "\n",
    "**Hint:** You may need to know how to use [*args](https://book.pythontips.com/en/latest/args_and_kwargs.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zWHYG-ktFV0x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_validation(classifier, X, Y, n, *args):\n",
    "  \"\"\"\n",
    "  Perform cross validation for the given classifier, \n",
    "      and return the cross validation error rate.\n",
    "  Inputs:\n",
    "  - classifier: function of classification method\n",
    "  - X: A 2-D numpy array of shape (num_train, dim_feat), containing the whole dataset\n",
    "  - Y: A 1-D numpy array of length num_train, containing the ground-true labels\n",
    "  - n: number of folds\n",
    "  - *args: parameters needed by the classifier.\n",
    "        In this assignment, there is only one parameter (k) for the kNN clasifier.\n",
    "        For other classifiers, there may be multiple paramters. \n",
    "        To keep this function general, \n",
    "        let's use *args here for an unspecified number of paramters.\n",
    "  Output:\n",
    "  - error_rate: a floating-point number indicating the cross validation error rate\n",
    "  \"\"\"\n",
    "  # Here is the pseudo code:\n",
    "  # errors = []\n",
    "  # size = X.shape[0] # get the number of examples\n",
    "  # train_sets, val_sets = split_nfold(size, n)  # call the split_nfold function\n",
    "  #\n",
    "  # for i in range(n):\n",
    "  #   train_index = train_sets[i]\n",
    "  #   val_index = val_sets[i]\n",
    "  #   # get the training and validation sets of input features from X\n",
    "  # \tX_train, X_val = X[...], X[...] \n",
    "  #\n",
    "  #   # get the training and validation labels from Y\n",
    "  # \ty_train, y_val = Y[...], Y[...] \n",
    "  #\n",
    "  #   # call the classifier to get prediction results for the current validation set\n",
    "  # \typred = # call classifier with X_val, X_train, y_train, and *args\n",
    "  #                                   \n",
    "  # \terror = # call compute_error_rate to compute the error rate by comparing ypred against y_val\n",
    "  # \tappend error to the list `errors`\n",
    "  # error_rate = mean of errors\n",
    "  np.random.seed(1)\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  # Initialize list for errors\n",
    "  errors = []\n",
    "  # find # of examples\n",
    "  size = X.shape[0]\n",
    "  # use split_nfold function to get training and val sets\n",
    "  train_sets, val_sets = split_nfold(size, n)\n",
    "  \n",
    "  # get corresponding value of X related to train and val sets\n",
    "  for i in range(n):\n",
    "    train_index = train_sets[i]\n",
    "    val_index = val_sets[i]\n",
    "    \n",
    "    # get the training and validation sets of input features from X\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    \n",
    "    # get the training and validation labels from Y\n",
    "    y_train, y_val = Y[train_index], Y[val_index]\n",
    "    \n",
    "    # call the classifier to get prediction results for the current validation set\n",
    "    y_pred = classifier(X_val, X_train, y_train, *args)\n",
    "    \n",
    "    # call compute_error_rate to compute the error rate by comparing ypred against y_val\n",
    "    error = compute_error_rate(y_pred, y_val)\n",
    "    errors.append(error)\n",
    "  # error_rate = mean of errors\n",
    "  error_rate = sum(errors)/len(errors)\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "  return error_rate \n",
    "\n",
    "# Unit test code here (you can uncomment the lines below to test)\n",
    "X_dataset = np.array([[1, 2], [0, 3], [-1, 1], [-1, 0], [2, 1]])\n",
    "Y_dataset = np.array([1, 1, 1, 0, 0])\n",
    "n = 5\n",
    "k = 3\n",
    "cross_validation(knn_predict, X_dataset, Y_dataset, n, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZKSHAuf_R9U"
   },
   "source": [
    "Side note: instead of `for i in range(n):`, you can also use\n",
    "\n",
    "`for (train_index, val_index) in zip(train_sets, val_sets):`\n",
    "\n",
    "Try it if you like as it can be more generic. No need to submit anything for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNJ62NKozii3"
   },
   "source": [
    "## Problem 2: Optical character recognition (OCR) **(18 points)**{-}\n",
    "\n",
    "We will now apply the above developed function to a real world problem of optical character recognition (OCR).\n",
    "\n",
    "**Load the MNIST dataset.** In the following code block, we have downloaded the MNIST dataset and split the data into trainning and test sets. This part has already been done, and you can directly run it with no need of modifying the code.  But do make sure that you understand the code as it will be useful for future labs.\n",
    "\n",
    "**Note:** after running the code, the training data (Xtrain, ytrain) has 10,000 examples, and the test data (Xtest, ytest) also has 10,000 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sgWufXl41uJX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "\n",
    "DATA_URL = 'https://www.cs.uic.edu/~zhangx/teaching/'\n",
    "\n",
    "# Download and import the MNIST dataset from Yann LeCun's website.\n",
    "# Each image is an array of 784 (28x28) float values  from 0 (white) to 1 (black).\n",
    "def load_data():\n",
    "    x_tr = load_images('train-images-idx3-ubyte.gz')\n",
    "    y_tr = load_labels('train-labels-idx1-ubyte.gz')\n",
    "    x_te = load_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_te = load_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    return x_tr, y_tr, x_te, y_te\n",
    "\n",
    "def load_images(filename):\n",
    "    maybe_download(filename)\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    return data.reshape(-1, 28 * 28) / np.float32(256)\n",
    "\n",
    "def load_labels(filename):\n",
    "    maybe_download(filename)\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    return data\n",
    "\n",
    "# Download the file, unless it's already here.\n",
    "def maybe_download(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        from urllib.request import urlretrieve\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(DATA_URL + filename, filename)\n",
    "\n",
    "Xtrain, ytrain, Xtest, ytest = load_data()\n",
    "\n",
    "train_size = 10000\n",
    "test_size  = 10000\n",
    "\n",
    "Xtrain = Xtrain[0:train_size]\n",
    "ytrain = ytrain[0:train_size]\n",
    "\n",
    "Xtest = Xtest[0:test_size]\n",
    "ytest = ytest[0:test_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XeimykK5pYe"
   },
   "source": [
    "### 2.1 Effect of different number of cross validation folds **(18 points)**\n",
    "\n",
    "In the following code block, we will perform cross validation on 1-NN classification. Call the `knn_predict` and `cross_validation` functions you have implemented, and compute the cross validation error rate for the first **1000** training examples with different number of folds $n \\in \\{3, 10, 50, 100, 1000\\}$. Then **print** the error rate for each different $n$ and **plot** a figure where the $x$-axis is $n = \\{3, 10, 50, 100, 1000\\}$, and the $y$-axis is the $n$-fold cross validation error rate. \n",
    "\n",
    "**Note about terminology:** In Problem 1, we used the term _dataset_, and the $n$-fold partitioning was on the _dataset_.  Now in the current setting, these **1000** training examples correspond to the _dataset_. In other words, this Section 2.2 will **not** use the test examples loaded from Problem 2, nor the remaining $10000 - 1000 = 9000$ training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "I-BHBavY64nc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-folds error rate: 12.70%\n",
      "\n",
      "10-folds error rate: 12.00%\n",
      "\n",
      "50-folds error rate: 12.10%\n",
      "\n",
      "100-folds error rate: 11.90%\n",
      "\n",
      "1000-folds error rate: 11.80%\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "size = 1000\n",
    "k = 1\n",
    "\n",
    "# Here is the pseudo code:\n",
    "#\n",
    "# get the feature/label of the first 'size' (i.e., 1000) number of training examples\n",
    "# cvXtrain = Xtrain[...]  \n",
    "# cvytrain = ytrain[...]  \n",
    "\n",
    "# trial_folds   = [3, 10, 50, 100, 1000]\n",
    "# trials = number of trials on #folds, i.e., get the length of trial_folds (=5)\n",
    "# cverror_rates = [0]*trials\n",
    "\n",
    "# for t = 0 ... trials-1\n",
    "# \terror_rate = # call the 'cross_validation' function to get the error rate \n",
    "#                #  for the current trial (of fold number)\n",
    "# \tcverror_rates[t] = error_rate\n",
    "#\n",
    "#   # print the error rate for the current trial.\n",
    "# \tprint('{:d}-folds error rate: {:.2f}%\\n'.format(trial_folds[t], error_rate)) \n",
    "#\n",
    "# plot the figure:\n",
    "# f = plt.figure()\n",
    "# plt.plot(...)\n",
    "# plt.xlabel(...)\n",
    "# plt.ylabel(...)\n",
    "# plt.show()\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "# retrieve 1000 training examples\n",
    "cvXtrain = Xtrain[:size]\n",
    "cvytrain = ytrain[:size]\n",
    "\n",
    "# define folds\n",
    "trial_folds = [3,10,50,100,1000]\n",
    "trials = len(trial_folds)\n",
    "cverror_rates = [0]*trials\n",
    "\n",
    "for t in range(trials):\n",
    "    error_rate = cross_validation(knn_predict, cvXtrain, cvytrain, trial_folds[t], k)\n",
    "    cverror_rates[t] = error_rate\n",
    "    \n",
    "    print('{:d}-folds error rate: {:.2f}%\\n'.format(trial_folds[t], error_rate))\n",
    "\n",
    "\n",
    "# plot the figure:\n",
    "f = plt.figure()\n",
    "plt.plot(trial_folds, cverror_rates)\n",
    "plt.xlabel(\"trial fold size\")\n",
    "plt.ylabel(\"cross validation error rate\")\n",
    "plt.show()\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqIJpMGSznO7"
   },
   "source": [
    "## Problem 3: Iris plant recognition **(46 point)**{-}\n",
    "\n",
    "The iris dataset includes 3 iris species of 50 examples each, where each example recorded petal and sepal length. For convenience, we will use the built-in functions in `scikit-learn` library to load dataset and create data partitions. For this experiment, we will use $80\\%$ (120) examples for training and $20\\%$ (30) for testing. \n",
    "\n",
    "Actually, we have done this data preparation work for you. You can directly use the training set (*X_train*, *Y_train*) and test set (*X_test*, *Y_test*) for the experiments, where *X* is features and *Y* is labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "bTjfkSdYFQI2"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# loading iris dataset\n",
    "iris = load_iris()\n",
    "# split dataset into training set and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBtnC-PuIeGz"
   },
   "source": [
    "### 3.1 Find the best $k$. **(18 points)**{-}\n",
    "In Problem 2, we conducted the experiments by arbitrarily setting $k$ to 1.\n",
    "In fact, the value of $k$ has a considerable impact on the performance of kNN. We will now determine the best value of this hyperparameter with $10$-fold cross-validation.\n",
    "To specify, we will vary $k$ in the range (1, 100) in increments of 1.\n",
    "Then we will find the best $k$ in terms of the lowest validation error rate.\n",
    "For this question, you need to:\n",
    "* **(9 points)** Store the validation error for each $k$ in an array and report the value of the best $k$. \n",
    "* **(9 points)** Plot a curve that shows the validation error rates as $k$ increases. \n",
    "\n",
    "**Note about terminology:** In Problem 1, we used the term _dataset_, and the $n$-fold partitioning was on the _dataset_.  Now in the current setting, (X_strain, Y_train) loaded above correspond to the _dataset_. In other words, this Section 3.1 will **not** use the test examples loaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "gPm48esA3xJG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best k: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAuY0lEQVR4nO3dd3xV9fnA8c+TDYQsMgxBVlhhLxUHLlBBBbRWq1Wq1mprrXX019Zt1ba27qrVSltX66itA+ugAgo4UUBGIMywEkMSIIvs8fz+uCcYIONk3Nzk3uf9et1X7jn3jOcb8bkn3/M9z1dUFWOMMYEjyNcBGGOM6VyW+I0xJsBY4jfGmABjid8YYwKMJX5jjAkwIb4OwI34+HgdOHCgr8MwxphuZeXKlXtVNeHw9d0i8Q8cOJAVK1b4OgxjjOlWRGRnY+utq8cYYwKMJX5jjAkwlviNMSbAWOI3xpgAY4nfGGMCjNcSv4hEiMiXIrJGRNaLyD3O+t+ISLaIrHZeZ3srBmOMMUfy5nDOSuB0VT0gIqHAJyLyvvPZo6r6kBfPbYwxpgleu+JXjwPOYqjzshrQxhjjQkV1Lb95ez25xRUdfmyv9vGLSLCIrAbygIWqutz56GcislZEnhWR2Cb2vUZEVojIivz8fG+GaYwxXc5TS7bx/Gc72JZ/oOWNW8mriV9Va1V1PNAPOFZERgNPA6nAeCAHeLiJfeep6mRVnZyQcMQTx8YY47d27C3lL0u3MWd8X05Ije/w43fKqB5VLQSWADNUNdf5QqgD/goc2xkxGGNMd6Cq3PX2esKDg7j97DSvnMObo3oSRCTGed8DmA5sFJHkBpudD6R7KwZjjOluFqTvYdnmfG46YxiJURFeOYc3R/UkAy+ISDCeL5jXVPUdEfmHiIzHc6N3B/BjL8ZgjDFdTkFpFW+v+YaauiPHu/zt40zSkqP4wfEDvHZ+ryV+VV0LTGhk/VxvndMYY7qDvyzbxjNLMxv9rFdYMH++dCIhwd7rie8WZZmNMcafLNqQywmpfXj6sklHfBYeEkREaLBXz2+J3xhjOtH2vaVsyy9l7pQBRPcI9UkMVqvHGGM60eKMXACmpSX5LAZL/MYY04kWZeQy4qjeHB3X02cxWOI3xphOUlhWxVc7CpiWlujTOCzxG2NMJ1myKZ/aOmW6D7t5wBK/McZ0mkUZucRHhjOuX4xP47DEb4wxnaCqpo6lm/KZNiKRoCDxaSw2nNMYY7ykrKqGz7ftQxUy9x6gpLKG6SN9280DlviNMcYrVJUfPv8VX2TuP7iud3gIJw3p+GqbrWWJ3xhjvOCt1dl8kbmfX541nJOHekrLJ/QOp0eYd5/KdcMSvzHGdLCi8mp+9+5Gxh0dw7WnpPq8T/9wlviNMaaDPbpwM/tLK3nuimO6XNIHG9VjjDEdKj27iBc/38FlUwYwpl+0r8NplCV+Y4zpIHV1yh1vpRPXK4xfnDnc1+E0yRK/McZ0kNdW7Gb17kJuOzvNZ5U33bDEb4wxHWB/aRV/WLCRYwfGcf6EFF+H0yxL/MYY0wEeWLCRkooa7jtvNCJd74ZuQzaqxxhjgL0HKikorWrTvjv2lfHqV7u5euoghh/Vu4Mj63iW+I0xAW9dVhEX/OUzqmrq2nyMpKhwbpg+rAOj8h5L/MaYgFZbp9zx1jqiIkK5e9ZI2tpLM3lAHJHh3SOlei1KEYkAlgHhznn+o6p3i0gc8C9gILADuEhVC7wVhzHGNOfVr3axJquIx743nlnj+vo6nE7hzZu7lcDpqjoOGA/MEJEpwC3AYlUdCix2lo0xptPtO1DJAws2MWVwHHPGB0bSBy9e8auqAgecxVDnpcAc4FRn/QvAEuDX3orDGGMaKq6opqK6FoA/vr+J0soa7pvT9UfidCSvdkiJSDCwEhgC/FlVl4tIkqrmAKhqjoj4dvJJY0zA+GhTHj9+cSVVtd/exP3xKYMZmtT1R+J0JK8mflWtBcaLSAzwpoiMdruviFwDXAPQv39/7wRojAkY5VW13PlWOv3ievDDEwcBEBkewtljkn0cWefrlFvQqlooIkuAGUCuiCQ7V/vJQF4T+8wD5gFMnjxZOyNOY4z/emrJVrIKynnl6ikcn9rH1+H4lNdu7opIgnOlj4j0AKYDG4G3gcudzS4H5nsrBmOMAdi+t5RnlmYyZ3zfgE/64N0r/mTgBaefPwh4TVXfEZHPgddE5CpgF3ChF2Mwxvi5qpo6lKY7BVThrvnphIcEcfvZaZ0YWdflzVE9a4EJjazfB0zz1nmNMYHjT4u28Oiiza62vXvWSBKjIrwcUffgKvGLyEnAUFV9TkQSgEhV3e7d0Iwxpmkb9xTz+IdbOGVYAscOimt224Te4VwwsV8nRdb1tZj4ReRuYDIwHHgOz3j8fwInejc0Y4xpnKpy51vpREWE8Nj3xhPbK8zXIXUrbm7ung/MBkoBVPUbILAGvRpjupTXV2Xz1Y4Cbpk5wpJ+G7hJ/FXOU7gKICK9vBuSMcY0raismvvfy2Bi/xgunHS0r8Ppltz08b8mIs8AMSJyNfBD4G/eDcsYE2hq65RrXlzBZ9v2HVx34eR+3Dvn0Oc+H/xgIwVlVbx41bEEBQVOmYWO1GLiV9WHROQMoBhPP/9dqrrQ65EZYwLKy8t3snhjHhdM7EefyDAy80t58fOdTEtL4pRhCQCszSrkpeW7uPz4gYzqG+3jiLsvNzd3/6iqvwYWNrLOGGPaLb+kkgf+t4kTh/ThoQvHIiJU1tQy47GPuXt+OgtuPJnQ4CDueCud+Mhwbj6ze0x40lW56eM/o5F1Mzs6EGNM4Lr//Qwqqmu5t0GVzPCQYO6dM4od+8qYtyyTV77cxdqsIu44J42oiFAfR9y9NXnFLyLXAj8FBovI2gYf9QY+9XZgxpjAsDxzH2+syuanp6aSmhB5yGdThyZwzphk/vzRVsJDgjh+cB9mB8hkKd7UXFfPy8D7wP0cOllKiaru92pUxpiAUF1bx13z15MS04PrTx/a6DZ3njuSJZvyKKuq5b7zRgVU3XxvaTLxq2oRUARcAuDUzY8AIkUkUlV3dU6Ixhh/9fynO9iUW8K8uZPoERbc6DZHRUfwl7mTKK2sYUiiPULUEdzc3J0FPAL0xVNCeQCQAYzybmjGGH+2p6iCxxZt5vQRiZwxMqnZbacOTeikqAKDm5u7vwWmAJtVdRCeAmvWx2+MaZf73t1ATZ3ym1nWfdPZ3CT+aqeiZpCIBKnqR3gmTzfGmDb5eEs+767N4brThtC/T09fhxNw3Dy5WygikcAy4CURyQNqvBuWMcaflFRUc9O/VpNVUA7AN4XlDOzTk2tOHuzjyAKTm8Q/BygHbgIuBaKBe70ZlDHGvzy2aAuLN+YxPS2JIIHUhEiuPTWViNDGb+ga72o28TuzZ81X1elAHfBCp0RljPEbG/cU8/xnO7jk2P78/vwxvg7H0EIfv6rWAmUiYkUxjDGtVlen3PFmOtE9QvnVWcN9HY5xuOnqqQDWichCnJr8AKr6c69FZYzxC6+vymLFzgIeuGAsMT2tbn5X4Sbxv+u8jDHGtaKyav7w/kYm9o/hu5Ns2sOuxE1ZZuvXN8a0Wn3d/H9cdZzVze9i3IzjN8aYVlmz26mbf8JARvaN8nU45jBeS/wicrSIfCQiGSKyXkRucNb/RkSyRWS18zrbWzEYYzpfbZ1y53xP3fybzrC6+V1Rs4lfRIJF5ME2HrsG+IWqpuEp+XCdiIx0PntUVcc7r/faeHxjTBdkdfO7vmb7+FW1VkQmiYg4E667pqo5QI7zvkREMoCUtodqjOnqCsuqeGDBRqub38W56er5GpgvInNF5Dv1r9acREQGAhOA5c6qn4nIWhF5VkRim9jnGhFZISIr8vPzW3M6Y4yPfL2rkOKKGq6fNsQKr3VhbhJ/HLAPOB2Y5bzOdXsCp87P68CNqloMPA2k4in0lgM83Nh+qjpPVSer6uSEBCvJakx3kFXoqcUzOD6yhS2NL7kZznllWw8uIqF4kv5LqvqGc7zcBp//FXinrcc3xnQt2QXlhAYLib3DfR2KaUaLV/wi0k9E3hSRPBHJFZHXRaTFpzHE83fe34EMVX2kwfrkBpudD6S3JXBjTNeTXVhOcnQPG7ffxbl5cvc5PPPvXugsX+asO6OF/U4E5uIp97DaWXcbcImIjAcU2AH8uFURG2O6rOyCMlJievg6DNMCN4k/QVWfa7D8vIjc2NJOqvoJ0NjXvg3fNMZPZReW2zSJ3YCbm7t7ReQyZ0x/sIhchudmrzHGHFRVU0deSaVd8XcDbhL/D4GLgD14RuF811lnjDEH5RSVowopsZb4uzo3E7H8XlVnd1I8xphuKtuZVrGfXfF3eW4mYkkQESukbYxpVv18unbF3/W5ubm7A/hURN7m0IlYHmlyD2NMwMkqLEcEkqMt8Xd1bhL/N84rCOjt3XCMMd1VdkE5ib3DCQuxau9dnZs+/qGqelknxWOM6aayC20Mf3dhffzGmA6RXVhOSmxPX4dhXLA+fmOMK/kllXyRuY9ZjZRbrq1TcgorOGeMXfF3B9bHb4xpkaryi3+vYdnmfNKSoxiSeGj1zbySCmrqlH42oqdbcFOd8x4AEemlqqUtbW+M8T8L0vewbLNnXozFGblHJP5sG8rZrbipznm8iGwAMpzlcSLylNcjM8Z0CaWVNdz7zgbSkqMYcVRvFmfkHbFNdqE9vNWduBl39RhwFk59HlVdA5zsxZiMMV3I4x9uIaeogt+eN5ozRyaxYud+CkqrDtnGHt7qXlwNuFXV3YetqvVCLMaYLmZLbgl//3g7F03ux6QBsUwfmUSdwkebDr3qzy4sJ7ZnKD3D3Nw2NL7mJvHvFpETABWRMBH5P5xuH2OMf3v1q90EBwm/njECgNF9o0nsHc6ijNxDtssuKLer/W7ETeL/CXAdkAJk4Zkr9zovxmSM6SK25R8gNSGSPpGeqRSDgoRpaUks3ZRPZc23f/hnF5bbw1vdSIuJX1X3quqlqpqkqomqepmqWj1+YwJAZn4pgxN6HbLujJGJlFbVsjxzP+AZ6pldUE5KjD281V1YUQ1jTKMqa2rJKihjcMKhQzdPSI0nIjToYHdPQVk15dW11tXTjdidGGNMo3buK6NOYXD8oVf8EaHBTB2awMINuRwzMI5vnKGc1tXTfVjiN8Y0KjP/AMARXT0A54xJZuGGXK5/5euD64YlRR6xnemaWkz8IhIOXAAMbLi9qt7rvbCMMb6WudfzoP6g+CMT/5zxfZnQP4bq2joAIsNDOSo6olPjM23n5op/PlAErAQq3R5YRI4GXgSOAuqAear6JxGJA/6F54tkB3CRqha0LmxjjLdl5peS2Duc3hGhR3wmIgzoc+QXguke3CT+fqo6ow3HrgF+oaqrRKQ3sFJEFgJXAItV9Q8icgtwC/DrNhzfGONFmfkHGu3mMd2fm1E9n4nImNYeWFVzVHWV874Ez0NfKcAc4AVnsxeA81p7bGOM92XuLWVQvPXb+yM3V/wnAVeIyHY8XT0CqKqOdXsSERkITACWA0mqmoPnIDkiktjEPtcA1wD079/f7amMMR1gf2kVhWXVpNoVv19yk/hntucEIhIJvA7cqKrFIuJqP1WdB8wDmDx5srYnBmNM62zf2/SIHtP9uXlydycQA8xyXjHOuhaJSCiepP+Sqr7hrM4VkWTn82TgyBqvxhif2pbvGdEz2Lp6/JKbevw3AC8Bic7rnyJyvYv9BPg7kHHYNI1vA5c77y/HM2rIGNOFZOaXEhosNqOWn3LT1XMVcFz97Fsi8kfgc+CJFvY7EZgLrBOR1c6624A/AK+JyFXALuDCNsRtjPGizPwD9I/rSUiwVXXxR24Sv3Bo/f1aZ12zVPWTZrab5uK8xhgfydxbekSNHuM/3CT+54DlIvKms3weni4cY4wfqq1Tdu4rZVpaowPujB9wM9n6IyKyBM+wTgGuVNWvm9/LGNNdZRWUUV2rpNqNXb/VZOIXkShn+GUcntIKOxp8Fqeq+70fnjGms2XWj+ixoZx+q7kr/peBc/HU6Gk4jl6c5cFejMsY4yPbnKqcjRVnM/6hycSvquc6Pwd1XjjGGF9bvn0/sT1DiesV5utQjJe4Gce/2M06Y0z398mWvSzckMsVJwzC7VP2pvtpro8/AugJxItILN8OzYwC+nZCbMaYTlRZU8td89MZ2KcnPz7FenL9WXN9/D8GbsST5FfybeIvBv7s3bCMMZ3tbx9vJ3NvKS/88FgiQoN9HY7xoub6+P8E/ElErlfVlp7SNcZ0Iaqe8Rhuu2t27y/jiQ+3MHP0UZwyLMGboZkuwM04/idEZDQwEohosP5FbwZmjGm7aQ8v5dIpA7jqJHdjMx5duJkgEe48d6SXIzNdgZubu3fjqcvzBHAa8AAw28txGWPaqLyqlsy9pby+MsvV9lU1dSzMyGXW2L70jbGibIHATQWm7+KprbNHVa8ExgHhXo3KGNNmBWVVAGzIKSa7sLzF7b/asZ+Sihor0RBA3CT+clWtA2pEJApP/Xy75W9MF7W/tOrg+8UZuS1uvygjl/CQIE4aGu/NsEwX4ibxrxCRGOCveEb3rAK+9GZQxpi2KyyrBiBIYFFG8/McqSqLMnI5aUg8PcPc1Gw0/sDNDFw/VdVCVf0LcAZwudPlY4zpguq7ek4elsDn2/ZSUuH5IqitU6547kvump9+cNsteQfYvb+caWlJPonV+EaTiV9EJh7+AuKAEOe9MaYLqk/8F046mupa5eMtewF49atdLNmUz4uf7+TTrZ51Czd4uoKsfz+wNPe33cPOzwhgMrAGz0NcY4HleMo0G2O6mIJSzxX+tLREYnqGsigjl+MGxfHAgk0cOyiO3OIK7pyfzvs3TGVRRi7j+kWTFBXRwlGNP2nuAa7TAETkVeAaVV3nLI8G/q9zwjPGtFZBWRW9w0OICA3mtOGJfLTR089fWlnD784bTVZhOVc+9xV/fH8Tq3cXctP0YT6O2HQ2Nzd3R9QnfQBVTQfGey0iY0y7FJRVEetU1pyelkRBWTVvrMrmR1MHMzSpN6cNT2TGqKN49tPtqHq2MYHFTeLPEJG/icipInKKiPwVyPB2YMaYtikoqya2ZygAJw+LJzRY6Bsdwc+nDTm4zV2zRtIjNJi+0RGkJff2VajGR9yM37oSuBa4wVleBjzttYiMMe1SUFpFn0jPFX/viFAeunAcA/r0OmS4Zt+YHjx12USCRKz8cgByU6unAnjUeRljuriCsiqGJH47X+6c8SmNbnfacBvJE6iaG875mvNznYisPfzV0oFF5FkRyROR9AbrfiMi2SKy2nmd3THNMMbUKyitIsbp6jGmMc1d8dd37ZzbxmM/DzwJHF7F81FVfaiNxzTGNKOqpo7Sqlrietq0iaZpzQ3nzHF+7mzLgVV1mYgMbGNcxpg2KHQe3oqx+XJNM5rr6ikRkeJGXiUiUtyOc/7M6S561pnSsanzXyMiK0RkRX5+fjtOZ0zg2O8kfrviN81pMvGram9VjWrk1VtVo9p4vqeBVDzPAeTw7dPBjZ1/nqpOVtXJCQk2I5AxbtQ/tRtrffymGa7L8YlIIofOwLWrtSdT1YM1Yp3nAd5p7TGMMU2r7+qJta4e0ww3M3DNFpEtwHZgKbADeL8tJxOR5AaL5wPpTW1rjGm9+q6eWOvqMc1wc8V/HzAFWKSqE0TkNOCSlnYSkVeAU4F4EckC7gZOFZHxgOL5Avlx28I2xjSmvha/Dec0zXGT+KtVdZ+IBIlIkKp+JCJ/bGknVW3sy+HvrQ/RGOPW/tIqeoYFExEa7OtQTBfmJvEXikgknlINL4lIHlDj3bCMMW1RUFZl3TymRW6KtM0ByoCbgAXANmCWN4MyxrRNYVk1sb2sm8c0z80V/zXAv1U1C3jBy/EYY9phf6ld8ZuWubnijwL+JyIfi8h1ImLFu43pogqtq8e44Gay9XtUdRRwHdAXWCoii7wemTGm1TxX/NbVY5rn5oq/Xh6wB9gHWD1XY7qYmto6iitqiLErftMCNw9wXSsiS4DFQDxwtaqO9XZgxpjWKSr3jOGPs6d2TQvc3NwdANyoqqu9HIsxph0K6itzWlePaYGbGbhu6YxAjDHtU1BmV/zGndb08RtjurD9pVanx7hjid8YP2GVOY1bbm7u9hKRIOf9MKdap3UiGtPF1Hf12HBO0xI3V/zLgAgRScEzsudKPPPpGmO6kILSKsJCguhhBdpMC9wkflHVMuA7wBOqej4w0rthGWNaq6CsirieYYiIr0MxXZyrxC8ixwOXAu8661zP3GWM6Rz7S6ttKKdxxU3ivxG4FXhTVdeLyGDgI69GZYxptcKyKhvKaVxxM45/KZ4pF3Fu8u5V1Z97OzBjTOsUlFUx4qgoX4dhugE3o3peFpEoEekFbAA2icgvvR+aMaY1Csqsq8e446avfqSqFovIpcB7wK+BlcCDXo3MGNOs2jrl5S93saeoHLCuHuOem8Qf6ozbPw94UlWrRUS9G5YxpiUvL9/JnfPXExwkCBAaHMSYlGhfh2W6ATeJ/xlgB7AGWCYiA4BibwZljGne3gOVPPi/TZyQ2oeXfnScDeE0reJmIpbHVTVFVc9Wj53AaS3tJyLPikieiKQ3WBcnIgtFZIvzM7ad8RsTkO5/byPl1bXcO2e0JX3Tam5u7kaLyCMissJ5PQz0cnHs54EZh627BVisqkPxPAVslT+NaaUvt+/n9VVZXD11MEMSI30djumG3IzjfxYoAS5yXsXAcy3tpKrLgP2HrZ7DtxO2v4DnvoExxqXq2jrufCudlJge/Oz0Ib4Ox3RTbvr4U1X1ggbL94jI6jaeL0lVcwBUNUdEmpzCUUSuAa4B6N+/fxtPZ4x/+WTLXjbllvDk9yfQM8weoDdt4+aKv1xETqpfEJETgXLvheShqvNUdbKqTk5ISPD26YzpFhZm5NIrLJgzRib5OhTTjbm5ZPgJ8KKI1I8TKwAub+P5ckUk2bnaT8YzgbsxxgVVZXFGLicPSyA8xCpwmrZrNvGLSDBwmaqOE5EoAFVtz1DOt/F8afzB+Tm/HccyJqCkZxeTW1zJ9DS72jft02ziV9VaEZnkvG9VwheRV4BTgXgRyQLuxpPwXxORq4BdwIVtCdqYtqisqWVtVhGTB8Q2OQRSVVmyOZ/84srmDyZw6vAEEntHdFh8FdW1LEjfQ1VNHQAJUeGcOizhYKwLM3IJEjhtRJO3xoxxxU1Xz9ci8jbwb6C0fqWqvtHcTqp6SRMfTXMfnjEd557/buDl5bt4+tKJzByT3Og2r6/K5v/+vcbV8b4zIYVHvje+w+K77c11vLEq+5B1D184jgsm9QNgcUYukwbEWlkG025uEn8csA84vcE6BZpN/MZ0JWt2F/LKl7sICRLufWcDJw9LoFf4of/8i8qquf+9DCb2j+HxSyY0+2DU/e9l8OGmPGpq6wgJbv/U1V9u388bq7K5euogrjhxEKrK9a98zf3vZzB9ZBKllTWs/6aYW2eOaPe5jHFTlvnKzgjEGG+prVPueCudhMhwHrxwHJc/+yWPf7iFW2emHbLdQx9soqCsihevOpZ+sT2bPeY5Y5J5Z20OK3cWcNzgPu2Kr+HY/JvPGE6PMM+N2/vmjGb2k5/w8AebGOo8qDXN+vdNB3Dz5O4LIhLTYDlWRJ71alTGdKCXl+9kXXYRd5w7klOGJXDR5H78/ePtbMktObjNuqwi/rl8Jz84fiCj+rZc6GzqsATCgoNYlJHb7vhe+GwHm3JLuHvWyINJH2B0SjRzpwzgH1/s5LnPdjAovhepCW4emjemeW66esaqamH9gqoWiMgE74VkTMepL2Z24pA+zBrr6de/ZWYaH2zI5dY31nH1yYMBePLDrcRHhnPzmcNcHTcyPIQpqX1YnJHH7ed8OwV1bnEFq3cXuo6vuraORxdu5vQRiY2Ozb/5zOG8u24PmfmlXD11kNXlMR3CTeIPEpFYVS0AT6E1l/sZ43P1xczumf1tMbO4XmHcNjONX72+lhX/WHlw28cvmUBUhPuJTKanJXLX/PVsyz9AakIkxRXVnPvEJ+SXtDAi6DC9woL5zaxRjSb16B6h3HluGje8upoZoxu/IW1Ma7lJ4A8Dn4nIf/Dc1L0I+J1XozKmA9QXM/vpqalHFDO76JijOW5wHAcqawCIigjl6Ljm+/UPNy0tibvmr2dxRi6pCZE8unAzew9UMm/uJFJie7g+zlFREfSJDG/y8znjUzg+tU+HDh01gc3Nzd0XRWQFnlE9AnxHVTd4PTJj2sFNMbMBfdrXX54S04O05CgWbcjjxCHxvPDZDi49rj9njjqqXcdtjCV905Fcddk4id6Svek26m+YPjN3kleLmZ2RlsiTH23lV/9ZS2zPMH55pg23NF1f+wcgdxOVNbXkFHm9tpzxkcqaWpZn7uOLzH0s2ZR38IbpmV4uZjYtLYk6xTPG/uw0om2yc9MNBMxN2meWZjJvWSYr7phORKgVuPInqsqPXljBx1v2HlwXERrU5A3TjjQmJZqUmB6kxPTggokpXj2XMR0lYBL/Vzv2c6Cyhk17Shh3dIyvwzEd6J21OXy8ZS8/P30IU1I9D1MN7NOLvjHub7C2VVCQ8MZPT6BnWLANtTTdRkAkflVlbVYRAGuziyzx+5EDlTX89t0NjE6J4obpwwgO6vzkmxRlN15N9xIQiT+roJyi8moA0p0vAOMfHlu4mbySSp6ZO9knSd+Y7iggEn/91X5C73DWZlvi727KqmrYtb/siPV7iip47rMdXHxMf8bbX3HGuBYYiT+7kLDgIM6fkMLfP9lORXWt3eDtJsqqapj5p4/Zue/IxA+ep3B/ddbwTo7KmO4tIBL/uqwiRiT3ZtKAWOYty2RDTjET+8f6OizjwpMfbmXnvjLumT2KxN5HPt069ugYYq0+vTGt4veJX1VZl13E7HF9GdvPU3UxPbvIEn83sDXvAH/9OJPvTEzh8hMG+jocY/yG3z/AtXNfGSUVNYxJieaoqAjiI8MO9vmbrktVuWt+OhGhwUfUzTfGtI/fJ/76m7lj+kUjIoxJiWadJf4u779rc/hs2z5+edZwEhrp4jHGtJ3fJ/51WYWEhQQxLKk3AGP6xbAlr4SyqhofR2aaUlJRzW/f8YzNv/S4Ab4Oxxi/4/+JP7uIkclRhDrzoo5NiaZOISOn2MeRmaY8tmgL+QcquW/OaBubb4wX+HXir6tT0rOLGZPy7VR6Y5wbvNbP3zVl5BTzvDM2f4LdgDfGK3wyqkdEdgAlQC1Qo6qTvXGe7ftKOVBZczDZg+fx+sTe4dbP3wXV1Sl3vpVOVESIjc03xot8OZzzNFXd2/JmbVef3Mf2O3Ty7LH9olmTVUh5VS0AIcFysCuoIVX1q8JbqgrQqjbV/44AwkKC2tz14uZ3+fqqLFbsLOCBC8ba2HxjvMivu3rWZRcRERrEkIRDp90b2y+GbfmlpN21gLS7FnDs7xaxJbfkkG2+3lXAcb9fzLLN+Z0ZstfU1inTH1nKQx9scr3P7W+uO/g7SrtrATMeW3bwy6M11uwu5LjfL2bJprwmtyksq+L+9zcysX8M353Ur9XnMMa456vEr8AHIrJSRK5pbAMRuUZEVojIivz8tiXfaWmJ3DJjBCGHXc1fNmUAd5yTxi0zR/DrGSOoU7hzfvrBpFZTW8ftb6aTV1LJHW+lU1Fd29jhu5XVuwvZll/KX5ZmsnFPyze2P926l5eW7+LcscncMnMEF0zsx5a8A6Rnt+6meG2dcvtb6w7+Lhv+BdHQg//bRGFZFfedN5ogu6FrjFf5KvGfqKoTgZnAdSJy8uEbqOo8VZ2sqpMTEhLadJITUuO54sRBR6yP6xXGj6YO5ienpHLtqan8asZwvsjcz9trvgHgn1/sZENOMVecMJBd+8t4esm2Np2/K1mUkUtwkBAVEcKdb6U3e+VeVVPHXfPTGdCnJw9dOI6fnJLKbWePIEhgYUZuq8770vKdpGd7fpdZBeU8tWTrEdus2V3Iy1/u4gfHD2RU3+hGjmKM6Ug+Sfyq+o3zMw94EzjWF3HUu/iY/ozrF81972SwLf8AD3+wmalD47l71khmj+vL00u3sXNfqS9DbLfFGbkcOzCOW2em8dWOAl5fld3ktn/7JJNt+aX8Zvaog8Xs+kSGM7F/LItbkfjzSyp58H+bOGmI53d53vi+PLM0k+17v/1d1tYpd85PJz4ynJvPHNb2BhpjXOv0xC8ivUSkd/174EwgvbPjaCg4SLjvvNHsK63k/D9/SmVNHffOGY2IcMc5aYQFB3HX/PVt6t/uCnbtK2Nz7gGmj0ziu5P6MbF/DPe/l0FRWfUR22YVlPHE4q2cNSqJ04YnHvLZ9JFJrP+mmG8K3c1dfP/7GVRU13LPHM8UiLedk0Z4SBB3NehWe+XLXazNKuKOc9KIirD5ao3pDL4Y1ZMEvOmM8AgBXlbVBT6I4xBj+8Vw6XH9+ecXu7j+9CEMiu8FQGJUBDedMYz73tnA8DsWgECwCI9+bzwzRh/l9biWbs7np/9cSXWd+y+d8JAgHvzuWGaMTgY83TwA09MSCXK+5GY98QmTfrvwiP702jr1fNHNGnXEcaenJfGH9zeyOCOXuccPbDTWG179mjKnH7+qpo7rTksl1bm5ntg7gpvPHMY9//32d1ldW8fxg/swe1xf1+0zxrRPpyd+Vc0ExnX2ed24ZWYao/tGc96EQyfNvvz4AQQL7CmuBOA/K7N4fVVWpyT+N1ZlERIcxNzj+7ve56ONedzxVjrHp8YT3SOURRm5DE2MZEAfz5fZqL7RzJs7mRU7Cxrd/5RhCaQ0Ml9takIvBvbpyaKMvCMSf1lVDbe9sY7YnmFcfIzn9xLXK5QfHLbd3CkDCBIhp6gCgLBg4dIpA/xq2KwxXZ3fl2VujcjwEC4+9sgEGxIcdMhN4vKqGv61YrfXJ3Sprq3jo415nDXqKG6ZOcL1fueOTWb2k5/wyAebuPnM4Xy5fT9Xnzz4kG2mj0xi+sikVsUjIkxPS+LFz3dyoLKGyPBv//k8+eFWsgvL+fdPjueYgXFNHiMkOMhKLBvjY349jt9bpqUlUVFdx6dbvfr8GV/t2E9xRQ3T0lqXoEenRDN3ygD+8cVOnvpoKzV1yvS0xJZ3dGFaWhJVtXV83OD5hvq6+RdM7Nds0jfGdA2W+NvguMFxRIaHsCij6QeSOsLijDzCQoKYOjS+1fvefOZw4nqF88yyTPr0CmP80R1T92bywFin+8jT9vq6+T1Cg7n1bPd/lRhjfMe6etogPCSYU4YlsDgjl7o6zwNHNbV1XPvSKlbvLmxx/9OHJ/KHC8Yc0q/92ordvL8uh7/MnUR4SDCqyqKMXE5M7UOv8Nb/Z4ruEcptZ4/g5tfWcNqIxA6rchkaHMSpwxOYvzqbZVvyUVX2HqjivjmjiI+0uvnGdAeW+NtoWloi767LYV12EeOOjuH5z3awcEMu545NpnczwxLzSyr414rdTB0Wz7ljPSNZsgvLuXv+esqra5m3NJPrpw1la94Bdu4r4+qpg5s8VkvOn5DCvgNVnN5B3Tz1rjttCJHhIdQPNOoX24PvW918Y7oNS/xtdNrwRILE82DUUdERPLZoC6cMS+CJSyY0O0KlpraO2U9+yn3vbODU4YlEhodw3383oCgnpPbhyY+2ct6ElINdKdPakbRF5Iibuh1hWFJvfnf+mA4/rjGmc1gffxvF9gpj8sA4Fmbk8dt3M6iqreOe2aNaHJYYEhzEb88fTW5xJX9atJklm/JYsH4P158+lIcvGkdwkHDPf9ezKCOXMSnRJEcfOazSGGPawxJ/O0xPSyQjp5j/rvmGa09JZaDz0FdLJvaP5eJjjubZT3dw6xvrGBzfix9NHURydA9unD6URRl5rNxZ0K6rfWOMaYol/naY7gyz7B/Xk2tPTW3Vvr+aMYLeESHkFFVw75zRhId4nge48sRBDEuKPOT4xhjTkayPvx0GJ0Ry0/RhnDwsvtUPcsX1CuOp709kc24JJzUYrhkaHMSj3xvPe+tyGNU3qqNDNsYYpDsUHps8ebKuWLHC12EYY0y3IiIrG5va1rp6jDEmwFjiN8aYAGOJ3xhjAowlfmOMCTCW+I0xJsBY4jfGmABjid8YYwKMJX5jjAkw3eIBLhHJB3a2Ypd4wLvTY3VNgdjuQGwzBGa7A7HN0L52D1DVhMNXdovE31oisqKxp9X8XSC2OxDbDIHZ7kBsM3in3dbVY4wxAcYSvzHGBBh/TfzzfB2AjwRiuwOxzRCY7Q7ENoMX2u2XffzGGGOa5q9X/MYYY5pgid8YYwKM3yV+EZkhIptEZKuI3OLreLxBRI4WkY9EJENE1ovIDc76OBFZKCJbnJ+xvo61o4lIsIh8LSLvOMuB0OYYEfmPiGx0/psf7+/tFpGbnH/b6SLyiohE+GObReRZEckTkfQG65psp4jc6uS2TSJyVlvP61eJX0SCgT8DM4GRwCUiMtK3UXlFDfALVU0DpgDXOe28BVisqkOBxc6yv7kByGiwHAht/hOwQFVHAOPwtN9v2y0iKcDPgcmqOhoIBi7GP9v8PDDjsHWNttP5f/xiYJSzz1NOzms1v0r8wLHAVlXNVNUq4FVgjo9j6nCqmqOqq5z3JXgSQQqetr7gbPYCcJ5PAvQSEekHnAP8rcFqf29zFHAy8HcAVa1S1UL8vN145gPvISIhQE/gG/ywzaq6DNh/2Oqm2jkHeFVVK1V1O7AVT85rNX9L/CnA7gbLWc46vyUiA4EJwHIgSVVzwPPlACT6MDRveAz4FVDXYJ2/t3kwkA8853Rx/U1EeuHH7VbVbOAhYBeQAxSp6gf4cZsP01Q7Oyy/+Vvil0bW+e14VRGJBF4HblTVYl/H400ici6Qp6orfR1LJwsBJgJPq+oEoBT/6OJoktOnPQcYBPQFeonIZb6NqkvosPzmb4k/Czi6wXI/PH8i+h0RCcWT9F9S1Tec1bkikux8ngzk+So+LzgRmC0iO/B04Z0uIv/Ev9sMnn/TWaq63Fn+D54vAn9u93Rgu6rmq2o18AZwAv7d5oaaameH5Td/S/xfAUNFZJCIhOG5EfK2j2PqcCIiePp8M1T1kQYfvQ1c7ry/HJjf2bF5i6reqqr9VHUgnv+uH6rqZfhxmwFUdQ+wW0SGO6umARvw73bvAqaISE/n3/o0PPex/LnNDTXVzreBi0UkXEQGAUOBL9t0BlX1qxdwNrAZ2Abc7ut4vNTGk/D8ibcWWO28zgb64BkFsMX5GefrWL3U/lOBd5z3ft9mYDywwvnv/RYQ6+/tBu4BNgLpwD+AcH9sM/AKnvsY1Xiu6K9qrp3A7U5u2wTMbOt5rWSDMcYEGH/r6jHGGNMCS/zGGBNgLPEbY0yAscRvjDEBxhK/McYEGEv8JiCJyMCGFRG76jGN8QZL/MYYE2As8ZuAJyKDnQJoxxy2/l8icnaD5edF5ALnyv5jEVnlvE5o5JhXiMiTDZbfEZFTnfdnisjnzr7/dmouGdNpLPGbgOaUQngduFJVvzrs41eB7znbheEpHfAentopZ6jqROfzx1txvnjgDmC6s/8K4Ob2tsOY1gjxdQDG+FACnjooF6jq+kY+fx94XETC8Ux8sUxVy0UkGnhSRMYDtcCwVpxzCp5Jgj71lKEhDPi87U0wpvUs8ZtAVoSnvvmJwBGJX1UrRGQJcBaeK/tXnI9uAnLxzIYVBFQ0cuwaDv2LOsL5KcBCVb2kA+I3pk2sq8cEsio8sxv9QES+38Q2rwJXAlOB/znrooEcVa0D5uKZGvBwO4DxIhIkIkfz7UxJXwAnisgQAKcCZWv+YjCm3Szxm4CmqqXAucBNItLYNJ0f4Jn6cJF6pvMEeAq4XES+wNPNU9rIfp8C24F1eGaTqp8qMx+4AnhFRNbi+SIY0WENMsYFq85pjDEBxq74jTEmwFjiN8aYAGOJ3xhjAowlfmOMCTCW+I0xJsBY4jfGmABjid8YYwLM/wNQrg48oraWTgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here is the pseudo code:\n",
    "# list_ks = 1,2,...,100\n",
    "# err_ks = 1D array of length 100\n",
    "# for k in list_ks:\n",
    "#   err_ks[k-1] = cross_validation under k \n",
    "# best_k = argmin(err_ks)+1\n",
    "# plot err_ks versus list_ks\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "list_ks = list(range(1,101))\n",
    "err_ks = [0]*100\n",
    "fold_size = 10\n",
    "\n",
    "for k in list_ks:\n",
    "    err_ks[k-1] = cross_validation(knn_predict, X_train, Y_train, fold_size, k)\n",
    "best_k = np.argmin(err_ks)+1\n",
    "print(\"best k:\", best_k)\n",
    "\n",
    "plt.plot(list_ks, err_ks)\n",
    "plt.xlabel('k value')\n",
    "plt.ylabel('cross validation error rate')\n",
    "plt.show()\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EysI5YBT3xJK"
   },
   "source": [
    "### 3.2 Evaluation on test set **(8 points)**{-}\n",
    "Since we have found the best hyperparameters for kNN classifier, it's time to evaluate this method on test data.\n",
    "\n",
    "**Task (8 points):** Report the classification error of kNN on test data, where $k$ is the optimal one from Section 3.1 (break tie arbitrarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "qJVXqpO43xJL",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test error is  3.3333333333333335 %\n"
     ]
    }
   ],
   "source": [
    "# Here is the pseudo code:\n",
    "# y_pred = knn_predict on X_test using X_train, Y_train, and best_k\n",
    "# use compute_error_rate to compute the error of y_pred compared with Y_test\n",
    "# Print the error rate with a line like 'The test error is x.y%'\n",
    "\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "#predict with knn_predict \n",
    "y_pred = knn_predict(X_test, X_train, Y_train, best_k)\n",
    "# compute and print test error\n",
    "err_test = compute_error_rate(y_pred, Y_test)\n",
    "print('The test error is ', err_test,'%')\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PD58RUy73xJN"
   },
   "source": [
    "### 3.3 F-score measurement **(20 points)**{-}\n",
    "So far we have mainly used classification accuracy to evaluate the performance of our model. As a performance measure, accuracy is inappropriate for imbalanced classification problems.\n",
    "An alternative is the F-score metrics.\n",
    "\n",
    "**Tasks**\n",
    "* **(10 points)** Implement the computation of the confusion matrix on test set using `y_test` and the prediction `y_pred` from Section 3.2.  You can compare your result with the one computed by [sklearn.metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to ensure your implementation is correct.\n",
    "* **(4 points)** Report the precision, recall, and F1-score for each class by using the built-in functions from [sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html).\n",
    "* **(6 points)** Write your own code to compute the F1-score for the three classes, and make sure they match the f1-score column of the sklearn result.\n",
    "\n",
    "**Hint:**  All definitions of confusion matrix, precision, recall, and F1-score can be found in the slides for Chapter19: DESGN AND ANALYSS OF MACHNE LEARNNG EXPERMENTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "E6myu3yF3xJN",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0  6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       1.00      1.00      1.00        11\n",
      "     class 1       1.00      0.92      0.96        13\n",
      "     class 2       0.86      1.00      0.92         6\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.95      0.97      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "[1.0, 0.9600000000000001, 0.923076923076923]\n"
     ]
    }
   ],
   "source": [
    "nclass = len(np.unique(Y_test))  # should be 3. Just be more adaptive to data.\n",
    "cm = np.zeros((nclass, nclass), dtype=int)  # confusion matrix is integer valued\n",
    "\n",
    "# Here is the pseudo code for Task 1: \n",
    "# for t = 0...nte-1  # nte is the number of test examples\n",
    "#    cm[c1, c2] += 1  # c1 and c2 corresponds to the class of the t-th test example\n",
    "#                     # according to Y_test and y_pred, respectively\n",
    "#\n",
    "# Here is the pseudo code for Task 3:\n",
    "# Well, please consult the textbook, as I really hope you can do it yourself,\n",
    "# especially when the right answer is provided by sklearn for comparison\n",
    "\n",
    "\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "#Task 1\n",
    "\n",
    "for t in range(len(Y_test)):\n",
    "    c1 = Y_test[t]\n",
    "    c2 = y_pred[t]\n",
    "    cm[c1, c2] += 1\n",
    "print(cm)\n",
    "\n",
    "#Task 2\n",
    "from sklearn.metrics import classification_report\n",
    "cr = classification_report(Y_test, y_pred)\n",
    "\n",
    "print(cr)\n",
    "\n",
    "#Task 3\n",
    "f1 = [0]*nclass\n",
    "for n in range(nclass):\n",
    "    #calculate precision\n",
    "    precision = cm[n,n] / cm.sum(axis=0)[n] \n",
    "    #calculate recall\n",
    "    recall = cm[n,n] / sum(cm[n])  \n",
    "    #calculate f1\n",
    "    f1[n] = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f1)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w0TZVbzIQ3Z"
   },
   "source": [
    "# Submission Instruction {-}\n",
    "\n",
    "You're almost done! Take the following steps to finally submit your work.\n",
    "\n",
    "1. After executing all commands and completing this notebook, save your `Lab_3.ipynb` as a PDF file, named as `X_Y_UIN.pdf`, where `X` is your first name, `Y` is your last name, and `UIN` is your UIN. Make sure that your PDF file includes all parts of your solution, including the plots. \n",
    "\n",
    "> * Print out all unit test case results before printing the notebook into a PDF.\n",
    "* If you use Colab, open this notebook in Chrome.  Then File -> Print -> set Destination to \"Save as PDF\".  If the web page freezes when printing, close Chrome and reopen the page. If Chrome doesn't work, try Firefox.\n",
    "* If you are working on your own computer, we recommend using the browser (not jupyter) for saving the PDF. For Chrome on a Mac, this is under *File->Print...->Open PDF in Preview*. When the PDF opens in Preview, you can use *Save...* to save it.\n",
    "* Sometimes, a figure that appears near the end of a page can get cut.  In this case, try to add some new lines in the preceding code block so that the figure is pushed to the beginning of the next page. Or insert some text blocks.\n",
    "\n",
    "2. Upload `X_Y_UIN.pdf` to Gradescope under `Lab_3_Written`.\n",
    "\n",
    "3. A template of `Lab_3.py` has been provided.  For all functions in `Lab_3.py`, copy the corresponding code snippets you have written into it, excluding the plot code.  **Do NOT** copy any code of plotting figures and do not import **matplotlib**.  This is because the auto-grader cannot work with plotting.  **Do NOT** change the function names.  \n",
    "\n",
    "4. Zip `Lab_3.py` and `Lab_3.ipynb` (**2 files**) into a zip file named `X_Y_UIN.zip`. Suppose the two files are in the folder `Lab_3`.  Then zip up the **two files inside the `Lab_3` folder**.  **Do NOT zip up the folder `Lab_3`** because the auto-grader cannot search inside a folder. Submit this zip file to Gradescope under `Lab_3_Code`. \n",
    "\n",
    "5. The autograder on Gradscope will be open all the time. We designed some simple test cases to help you check wehther your functions are executable. You will see the results of running autograder once you submit your code. Please follow the error messages to debug. Since those simple test cases are designed for debugging, it does not guaranttee your solution will work well on the real dataset. It is your responsibility to make your code logically correct. Since all functions are tested in batch, the autograder might take a few minutes to run after submission.\n",
    "\n",
    "<font color='red'>If you *only* try to get real-time feedback from auto-grader, it will be fine to just upload `Lab_3.py` to `Lab_3_Code`</font>.  However, the final submission for grading should still follow the above point 4.\n",
    "\n",
    "You can submit to Gradescope as often as you like. We will only consider your last submission before the deadline."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
