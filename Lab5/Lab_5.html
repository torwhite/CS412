<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>lab_5</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="Lab_5_files/libs/clipboard/clipboard.min.js"></script>
<script src="Lab_5_files/libs/quarto-html/quarto.js"></script>
<script src="Lab_5_files/libs/quarto-html/popper.min.js"></script>
<script src="Lab_5_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Lab_5_files/libs/quarto-html/anchor.min.js"></script>
<link href="Lab_5_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Lab_5_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Lab_5_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Lab_5_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Lab_5_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<section id="lab-5-naive-bayes-classification" class="level1">
<h1><strong>Lab 5: Naive Bayes Classification</strong></h1>
<p>CS 412</p>
<p><strong><em>This lab can be conducted in groups.</em></strong></p>
<p>In this lab, you will learn how to apply the Naive Bayes model to filter spam SMS messages.</p>
<p><strong><em>Deadline:</em></strong> <strong>23:59 PM, Mar 17</strong>.</p>
<p><font color="red"> Please refer to <code>Lab_Guideline.pdf</code> in the same Google Drive folder as this Jupyter notebook; the guidelines there apply to all the labs.</font></p>
</section>
<section id="naive-bayes-classification" class="level1 unnumbered">
<h1 class="unnumbered"><strong>Naive Bayes Classification</strong></h1>
<p>In this problem, you will implement the Naive Bayes classification method and use it for SMS message classifcation. The SMS dataset <code>SMSSpamCollection</code> has been provided in the assignment folder, which also can be downloaded from the <a href="http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection">UCI Link</a>. In case the repository gets offline occasionally, we have made local copies for the <a href="https://www.cs.uic.edu/~zhangx/teaching/SMSSpamCollection.dat">dataset</a> and <a href="https://www.cs.uic.edu/~zhangx/teaching/readme.txt">readme</a>.</p>
<p>To help you to better understand this algorithm, you are <strong>not</strong> allowed to use any off-the-shelf naive Bayes implementations from third-party libraries. We will implement it with detailed step-by-step instructions.</p>
<section id="recap-of-the-naive-bayes-algorithm" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="recap-of-the-naive-bayes-algorithm">Recap of the Naive Bayes Algorithm</h3>
<p>Naive Bayes classification is a fast and simple classification method. Its efficiency stems from some simplifications we make about the underlying probability distributions, namely, the assumption about the conditional independence of features. Suppose for any class <span class="math inline">\(Y\)</span>, we have a probability distribution over all possible combinations of values for a feature vector <span class="math inline">\(X\)</span>: <span class="math display">\[
P(X|Y).
\]</span> The main idea of Bayesian classification is to reverse the direction of dependence — we want to predict the label based on the features: <span class="math display">\[
P(Y|X).
\]</span> This is made possible by the Bayes theorem: <span class="math display">\[\begin{equation}
P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}. \tag{1}
\end{equation}\]</span></p>
<p>To make it more concrete, let us consider the SMS message classification problem. Ignoring punctuations, each SMS message contains an ordered sequence of <span class="math inline">\(T\)</span> words (case-insensitive) <span class="math inline">\(X = \{X_1, ...,X_T\}\)</span>. That is, <span class="math inline">\(X\)</span> corresponds to an SMS message, and <span class="math inline">\(X_i\)</span> corresponds to the <span class="math inline">\(i\)</span>-th word in it. For each message from the training set, there is a corresponding label <span class="math inline">\(Y\in\{spam, ham\}\)</span>.</p>
<p><strong>Model specification and key assumption.</strong> The conditional distribution can be written as: <span class="math display">\[
P(X|Y) = P(X_1, ..., X_T|Y).
\]</span> Since this conditional probability is intractable, we simplify it in two steps:</p>
<ol type="1">
<li><p><strong>Assume</strong> that all features <span class="math inline">\(X_i\)</span> are independent, conditional on the category <span class="math inline">\(Y\)</span>. This leads to a naive Bayes model which writes formally as <span class="math display">\[\begin{equation}
P(X|Y) = P(X_1, ..., X_T|Y) = \prod_{i=1}^T P(X_i|Y). \tag{2}
\end{equation}\]</span></p></li>
<li><p><strong>Assume</strong> that <span class="math inline">\(P(X_i | Y) = P(X_j|Y)\)</span> for all <span class="math inline">\(i \neq j\)</span>. In other words, given the label <span class="math inline">\(Y\)</span>, the value of the <span class="math inline">\(7\)</span>-th word has the same distribution as the value of the <span class="math inline">\(10\)</span>-th word. Note this is not assumed by default in naive Bayes, and we make this additional assumption to significantly simplify our model. It is often referred to as “tying” the probability <span class="math inline">\(P(X_i|Y)\)</span> over <span class="math inline">\(i\)</span>. As a result, the order of words no longer matters for <span class="math inline">\(P(X|Y)\)</span>, i.e., <span class="math display">\[P(X=\text{'cat is cute'}|Y) \ \ = \ \ P(X=\text{'cute cat is'}|Y) \quad \text{for all } Y.
\]</span></p></li>
</ol>
<p>Plugging Eq (2) into the Bayes theorem in Eq (1), we arrive at <span class="math display">\[
\begin{aligned}
P(Y|X) &amp;= \frac{P(Y) P(X|Y)}{P(X)} = \frac{P(Y)\prod_{i=1}^T P(X_i|Y)}{P(X)} \\
&amp;\propto P(Y)\prod_{i=1}^T P(X_i|Y),
\end{aligned}
\]</span> where <span class="math inline">\(\propto\)</span> denotes proportionality. Since the denominator <span class="math inline">\(P(X)\)</span> does not depend on <span class="math inline">\(Y\)</span>, the prediction probability is proportional to the numerator.</p>
<p><strong>Making predictions.</strong> Naturally, given an SMS message <span class="math inline">\(X\)</span>, we can first compute <span class="math inline">\(P(Y|X)\)</span> for all possible categories <span class="math inline">\(Y\)</span> (in this example, only two categories), and then make predictions by outputting the <span class="math inline">\(Y\)</span> that maximizes the probability. This can be expressed mathmatically as: <span class="math display">\[
\arg\max_Y P(Y)\prod_{i=1}^T P(X_i|Y) = \arg\max_Y \left\{\log P(Y) + \sum_{i=1}^T \log P(X_i|Y) \right\}. \tag{3}
\]</span> If there is a tie, we just break it arbitrarily. Here the logarithm uses natural basis.</p>
<p><strong>Learning the model.</strong> To apply the prediction rule in Eq (3), we need to first figure out (formally termed “estimate”) the value of <span class="math inline">\(P(Y)\)</span> and <span class="math inline">\(P(X_i|Y)\)</span> by using the training data. Recall that since we tie the conditional probabilities <span class="math inline">\(P(X_i|Y)\)</span> across all <span class="math inline">\(i\)</span>, the subscript <span class="math inline">\(i\)</span> can be dropped. However, we still carry it just for clarity.</p>
<p>Firstly, <span class="math inline">\(P(Y=y)\)</span> can be estimated by computing the frequency of category <span class="math inline">\(y\)</span> in the whole training set (<span class="math inline">\(y\)</span> can be either “<span class="math inline">\(spam\)</span>” or “<span class="math inline">\(ham\)</span>”). Here, in order to avoid confusion, we have used the convention that capital letters denote random variables, and lowercase letters denote their possible instantiations.</p>
<p>Secondly, <span class="math inline">\(P(X_i = w |Y=y)\)</span> for a word <span class="math inline">\(w\)</span> (e.g., “cat”) can be estimated by counting the frequency that it appears in the training message set for a given category <span class="math inline">\(y\)</span> (derivation not required in the course): <span class="math display">\[\begin{align}
P(X_i &amp;= w|Y=y)
= \frac{Count(w, y)}{Count(y)}, \ \ where
\tag{4} \\
Count(w, y) &amp;= \text{total number of occurrence of $w$ in all SMS messages of category } y \\
Count(y) &amp;= \text{total number of words appearing in SMS messages of category } y.
\end{align}\]</span></p>
<p><em>Remark 1:</em> If <span class="math inline">\(w\)</span> appears in a single message for 3 times, then it contributes to <span class="math inline">\(Count(w, y)\)</span> by 3, not 1. Similarly, <span class="math inline">\(Count(y)\)</span> indeed equals the total length of all messages in category <span class="math inline">\(y\)</span>.</p>
<p><em>Remark 2:</em> Obviously, the right-hand side of Eq (4) does not depend on <span class="math inline">\(i\)</span>. This is consistent with our previous note that we carry the subscript <span class="math inline">\(i\)</span> in <span class="math inline">\(P(X_i|Y)\)</span> only for clarity, while in fact different <span class="math inline">\(i\)</span> share the same <span class="math inline">\(P(X_i|Y)\)</span>.</p>
<p>For example, suppose there are four messages <span class="math display">\[
\text{{'cat is cute', ham}, {'dog rocks', spam}, {'whatever is is right', ham}, {'hello', spam}.}
\]</span> Then <span class="math inline">\(P(X_i = '\text{is}' | \text{ham}) = 3 / 7\)</span> (<strong>not</strong> <span class="math inline">\(2/7\)</span>), and <span class="math inline">\(P(X_i = '\text{is}' | \text{spam}) = 0 / 3\)</span>.</p>
<p>Quiz (no need to submit): compute <span class="math inline">\(P(X_i = w | \text{ham})\)</span>, for $w = $ ‘cat’, ‘is’, ‘cute’, ‘whatever’, and ‘right’. Check if their sum is 1. Now appreciate why in Remark 1, a word appearing for mulitple times in a single message should be counted multiple times.</p>
<p>You may have noticed that any word <span class="math inline">\(w\)</span> with <span class="math inline">\(Count(w,y)=0\)</span> leads to <span class="math inline">\(P(X_i = w|Y=y) = 0\)</span>. As a result, by Eq (2), any message <span class="math inline">\(x\)</span> has conditional probably <span class="math inline">\(P(X = x |Y=y) = 0\)</span> if <span class="math inline">\(w\)</span> appears in <span class="math inline">\(x\)</span>. Such a “veto” is not favorable, and can create significant problems when a word in the test data has never appeared in the training data (think why?). To bypass this issue, we can add pseudo-count, a.k.a additive smoothing: <span class="math display">\[
\hat{P}(X_i = w|Y=y) = \frac{Count(w, y) + \alpha}{Count(y) + N\alpha}, \tag{5}
\]</span> where <span class="math inline">\(\alpha\)</span> is a smoothing parameter. <span class="math inline">\(\alpha=0\)</span> corresponds to no smoothing. In our experiment, let us set <span class="math inline">\(\alpha = 1.0\)</span>. <span class="math inline">\(N\)</span> denotes the number of distinct words in the vocabulary, and let us set <span class="math inline">\(N = 20,000\)</span> in this lab.</p>
<p>Now Let’s start with data preprocessing.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set up code for this experiment</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="data-preprocessing-25-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="data-preprocessing-25-points">1. Data Preprocessing <strong>(25 points)</strong></h3>
<p>We will use <code>pandas</code> to import the dataset. Since <code>SMSSpamCollection</code> separates labels and text content in each message by a tab, we will use ’ as the value for the <code>sep</code> argument and read raw data into a pandas dataframe. As a result, we store labels and SMS messages into two columns. To facilitate the subsequent steps, we also rename the columns by passing a list <code>['label', 'sms_message']</code> to the <code>names</code> argument of the <code>read_table()</code> method.</p>
<p>Let us print the first five rows of the dataframe to get a basic understanding of the dataset.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the dataset to the server</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the data using the read_csv() method from pandas</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> shutil</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">'https://www.cs.uic.edu/~zhangx/teaching/SMSSpamCollection.dat'</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>file_name <span class="op">=</span> <span class="st">'SMSSpamCollection.dat'</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> urllib.request.urlopen(url) <span class="im">as</span> response, <span class="bu">open</span>(file_name, <span class="st">'wb'</span>) <span class="im">as</span> out_file:</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    shutil.copyfileobj(response, out_file)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(file_name,</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>                    sep<span class="op">=</span><span class="st">'</span><span class="ch">\t</span><span class="st">'</span>,</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>                    header<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>                    names<span class="op">=</span>[<span class="st">'label'</span>, <span class="st">'sms_message'</span>])</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>label</th>
      <th>sms_message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ham</td>
      <td>Go until jurong point, crazy.. Available only ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ham</td>
      <td>Ok lar... Joking wif u oni...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>spam</td>
      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ham</td>
      <td>U dun say so early hor... U c already then say...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ham</td>
      <td>Nah I don't think he goes to usf, he lives aro...</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<section id="step-1-convert-string-labels-to-numerical-labels-not-for-grading" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="step-1-convert-string-labels-to-numerical-labels-not-for-grading">Step 1: Convert string labels to numerical labels (not for grading)</h4>
<p>As we can see, there are 2 columns. The first column, which is named <code>label</code>, takes two values <code>spam</code> (the message is spam) and <code>ham</code> (the message is not spam). The second column is the text content of the SMS message that is being classified. It is a string in which words are separated by space.</p>
<p>Note that the string-typed labels are unwieldy for calculating performance metrices, e.g., when calculating precision and recall scores. Hence, let’s convert the lables to binary variables, 0 for <code>ham</code> and 1 for <code>spam</code>.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the next line only once after running the previous code block</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Running it more than once will turn the labels into NaN</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">#df['label'] = df.label.map({'ham':0, 'spam':1})</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>df.head()</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>label</th>
      <th>sms_message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>Go until jurong point, crazy.. Available only ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>Ok lar... Joking wif u oni...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>U dun say so early hor... U c already then say...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>Nah I don't think he goes to usf, he lives aro...</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</section>
<section id="step-2-bag-of-words-16-points" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="step-2-bag-of-words-16-points">Step 2: Bag of words <strong>(16 points)</strong></h4>
<p>What we have in our dataset is a large collection of text data (5,572 rows/messages). Most ML algorithms rely on numerical data to be fed into them as input, but SMS messages are usually text heavy.</p>
<p>To address this issue, we would like to introduce the concept of Bag of Words (BoW), which is designed for problems with a ‘bag of words’ or a collection of text data. The basic idea is to count the frequency of the words in the text. It is important to note that BoW treats each word individually, ignoring the order in which the words occur.</p>
<p>To count the frequency of the words in text, usually we need to process the input text data in four steps:</p>
<ul>
<li>Convert all strings into their lower case form</li>
<li>Removing all punctuations</li>
<li>Tokenization, i.e., split a sentence into individual words</li>
<li>Count frequencies</li>
</ul>
<p>Once this has been done, we are supposed to obtain a vocabulary dictionary with frequencies of each words for the given text data.</p>
<div class="cell" data-tags="[]" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> count_frequency(documents):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">    count occurrence of each word in the document set.</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">    - documents: list, each entity is a string type SMS message</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Outputs:</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">    - frequency: a dictionary. The key is the unique words, and the value is the number of occurrences of the word</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: covert all strings into their lower case form</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    lower_case_doc <span class="op">=</span> []</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> documents:</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        lower_case_doc.append(s.lower())</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: remove all punctuations</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    no_punc_doc <span class="op">=</span> []</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> lower_case_doc:</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        no_punc_doc.append(s.translate(<span class="bu">str</span>.maketrans(<span class="st">''</span>,<span class="st">''</span>,string.punctuation)))</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>     </span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: tokenize a sentence, i.e., split a sentence into individual words </span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># using a delimiter. The delimiter specifies what character we will use to identify the beginning </span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and the end of a word.</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    words_doc <span class="op">=</span> []</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> no_punc_doc:</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        words_doc.append(s.split(<span class="st">' '</span>))</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 4: count frequencies. To count the occurrence of each word in the document set. </span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We can use the `Counter` method from the Python `collections` library for this purpose. </span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># `Counter` counts the occurrence of each item in the list and returns a dictionary with </span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the key as the item being counted and the corresponding value being the count of that item in the list. </span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    all_words <span class="op">=</span> []</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> words_doc:</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        all_words.extend(s)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    frequency <span class="op">=</span> Counter(all_words)</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># *****</span><span class="re">END</span><span class="co"> OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> frequency</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Unit test case:</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a><span class="co"># documents = ['Hello, how are you!', </span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a><span class="co">#              'Win money, win from home.',</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a><span class="co">#              'Call me now.',</span></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="co">#              'Hello, Call hello you tomorrow?']</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a><span class="co"># sample outputs:</span></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Counter({'hello': 3, 'you': 2, 'win': 2, 'call': 2, 'how': 1, 'are': 1, 'money': 1, 'from': 1, 'home': 1,</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a><span class="co"># 'me': 1, 'now': 1, 'tomorrow': 1})</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>documents <span class="op">=</span> [<span class="st">'Hello, how are you!'</span>,</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Win money, win from home.'</span>,</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Call me now.'</span>,</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Hello, Call hello you tomorrow?'</span>]</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>freq <span class="op">=</span> count_frequency(documents)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="step-3-create-training-and-test-sets-9-points" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="step-3-create-training-and-test-sets-9-points">Step 3: Create training and test sets <strong>(9 points)</strong></h4>
<p>We will partition the <code>SMSSpamCollection</code> dataset into training and test sets so that we can analyze the model’s performance on data it has not witnessed during training. In Lab 3, we have implemented the <code>split_nfold()</code> method from scratch for data partition. In this Lab, we will learn to use the <code>scikit</code> library. <code>scikit</code> is a powerfull tool for machine learning and data mining, providing plenty of well-designed methods for data analysis. We’ll use its <code>train_test_split()</code> method to create training and testing sets. In this experiment, we use 80% data for training and the remaining 20% data for testing. To ensure your results are replicable, you need to set the <code>random_state</code> argument of <code>train_test_split()</code> to <strong>1</strong>.</p>
<p>There is no cross validation in this Lab.</p>
<div class="cell" data-tags="[]" data-execution_count="40">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># learn to read API documentation</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># you can get detailed instructions about this method through this link:</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(df[<span class="st">'sms_message'</span>], df[<span class="st">'label'</span>], test_size<span class="op">=</span><span class="fl">.2</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> X_test.tolist()</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># *****</span><span class="re">END</span><span class="co"> OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'The original dataset contains </span><span class="sc">{</span>df<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> examples in total.'</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'The training set contains </span><span class="sc">{</span>X_train<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> examples.'</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'The testing set contains </span><span class="sc">{</span><span class="bu">len</span>(X_test)<span class="sc">}</span><span class="ss"> examples.'</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The original dataset contains 5572 examples in total.
The training set contains 4457 examples.
The testing set contains 1115 examples.</code></pre>
</div>
</div>
</section>
</section>
<section id="implementing-naive-bayes-method-from-scratch-75-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="implementing-naive-bayes-method-from-scratch-75-points">2. Implementing Naive Bayes method from scratch <strong>(75 points)</strong></h3>
<section id="step-1-training-the-naive-bayes-model-25-points" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="step-1-training-the-naive-bayes-model-25-points">Step 1: training the Naive Bayes Model <strong>(25 points)</strong></h4>
<p>Now that we know what Naive Bayes is, we can take a closer look at how to calculate the posterior probability <span class="math display">\[
P(Y|X) \propto P(Y)\prod_{i=1}^T P(X_i|Y).
\]</span></p>
<p>The goal of training is to learn the prior and conditional probability from data. The calculation of the prior <span class="math inline">\(P(Y=y)\)</span> is straightforward. It can be estimated via the frequency of messages in the training set that belong to class <span class="math inline">\(y\)</span>, e.g., <span class="math display">\[
P(Y=spam) = \frac{\# \text{training messages in the spam category}}{\# \text{training messages}}.
\]</span></p>
<p>The conditional probability given the class label — <span class="math inline">\(P(X_i|Y)\)</span> — can also be estimated from the data by using Eq (5). As we assumed above, it is indeed independent of <span class="math inline">\(i\)</span> (i.e., shared by all <span class="math inline">\(i\)</span>). We will leave it to you to translate Eq (5) into a concrete computation scheme. No pseudo-code is provided because by now you should be able to do it. However, do make sure that your implementation complies with the input and output data types as specified in the code.</p>
<p><strong>Hint</strong>: - <code>count_frequency()</code> can be useful for computing the conditional probability. - You need to apply the <strong>pseudo-count</strong> trick to handle unseen words when computing the conditional probability for testing data. It is natual to ask how to carry the quantity Count(y) in Equation 5 through the variables that are passed through the functions. To address this issue, we can create a ‘dummy’ entry in cond_prob (the variable returned by the function conditional_prob), and set its value to <span class="math inline">\(\hat{P}(X_i = dummy | Y = y) = \alpha / (Count(y) + N \alpha)\)</span>. Then all the words that appear in test data but not in training data can directly use that entry. Please do not use other key names than ‘dummy’, because the auto-grader follows this protocal.</p>
<div class="cell" data-tags="[]" data-execution_count="56">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_NB_model(X_train, y_train):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">    training a naive bayes model from the training data.</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">    - X_train: an array of shape (num_train,) which stores SMS messages. each entity is a string type SMS message</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">    - y_train: an array of shape (num_train,). the ground true label for each training data.</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Output:</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">    - prior: a dictionary, whose key is the class label, and value is the prior probability.</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co">    - conditional: a dictionary whose key is the class label y, and value is another dictionary.</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co">                   In the latter dictionary, the key is word w, and the value is the</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co">                   conditional probability P(X_i = w | y).</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># To make your code more readable, you can implement some auxiliary functions</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># such as `prior_prob` and `conditional_prob` outside of this train_NB_model function</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute the prior probability</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    prior <span class="op">=</span> prior_prob(y_train)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute the conditional probability</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    conditional <span class="op">=</span> conditional_prob(X_train, y_train)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> prior, conditional</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Start your auxiliary functions</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_smooth(count_x, count_all, alpha<span class="op">=</span><span class="fl">1.0</span>, N<span class="op">=</span><span class="dv">20000</span>):</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="co">    compute the conditional probability for a specific word</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a><span class="co">    - count_x: the number of occurrence of the word</span></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a><span class="co">    - count_all: the total number of words</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a><span class="co">    - alpha: smoothing parameter</span></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="co">    - N: the number of different values of feature x</span></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a><span class="co">    Outputs:</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a><span class="co">    - prob: conditional probability</span></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (count_x <span class="op">+</span> alpha) <span class="op">/</span> (count_all <span class="op">+</span> N<span class="op">*</span>alpha)</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prior_prob(y_train):</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a><span class="co">    compute the prior probability</span></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a><span class="co">    - y_train: an array that stores ground true label for training data</span></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a><span class="co">    Outputs:</span></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a><span class="co">    - prior: a dictionary. key is the class label, value is the prior probability.</span></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>    prior <span class="op">=</span> {}</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>    num_train <span class="op">=</span> <span class="bu">len</span>(y_train)</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> np.unique(y_train)</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(labels)):</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>        prior[i] <span class="op">=</span> <span class="bu">len</span>(y_train[y_train <span class="op">==</span> i])<span class="op">/</span>num_train</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># *****</span><span class="re">END</span><span class="co"> OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****   </span></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> prior</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conditional_prob(X_train, y_train):</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a><span class="co">    compute the conditional probability for a document set</span></span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a><span class="co">    - X_train: an array of shape (num_train,) which stores SMS messages. each entity is a string type SMS message</span></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a><span class="co">    - y_train: an array of shape (num_train,). the ground true label for each training data.</span></span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a><span class="co">    Ouputs:</span></span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a><span class="co">    - cond_prob: a dictionary. key is the class label, value is a dictionary in which the key is word, the value is the conditional probability of feature x_i given y.</span></span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># docs with label '0' to ham, '1' to spam list</span></span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> np.unique(y_train)</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>    X_train <span class="op">=</span> pd.Series(X_train)</span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>    y_train <span class="op">=</span> pd.Series(y_train)</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>    num_train <span class="op">=</span> <span class="bu">len</span>(X_train)</span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>    <span class="co">#FIXME add general case for creating the nested dict</span></span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a>    cond_prob <span class="op">=</span> {<span class="dv">0</span>: {} , <span class="dv">1</span>: {}} </span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>    <span class="co"># empty list with training examples of label 0</span></span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a>    ham <span class="op">=</span> []</span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a>    <span class="co"># empty list with training examples of label 1</span></span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a>    spam <span class="op">=</span> []</span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">FIXME</span><span class="co"> general case</span></span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, v <span class="kw">in</span> X_train.items():</span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> y_train[i] <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a>            ham.append(X_train[i])</span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a>            spam.append(X_train[i])</span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a>     </span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute frequency in docs</span></span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a>    freq_ham <span class="op">=</span> count_frequency(ham)</span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a>    freq_spam <span class="op">=</span> count_frequency(spam)</span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate conditional probability</span></span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a>    <span class="co"># for ham</span></span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a>    count_ham <span class="op">=</span> <span class="bu">len</span>(freq_ham) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add 0 labels</span></span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> key <span class="kw">in</span> freq_ham:</span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a>        count_x <span class="op">=</span> freq_ham[key]</span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a>        cond_prob[<span class="dv">0</span>][key] <span class="op">=</span> add_smooth(count_x, count_ham)</span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add dummy case for ham </span></span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a>    cond_prob[<span class="dv">0</span>][<span class="st">'dummy'</span>] <span class="op">=</span> add_smooth(<span class="dv">0</span>, count_ham)</span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a>    <span class="co"># for spam</span></span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a>    count_spam <span class="op">=</span> <span class="bu">len</span>(freq_spam) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add 1 labels</span></span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> key <span class="kw">in</span> freq_spam:</span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a>        count_x <span class="op">=</span> freq_spam[key]</span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a>        cond_prob[<span class="dv">1</span>][key] <span class="op">=</span> add_smooth(count_x, count_spam)</span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a>                                    </span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add dummy case for spam </span></span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a>    cond_prob[<span class="dv">1</span>][<span class="st">'dummy'</span>] <span class="op">=</span> add_smooth(<span class="dv">0</span>, count_spam)</span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a>    <span class="co"># *****</span><span class="re">END</span><span class="co"> OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****   </span></span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cond_prob</span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a> <span class="co">#unit test case:</span></span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a>x_train <span class="op">=</span> [<span class="st">'Hello, how are you!'</span>,</span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Win money, win from home.'</span>,</span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a>           <span class="st">'Call me now.'</span>,</span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a>           <span class="st">'Hello, Call hello you tomorrow?'</span>]</span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a><span class="co">#y_train = np.array([0,1,1,0,1])</span></span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a><span class="co"># sample outputs:</span></span>
<span id="cb7-130"><a href="#cb7-130" aria-hidden="true" tabindex="-1"></a><span class="co"># prior: {0: 0.5, 1: 0.5}</span></span>
<span id="cb7-131"><a href="#cb7-131" aria-hidden="true" tabindex="-1"></a><span class="co"># conditional: {0: {'hello': 0.0001999100404817832, 'how': 9.99550202408916e-05, </span></span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a><span class="co">#                   'are': 9.99550202408916e-05, 'you': 0.0001499325303613374, </span></span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a><span class="co">#                   'call': 9.99550202408916e-05, 'tomorrow': 9.99550202408916e-05,</span></span>
<span id="cb7-134"><a href="#cb7-134" aria-hidden="true" tabindex="-1"></a><span class="co">#                   'dummy': 4.99775101204458e-05}, </span></span>
<span id="cb7-135"><a href="#cb7-135" aria-hidden="true" tabindex="-1"></a><span class="co">#               1: {'win': 0.00014994002399040384, 'money': 9.996001599360256e-05, </span></span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a><span class="co">#                   'from': 9.996001599360256e-05, 'home': 9.996001599360256e-05, </span></span>
<span id="cb7-137"><a href="#cb7-137" aria-hidden="true" tabindex="-1"></a><span class="co">#                   'call': 9.996001599360256e-05, 'me': 9.996001599360256e-05, </span></span>
<span id="cb7-138"><a href="#cb7-138" aria-hidden="true" tabindex="-1"></a><span class="co">#                   'now': 9.996001599360256e-05}, 'dummy': 4.998000799680128e-05}}</span></span>
<span id="cb7-139"><a href="#cb7-139" aria-hidden="true" tabindex="-1"></a><span class="co">#x_train = np.array(['Hello, how are you!',</span></span>
<span id="cb7-140"><a href="#cb7-140" aria-hidden="true" tabindex="-1"></a><span class="co">#            'Win money, win from home.',</span></span>
<span id="cb7-141"><a href="#cb7-141" aria-hidden="true" tabindex="-1"></a> <span class="co">#           'Call me now.',</span></span>
<span id="cb7-142"><a href="#cb7-142" aria-hidden="true" tabindex="-1"></a>  <span class="co">#          'Hello, Call hello you tomorrow?'])</span></span>
<span id="cb7-143"><a href="#cb7-143" aria-hidden="true" tabindex="-1"></a>y_train_mini <span class="op">=</span> np.array([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>])</span>
<span id="cb7-144"><a href="#cb7-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-145"><a href="#cb7-145" aria-hidden="true" tabindex="-1"></a>prior <span class="op">=</span> prior_prob(y_train_mini)</span>
<span id="cb7-146"><a href="#cb7-146" aria-hidden="true" tabindex="-1"></a>cond_prob <span class="op">=</span> conditional_prob(x_train, y_train_mini)</span>
<span id="cb7-147"><a href="#cb7-147" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prior, cond_prob)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{0: 0.5, 1: 0.5} {0: {'hello': 0.000199930024491428, 'how': 9.9965012245714e-05, 'are': 9.9965012245714e-05, 'you': 0.000149947518368571, 'call': 9.9965012245714e-05, 'tomorrow': 9.9965012245714e-05, 'dummy': 4.9982506122857e-05}, 1: {'win': 0.00014994002399040384, 'money': 9.996001599360256e-05, 'from': 9.996001599360256e-05, 'home': 9.996001599360256e-05, 'call': 9.996001599360256e-05, 'me': 9.996001599360256e-05, 'now': 9.996001599360256e-05, 'dummy': 4.998000799680128e-05}}</code></pre>
</div>
</div>
</section>
<section id="step-2-predict-label-for-test-data-25-points" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="step-2-predict-label-for-test-data-25-points">Step 2: predict label for test data <strong>(25 points)</strong></h4>
<p>Once we have the two models <span class="math inline">\(P(Y)\)</span> and <span class="math inline">\(P(X_i|Y)\)</span> from <em>training</em>, we can use them to predict the label for a given test message. To this end, we need to compute the probability of all possible labels, and then predict the one with maximum probability value: <span class="math display">\[
\arg\max_Y P(Y)\prod_{i=1}^T P(X_i|Y). \tag{6}
\]</span></p>
<p><strong>Avoid numerical underflow with log-trick.</strong> As shown in the above equation, the calculation involves multiplying many probabilities together. Since probabilities lie in <span class="math inline">\((0,1]\)</span>, multiplying many of them together can lead to numerical underflow (i.e., a floating point number close to 0 gets rounded down to 0 by a computer), especially when <span class="math inline">\(T\)</span> is large, i.e., the test message is long.</p>
<p>To overcome this problem, it is common to change the calculation from the product of probabilities to the sum of log probabilities. That is, take the natual logarithm of the right-hand side of Eq (6) as <span class="math display">\[
g_Y(X) = \log P(Y) + \sum_{i=1}^T \log P(X_i|Y). \tag{7}
\]</span> It is much more numerically stable to compute <span class="math inline">\(g_Y(X)\)</span> and to take <span class="math inline">\(\arg\max_Y g_Y(X)\)</span> to find the most likely class label (as the output prediction).</p>
<p>With <span class="math inline">\(g_Y(X)\)</span>, we can easily compute the posterior probability by <span class="math display">\[
P(Y|X) = \frac{\exp (g_Y(X) - m)}{\sum_y \exp (g_y(X)-m)},
\text{ where  }
m = \max_y g_y(X).
\]</span> In our message classification problem, the summation in the denominator is just over <code>positive</code> and <code>negative</code>. Note we subtract by <span class="math inline">\(m\)</span>, which does not change the result because the numerator and denominator cancel. However it is numerically useful because sometimes all <span class="math inline">\(g_Y(X)\)</span> are overly small and can cause numerical underflow inside exponentiation. By subtracting <span class="math inline">\(m\)</span>, <span class="math inline">\(g_Y(X) - m\)</span> will be 0 (properly scaled) for at least one value of <span class="math inline">\(Y\)</span>, and be negative for the other. And even if another <span class="math inline">\(y\)</span> still suffers underflow in exponentiating <span class="math inline">\(g_y(X)-m\)</span>, the posterior probabilities will still be correct.</p>
<p>Again, you are expected to implement Eq (7) and loop over all test examples by yourself with no pseudo-code given.</p>
<div class="cell" data-tags="[]" data-execution_count="43">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> softmax</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_label(X_test, prior_prob, cond_prob):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co">    predict the class labels for the testing set</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">    - X_test: an array of shape (num_test,) which stores test data. </span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">              Each entity is a string type SMS message.</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">    - prior_prob: a dictionary which stores the prior probability for all categories</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">              We previously used "prior_prob" as the name of function.  </span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co">              Here it is used as a dictionary name.  No confusion should arise.</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co">    - cond_prob: a dictionary whose key is the class label y, and value is another dictionary.</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co">                   In the latter dictionary, the key is word w, and the value is the</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co">                   conditional probability P(X_i = w | y).</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Outputs:</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co">    - predict: an array that stores predicted labels</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co">    - test_prob: an array of shape (num_test, num_classes) which stores the posterior probability of each class</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize variables</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    num_test <span class="op">=</span> <span class="bu">len</span>(X_test)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    num_class <span class="op">=</span> <span class="bu">len</span>(prior_prob)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># empty list to hold test probs calculated each index is a class</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    prob <span class="op">=</span> np.empty((num_test, num_class))</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    predict <span class="op">=</span> np.empty(num_test)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>    <span class="co">#word_count = {}</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># predict label</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># iterate over all entries in X_test </span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_test):</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># find word count of specific text example</span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">#word_count = count_frequency([X_test[i]])</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute conditional probability for j classes</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(num_class):</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>            <span class="co"># calculate sum of log probabilities</span></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>            prob[i][j] <span class="op">=</span> compute_test_prob(count_frequency([X_test[i]]), prior_prob[j],cond_prob[j])</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>     </span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># predict is argmax of each row of prob</span></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>    predict <span class="op">=</span> prob.argmax(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate posterior probability, subtract predict for computational ease</span></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create empty matrix for calculation</span></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>    prob_minus_m <span class="op">=</span> np.empty((num_test, num_class))</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_test):</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> prob[i][predict[i]]</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>        vm <span class="op">=</span> np.full((num_class, ), m)</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>        prob_minus_m[i] <span class="op">=</span> vm</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>    test_prob <span class="op">=</span> softmax(prob <span class="op">-</span> vm)</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># *****</span><span class="re">END</span><span class="co"> OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> predict, test_prob</span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_test_prob(word_count, prior_cat, cond_cat):</span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a><span class="co">    predict the class label for one test example</span></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a><span class="co">    - word_count: a dictionary which stores the frequencies of each word in a SMS message. </span></span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a><span class="co">                  Key is the word, value is the number of its occurrence in that message</span></span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a><span class="co">    - prior_cat: a scalar. prior probability of a specific category</span></span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a><span class="co">    - cond_cat: a dictionary. conditional probability of a specific category</span></span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a><span class="co">    Outputs:</span></span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a><span class="co">    - prob: discriminant value g_y of a specific category for the test example </span></span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a><span class="co">                  (no need of normalization, i.e., not exactly the posterior probability)</span></span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>    <span class="co"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a>    <span class="co">#initialize conditional probability  </span></span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a>    log_cond_prob <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fetch cond prob of each word in i of x_test from cond prob dict</span></span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> w, n <span class="kw">in</span> word_count.items():</span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> w <span class="kw">in</span> cond_cat:</span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a>            log_cond_prob <span class="op">+=</span>  n <span class="op">*</span> np.log(cond_cat[w])</span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a>            log_cond_prob <span class="op">+=</span>  n <span class="op">*</span> np.log(cond_cat[<span class="st">'dummy'</span>])</span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a>     </span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a>    prob <span class="op">=</span> np.log(prior_cat) <span class="op">+</span> log_cond_prob</span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a>    <span class="co"># *****</span><span class="re">END</span><span class="co"> OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> prob</span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a><span class="co"># unit test case:</span></span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a><span class="co"># x_test = np.array(['Hello, how are you!'])</span></span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a><span class="co"># sample outputs:</span></span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a><span class="co"># y_pred: [0] </span></span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a><span class="co"># test_prob: [[0.97958684 0.02041316]]</span></span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a>x_test <span class="op">=</span> np.array([<span class="st">'Hello, how are you today!'</span>])</span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true" tabindex="-1"></a>y_pred, test_prob <span class="op">=</span> predict_label(x_test, prior, cond_prob)</span>
<span id="cb9-90"><a href="#cb9-90" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y_pred, test_prob)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0] [[0.97959683 0.02040317]]</code></pre>
</div>
</div>
</section>
<section id="step-3-compute-performance-metrics-9-points" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="step-3-compute-performance-metrics-9-points">Step 3: compute performance metrics <strong>(9 points)</strong></h4>
<p>You may have noticed that the classes are heavily imbalanced. There are only 747 <code>spam</code> messages, compared with 4827 <code>ham</code> messages. If a classifier simply predicts all messages as <code>ham</code>, it will get around 86% accuracy (pretty high). Therefore, accuracy is not a good metric in this case for evaluating the performance of the classifier. As we did before, we can use F-score metrics. But this time we will not implement it from the scratch. Instead, we will learn how to use the builtin methods from <code>scikit</code>. <a href="https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics">Here</a> is a summary of well-implemented and commonly used metrics for evaluating the quality of a model’s predictions.</p>
<p>In this task, you need to <strong>report</strong> the testing accuracy, confusion matrix, and F1 score of the Naive Bayes method by choosing proper functions from <code>scikit</code> to compute those metrics with required arguments.</p>
<p>Hint: you need to import methods from <code>sklearn.metrics</code> before using them.</p>
<div class="cell" data-tags="[]" data-execution_count="45">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_metrics(y_pred, y_true):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co">    compute the performance metrics</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Inputs:</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co">    - y_pred: an array of predictions</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co">    - y_true: an array of ground true labels</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Outputs:</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co">    - acc: accuracy</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">    - cm: confusion matrix</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">    - f1: f1_score</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute accuracy</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> accuracy_score(y_true, y_pred)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># confusion matrix</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    cm <span class="op">=</span> confusion_matrix(y_true, y_pred)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># f1 score</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> sklearn.metrics <span class="im">import</span> f1_score</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    f1 <span class="op">=</span> f1_score(y_true, y_pred)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># *****</span><span class="re">END</span><span class="co"> OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> acc, cm, f1</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="co"># unit test case:</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a><span class="co"># y_pred = np.array([0,1,1,1,0,1])</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a><span class="co"># y_true = np.array([0,1,0,0,1,1])</span></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a><span class="co"># sample outputs:</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a><span class="co"># acc: 0.5 </span></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a><span class="co"># cm: [[1 2]</span></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a><span class="co">#      [1 2]] </span></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a><span class="co"># f1: 0.5714285714285715</span></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> np.array([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> np.array([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>])</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>acc, cm, f1 <span class="op">=</span> compute_metrics(y_pred, y_true)</span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(acc, cm, f1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.5 [[1 2]
 [1 2]] 0.5714285714285715</code></pre>
</div>
</div>
</section>
<section id="step-4-plot-roc-curve-and-print-other-results-16-points" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="step-4-plot-roc-curve-and-print-other-results-16-points">Step 4: Plot ROC curve and print other results <strong>(16 points)</strong></h4>
<p>ROC (Receiver Operating Characteristics) curve is one of the most commonly used metrics for evaluating the performance of machine learning algorithms, especially when the classes are imbalanced.</p>
<p>ROC is a probability curve for different classes. ROC tells us how good the model is for distinguishing the given classes, in term of the <strong>predicted probability</strong> (not the final hard label in pos/neg). A typical ROC curve has False Positive Rate (FPR) on the <span class="math inline">\(x\)</span>-axis and True Positive Rate (TPR) on the <span class="math inline">\(y\)</span>-axis. To obtain the FPR and TPR, you can use the <code>roc_curve</code> method from <code>scikit</code>. This <code>roc_curve</code> function takes two arguments: 1) the ground truth labels of the test examples, and 2) the predicted probability that each example is positive. It returns the FPR and TPR which can be used for plotting.</p>
<p>You can even compute the area under the curve (AUC) by calling <code>roc_auc_score</code> which takes the same arguments as <code>roc_curve</code> required.</p>
<p>In this task, <strong>plot</strong> the ROC curve and compute the AUC score.</p>
<div class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve, roc_auc_score</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_roc_curve(fpr, tpr):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  plt.plot(fpr, tpr, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'ROC'</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  plt.plot([<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  plt.xlabel(<span class="st">'FPR'</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  plt.ylabel(<span class="st">'TPR'</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  plt.title(<span class="st">'ROC Curve'</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>  plt.legend()</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>  plt.show()</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># We now compute the test performance.</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co"># X_train, X_test, y_train, y_test are the same as above</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co"># training naive Bayes model </span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>prior, cond <span class="op">=</span> train_NB_model(X_train, y_train)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate on test set</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>y_pred, prob <span class="op">=</span> predict_label(X_test, prior, cond)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Implement the following:</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="co">#   1. compute the fpr and tpr for roc curve using the probability of being positive</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a><span class="co">#   2. compute the auc score</span></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a><span class="co">#   3. plot roc curve by calling the plot_roc_curve() method</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a><span class="co">#   4. print AUC score, test accuracy, F-score, and Confusion matrix</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a><span class="co"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****  </span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a><span class="co"># 1</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>fpr, tpr, thresholds <span class="op">=</span> roc_curve(y_test, y_pred)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="co"># 2</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>auc <span class="op">=</span> roc_auc_score(y_test, y_pred )</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a><span class="co"># 3 </span></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>plot_roc_curve(fpr, tpr)</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a><span class="co">#4</span></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>acc, cm, f1 <span class="op">=</span> compute_metrics(y_test, y_pred)</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"AUC score: "</span>, roc_auc_score(fpr, tpr), <span class="st">"test accuracy: "</span>, acc, <span class="st">"F-score: "</span>, f1, <span class="st">"Confusion matrix: "</span>, cm)</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a><span class="co"># *****</span><span class="re">END</span><span class="co"> OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="Lab_5_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>AUC score:  1.0 test accuracy:  0.9497757847533632 F-score:  0.7647058823529412 Confusion matrix:  [[968  56]
 [  0  91]]</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="submission-instruction" class="level1 unnumbered">
<h1 class="unnumbered">Submission Instruction</h1>
<p>You’re almost done! Take the following steps to finally submit your work.</p>
<ol type="1">
<li>After executing all commands and completing this notebook, save your <code>Lab_5.ipynb</code> as a PDF file, named as <code>X_Y_UIN.pdf</code>, where <code>X</code> is your first name, <code>Y</code> is your last name, and <code>UIN</code> is your UIN. Make sure that your PDF file includes all parts of your solution, including the plots.</li>
</ol>
<blockquote class="blockquote">
<ul>
<li>Print out all unit test case results before printing the notebook into a PDF.</li>
<li>If you use Colab, open this notebook in Chrome. Then File -&gt; Print -&gt; set Destination to “Save as PDF”. If the web page freezes when printing, close Chrome and reopen the page. If Chrome doesn’t work, try Firefox.</li>
<li>If you are working on your own computer, we recommend using the browser (not jupyter) for saving the PDF. For Chrome on a Mac, this is under <em>File-&gt;Print…-&gt;Open PDF in Preview</em>. When the PDF opens in Preview, you can use <em>Save…</em> to save it.</li>
<li>Sometimes, a figure that appears near the end of a page can get cut. In this case, try to add some new lines in the preceding code block so that the figure is pushed to the beginning of the next page. Or insert some text blocks.</li>
</ul>
</blockquote>
<ol start="2" type="1">
<li><p>Upload <code>X_Y_UIN.pdf</code> to Gradescope under <code>Lab_5_Written</code>.</p></li>
<li><p>A template of <code>Lab_5.py</code> has been provided. For all functions in <code>Lab_5.py</code>, copy the corresponding code snippets you have written into it, excluding the plot code. <strong>Do NOT</strong> copy any code of plotting figures and do not import <strong>matplotlib</strong>. This is because the auto-grader cannot work with plotting. <strong>Do NOT</strong> change the function names.</p></li>
<li><p>Zip <code>Lab_5.py</code> and <code>Lab_5.ipynb</code> (<strong>2 files</strong>) into a zip file named <code>X_Y_UIN.zip</code>. Suppose the two files are in the folder <code>Lab_5</code>. Then zip up the <strong>two files inside the <code>Lab_5</code> folder</strong>. <strong>Do NOT zip up the folder <code>Lab_5</code></strong> because the auto-grader cannot search inside a folder. Submit this zip file to Gradescope under <code>Lab_5_Code</code>.</p></li>
<li><p>The autograder on Gradscope will be open all the time. We designed some simple test cases to help you check wehther your functions are executable. You will see the results of running autograder once you submit your code. Please follow the error messages to debug. Since those simple test cases are designed for debugging, it does not guaranttee your solution will work well on the real dataset. It is your responsibility to make your code logically correct. Since all functions are tested in batch, the autograder might take a few minutes to run after submission.</p></li>
</ol>
<p><font color="red">If you <em>only</em> try to get real-time feedback from auto-grader, it will be fine to just upload <code>Lab_5.py</code> to <code>Lab_5_Code</code></font>. However, the final submission for grading should still follow the above point 4.</p>
<p>You can submit to Gradescope as often as you like. We will only consider your last submission before the deadline.</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>