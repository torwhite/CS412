{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-C8omlh6Pq4"
   },
   "source": [
    "# **Lab 5: Naive Bayes Classification**\n",
    "\n",
    "CS 412\n",
    "\n",
    "***This lab can be conducted in groups.***\n",
    "\n",
    "In this lab, you will learn how to apply the Naive Bayes model to filter spam SMS messages. \n",
    "\n",
    "***Deadline:***\n",
    "**23:59 PM, Mar 17**.\n",
    "\n",
    "\n",
    "<font color='red'> Please refer to `Lab_Guideline.pdf` in the same Google Drive folder as this Jupyter notebook; the guidelines there apply to all the labs.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuqxFfyO6Pq6"
   },
   "source": [
    "# **Naive Bayes Classification** {-}\n",
    "\n",
    "In this problem, you will implement the Naive Bayes classification method and use it for SMS message classifcation. The SMS dataset `SMSSpamCollection` has been provided in the assignment folder, which also can be downloaded from the [UCI Link](http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection).  In case the repository gets offline occasionally, we have made local copies for the [dataset](https://www.cs.uic.edu/~zhangx/teaching/SMSSpamCollection.dat) and [readme](https://www.cs.uic.edu/~zhangx/teaching/readme.txt).\n",
    "\n",
    "To help you to better understand this algorithm, you are **not** allowed to use any off-the-shelf naive Bayes implementations from third-party libraries. We will implement it with detailed step-by-step instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "io7R6hOn6Pq9"
   },
   "source": [
    "### Recap of the Naive Bayes Algorithm  {-}\n",
    "Naive Bayes classification is a fast and simple classification method. Its efficiency stems from some simplifications we make about the underlying probability distributions, namely, the assumption about the conditional independence of features. Suppose for any class $Y$, we have a probability distribution over all possible combinations of values for a feature vector $X$:\n",
    "$$\n",
    "P(X|Y).\n",
    "$$\n",
    "The main idea of Bayesian classification is to reverse the direction of dependence --- we want to predict the label based on the features:\n",
    "$$\n",
    "P(Y|X).\n",
    "$$\n",
    "This is made possible by the Bayes theorem:\n",
    "\\begin{equation}\n",
    "P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}. \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "To make it more concrete, let us consider the SMS message classification problem. Ignoring punctuations, each SMS message contains an ordered sequence of $T$ words (case-insensitive) $X = \\{X_1, ...,X_T\\}$. That is, $X$ corresponds to an SMS message, and $X_i$ corresponds to the $i$-th word in it. For each message from the training set, there is a corresponding label $Y\\in\\{spam, ham\\}$. \n",
    "\n",
    "**Model specification and key assumption.** The conditional distribution can be written as:\n",
    "$$\n",
    "P(X|Y) = P(X_1, ..., X_T|Y).\n",
    "$$\n",
    "Since this conditional probability is intractable, we simplify it in two steps:\n",
    "\n",
    "1. **Assume** that all features $X_i$ are independent, conditional on the category $Y$. This leads to a naive Bayes model which writes formally as\n",
    "\\begin{equation}\n",
    "P(X|Y) = P(X_1, ..., X_T|Y) = \\prod_{i=1}^T P(X_i|Y). \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "2. **Assume** that $P(X_i | Y) = P(X_j|Y)$ for all $i \\neq j$.\n",
    "In other words, given the label $Y$, the value of the $7$-th word has the same distribution as the value of the $10$-th word. Note this is not assumed by default in naive Bayes, and we make this additional assumption to significantly simplify our model.\n",
    "It is often referred to as \"tying\" the probability $P(X_i|Y)$ over $i$.\n",
    "As a result, the order of words no longer matters for $P(X|Y)$,\n",
    "i.e., \n",
    "$$P(X=\\text{'cat is cute'}|Y) \\ \\ = \\ \\ P(X=\\text{'cute cat is'}|Y) \\quad \\text{for all } Y.\n",
    "$$\n",
    "\n",
    "Plugging Eq (2) into the Bayes theorem in Eq (1), we arrive at\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(Y|X) &= \\frac{P(Y) P(X|Y)}{P(X)} = \\frac{P(Y)\\prod_{i=1}^T P(X_i|Y)}{P(X)} \\\\\n",
    "&\\propto P(Y)\\prod_{i=1}^T P(X_i|Y),\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\propto$ denotes proportionality. Since the denominator $P(X)$ does not depend on $Y$, the prediction probability is proportional to the numerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWSd8rJUqhZ7"
   },
   "source": [
    "**Making predictions.**\n",
    "Naturally, given an SMS message $X$, we can first compute $P(Y|X)$ for all possible categories $Y$ (in this example, only two categories), and then make predictions by outputting the $Y$ that maximizes the probability. This can be expressed mathmatically as:\n",
    "$$\n",
    "\\arg\\max_Y P(Y)\\prod_{i=1}^T P(X_i|Y) = \\arg\\max_Y \\left\\{\\log P(Y) + \\sum_{i=1}^T \\log P(X_i|Y) \\right\\}. \\tag{3}\n",
    "$$\n",
    "If there is a tie, we just break it arbitrarily.\n",
    "Here the logarithm uses natural basis.\n",
    "\n",
    "**Learning the model.**\n",
    "To apply the prediction rule in Eq (3), we need to first figure out (formally termed \"estimate\") the value of $P(Y)$ and $P(X_i|Y)$ by using the training data. Recall that since we tie the conditional probabilities $P(X_i|Y)$ across all $i$,\n",
    "the subscript $i$ can be dropped.\n",
    "However, we still carry it just for clarity. \n",
    "\n",
    "Firstly, $P(Y=y)$ can be estimated by computing the frequency of category $y$ in the whole training set ($y$ can be either \"$spam$\" or \"$ham$\"). Here, in order to avoid confusion, we have used the convention that capital letters denote random variables, and lowercase letters denote their possible instantiations.\n",
    "\n",
    "Secondly, $P(X_i = w |Y=y)$ for a word $w$ (e.g., \"cat\") can be estimated by counting the frequency that it appears in the training message set for a given category $y$ (derivation not required in the course):\n",
    "\\begin{align}\n",
    "P(X_i &= w|Y=y) \n",
    "= \\frac{Count(w, y)}{Count(y)}, \\ \\ where\n",
    "\\tag{4} \\\\\n",
    " Count(w, y) &= \\text{total number of occurrence of $w$ in all SMS messages of category } y \\\\\n",
    "Count(y) &= \\text{total number of words appearing in SMS messages of category } y.\n",
    "\\end{align}\n",
    "\n",
    "*Remark 1:* If $w$ appears in a single message for 3 times, then it contributes to $Count(w, y)$ by 3, not 1. Similarly, $Count(y)$ indeed equals the total length of all messages in category $y$.\n",
    "\n",
    "*Remark 2:* Obviously, the right-hand side of Eq (4) does not depend on $i$. This is consistent with our previous note that we carry the subscript $i$ in $P(X_i|Y)$ only for clarity, while in fact different $i$ share the same $P(X_i|Y)$.\n",
    "\n",
    "For example, suppose there are four messages \n",
    "$$\n",
    "\\text{{'cat is cute', ham}, {'dog rocks', spam}, {'whatever is is right', ham}, {'hello', spam}.}\n",
    "$$\n",
    "Then $P(X_i = '\\text{is}' | \\text{ham}) = 3 / 7$ (**not** $2/7$),\n",
    "and $P(X_i = '\\text{is}' | \\text{spam}) = 0 / 3$.\n",
    "\n",
    "Quiz (no need to submit): compute $P(X_i = w | \\text{ham})$, for $w = $ 'cat', 'is', 'cute', 'whatever', and 'right'. Check if their sum is 1. Now appreciate why in Remark 1, a word appearing for mulitple times in a single message should be counted multiple times.\n",
    "\n",
    "You may have noticed that any word $w$ with $Count(w,y)=0$ leads to $P(X_i = w|Y=y) = 0$.\n",
    "As a result, by Eq (2), any message $x$ has conditional probably $P(X = x |Y=y) = 0$ if $w$ appears in $x$.\n",
    "Such a \"veto\" is not favorable, and can create significant problems when a word in the test data has never appeared in the training data (think why?).\n",
    "To bypass this issue, we can add pseudo-count, a.k.a additive smoothing:\n",
    "$$\n",
    "\\hat{P}(X_i = w|Y=y) = \\frac{Count(w, y) + \\alpha}{Count(y) + N\\alpha}, \\tag{5}\n",
    "$$\n",
    "where $\\alpha$ is a smoothing parameter. $\\alpha=0$ corresponds to no smoothing. In our experiment, let us set $\\alpha = 1.0$. $N$ denotes the number of distinct words in the vocabulary, and let us set $N = 20,000$ in this lab.\n",
    "\n",
    "Now Let's start with data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TqrY7MaJ6PrA"
   },
   "outputs": [],
   "source": [
    "# set up code for this experiment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8VYKq9-6PrP"
   },
   "source": [
    "### 1. Data Preprocessing **(25 points)** {-}\n",
    "\n",
    "We will use `pandas` to import the dataset. Since `SMSSpamCollection` separates labels and text content in each message by a tab, we will use '\\t' as the value for the `sep` argument and read raw data into a pandas dataframe. As a result, we store labels and SMS messages into two columns. To facilitate the subsequent steps, we also rename the columns by passing a list `['label', 'sms_message']` to the `names` argument of the `read_table()` method.\n",
    "\n",
    "Let us print the first five rows of the dataframe to get a basic understanding of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FXBuWAZ06PrQ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                        sms_message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the dataset to the server\n",
    "# Import the data using the read_csv() method from pandas\n",
    "\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "url = 'https://www.cs.uic.edu/~zhangx/teaching/SMSSpamCollection.dat'\n",
    "file_name = 'SMSSpamCollection.dat'\n",
    "with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
    "    shutil.copyfileobj(response, out_file)\n",
    "\n",
    "df = pd.read_csv(file_name,\n",
    "                    sep='\\t',\n",
    "                    header=None,\n",
    "                    names=['label', 'sms_message'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzzbwM0h6Prc"
   },
   "source": [
    "#### Step 1: Convert string labels to numerical labels (not for grading) {-}\n",
    "\n",
    "As we can see, there are 2 columns. The first column, which is named `label`, takes two values `spam` (the message is spam) and `ham` (the message is not spam). The second column is the text content of the SMS message that is being classified.  It is a string in which words are separated by space.\n",
    "\n",
    "Note that the string-typed labels are unwieldy for calculating performance metrices, e.g., when calculating precision and recall scores. Hence, let's convert the lables to binary variables, 0 for `ham` and 1 for `spam`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U5myRuvh6Prd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        sms_message\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the next line only once after running the previous code block\n",
    "# Running it more than once will turn the labels into NaN\n",
    "#df['label'] = df.label.map({'ham':0, 'spam':1})\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2Qsp7_C6Prg"
   },
   "source": [
    "#### Step 2: Bag of words **(16 points)** {-}\n",
    "\n",
    "What we have in our dataset is a large collection of text data (5,572 rows/messages). Most ML algorithms rely on numerical data to be fed into them as input, but SMS messages are usually text heavy. \n",
    "\n",
    "To address this issue, we would like to introduce the concept of Bag of Words (BoW), which is designed for problems with a 'bag of words' or a collection of text data. The basic idea is to count the frequency of the words in the text. It is important to note that BoW treats each word individually, ignoring the order in which the words occur. \n",
    "\n",
    "To count the frequency of the words in text, usually we need to process the input text data in four steps:\n",
    "\n",
    "- Convert all strings into their lower case form\n",
    "- Removing all punctuations\n",
    "- Tokenization, i.e., split a sentence into individual words\n",
    "- Count frequencies\n",
    "\n",
    "Once this has been done, we are supposed to obtain a vocabulary dictionary with frequencies of each words for the given text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Gf3M2dHK6Pri",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_frequency(documents):\n",
    "    \"\"\"\n",
    "    count occurrence of each word in the document set.\n",
    "    Inputs:\n",
    "    - documents: list, each entity is a string type SMS message\n",
    "    Outputs:\n",
    "    - frequency: a dictionary. The key is the unique words, and the value is the number of occurrences of the word\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    # Step 1: covert all strings into their lower case form\n",
    "    lower_case_doc = []\n",
    "    for s in documents:\n",
    "        lower_case_doc.append(s.lower())\n",
    "        \n",
    "    # Step 2: remove all punctuations\n",
    "    no_punc_doc = []\n",
    "    for s in lower_case_doc:\n",
    "        no_punc_doc.append(s.translate(str.maketrans('','',string.punctuation)))\n",
    "     \n",
    "    # Step 3: tokenize a sentence, i.e., split a sentence into individual words \n",
    "    # using a delimiter. The delimiter specifies what character we will use to identify the beginning \n",
    "    # and the end of a word.\n",
    "    words_doc = []\n",
    "    for s in no_punc_doc:\n",
    "        words_doc.append(s.split(' '))\n",
    "        \n",
    "    # Step 4: count frequencies. To count the occurrence of each word in the document set. \n",
    "    # We can use the `Counter` method from the Python `collections` library for this purpose. \n",
    "    # `Counter` counts the occurrence of each item in the list and returns a dictionary with \n",
    "    # the key as the item being counted and the corresponding value being the count of that item in the list. \n",
    "    all_words = []\n",
    "    for s in words_doc:\n",
    "        all_words.extend(s)\n",
    "\n",
    "    frequency = Counter(all_words)\n",
    "        \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return frequency\n",
    "\n",
    "# Unit test case:\n",
    "# documents = ['Hello, how are you!', \n",
    "#              'Win money, win from home.',\n",
    "#              'Call me now.',\n",
    "#              'Hello, Call hello you tomorrow?']\n",
    "# sample outputs:\n",
    "# Counter({'hello': 3, 'you': 2, 'win': 2, 'call': 2, 'how': 1, 'are': 1, 'money': 1, 'from': 1, 'home': 1,\n",
    "# 'me': 1, 'now': 1, 'tomorrow': 1})\n",
    "documents = ['Hello, how are you!',\n",
    "            'Win money, win from home.',\n",
    "            'Call me now.',\n",
    "            'Hello, Call hello you tomorrow?']\n",
    "\n",
    "freq = count_frequency(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJ5l72qQ6Prn"
   },
   "source": [
    "#### Step 3: Create training and test sets **(9 points)** {-}\n",
    "\n",
    "We will partition the `SMSSpamCollection` dataset into training and test sets so that we can analyze the model's performance on data it has not witnessed during training. In Lab 3, we have implemented the `split_nfold()` method from scratch for data partition. In this Lab, we will learn to use the `scikit` library. `scikit` is a powerfull tool for machine learning and data mining, providing plenty of well-designed methods for data analysis. We'll use its `train_test_split()` method to create training and testing sets. In this experiment, we use 80% data for training and the remaining 20% data for testing. To ensure your results are replicable, you need to set the `random_state` argument of `train_test_split()` to **1**.\n",
    "\n",
    "There is no cross validation in this Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "KXkkpzP46Pro",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dataset contains 5572 examples in total.\n",
      "The training set contains 4457 examples.\n",
      "The testing set contains 1115 examples.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# learn to read API documentation\n",
    "# you can get detailed instructions about this method through this link:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['sms_message'], df['label'], test_size=.2, random_state=1)\n",
    "\n",
    "X_test = X_test.tolist()\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "print(f'The original dataset contains {df.shape[0]} examples in total.')\n",
    "print(f'The training set contains {X_train.shape[0]} examples.')\n",
    "print(f'The testing set contains {len(X_test)} examples.')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVB4Giy56Prv"
   },
   "source": [
    "### 2. Implementing Naive Bayes method from scratch **(75 points)**{-}\n",
    "\n",
    "#### Step 1: training the Naive Bayes Model **(25 points)** {-}\n",
    "\n",
    "Now that we know what Naive Bayes is, we can take a closer look at how to calculate the posterior probability\n",
    "$$\n",
    "P(Y|X) \\propto P(Y)\\prod_{i=1}^T P(X_i|Y).\n",
    "$$\n",
    "\n",
    "The goal of training is to learn the prior and conditional probability from data. The calculation of the prior $P(Y=y)$ is straightforward. It can be estimated via the frequency of messages in the training set that belong to class $y$, e.g.,\n",
    "$$\n",
    "P(Y=spam) = \\frac{\\# \\text{training messages in the spam category}}{\\# \\text{training messages}}.\n",
    "$$\n",
    "\n",
    "The conditional probability given the class label --- $P(X_i|Y)$ --- can also be estimated from the data by using Eq (5). As we assumed above, it is indeed independent of $i$ (i.e., shared by all $i$). We will leave it to you to translate Eq (5) into a concrete computation scheme. No pseudo-code is provided because by now you should be able to do it. However, do make sure that your implementation complies with the input and output data types as specified in the code.\n",
    "\n",
    "**Hint**: \n",
    "- `count_frequency()` can be useful for computing the conditional probability.\n",
    "- You need to apply the **pseudo-count** trick to handle unseen words when computing the conditional probability for testing data. It is natual to ask how to carry the quantity Count(y) in Equation 5 through the variables that are passed through the functions.  To address this issue, we can create a 'dummy' entry in cond_prob (the variable returned by the function conditional_prob), and set its value to $\\hat{P}(X_i = dummy | Y = y) = \\alpha / (Count(y) + N \\alpha)$.  Then all the words that appear in test data but not in training data can directly use that entry.  Please do not use other key names than 'dummy', because the auto-grader follows this protocal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "CATiEZSw6Prw",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.5, 1: 0.5} {0: {'hello': 0.000199930024491428, 'how': 9.9965012245714e-05, 'are': 9.9965012245714e-05, 'you': 0.000149947518368571, 'call': 9.9965012245714e-05, 'tomorrow': 9.9965012245714e-05, 'dummy': 4.9982506122857e-05}, 1: {'win': 0.00014994002399040384, 'money': 9.996001599360256e-05, 'from': 9.996001599360256e-05, 'home': 9.996001599360256e-05, 'call': 9.996001599360256e-05, 'me': 9.996001599360256e-05, 'now': 9.996001599360256e-05, 'dummy': 4.998000799680128e-05}}\n"
     ]
    }
   ],
   "source": [
    "def train_NB_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    training a naive bayes model from the training data.\n",
    "    Inputs:\n",
    "    - X_train: an array of shape (num_train,) which stores SMS messages. each entity is a string type SMS message\n",
    "    - y_train: an array of shape (num_train,). the ground true label for each training data.\n",
    "    Output:\n",
    "    - prior: a dictionary, whose key is the class label, and value is the prior probability.\n",
    "    - conditional: a dictionary whose key is the class label y, and value is another dictionary.\n",
    "                   In the latter dictionary, the key is word w, and the value is the\n",
    "                   conditional probability P(X_i = w | y).\n",
    "    \"\"\"\n",
    "\n",
    "    # To make your code more readable, you can implement some auxiliary functions\n",
    "    # such as `prior_prob` and `conditional_prob` outside of this train_NB_model function\n",
    "\n",
    "    # compute the prior probability\n",
    "    prior = prior_prob(y_train)\n",
    "    \n",
    "    # compute the conditional probability\n",
    "    conditional = conditional_prob(X_train, y_train)\n",
    "\n",
    "    return prior, conditional\n",
    "\n",
    "# Start your auxiliary functions\n",
    "def add_smooth(count_x, count_all, alpha=1.0, N=20000):\n",
    "\n",
    "    \"\"\"\n",
    "    compute the conditional probability for a specific word\n",
    "    Inputs:\n",
    "    - count_x: the number of occurrence of the word\n",
    "    - count_all: the total number of words\n",
    "    - alpha: smoothing parameter\n",
    "    - N: the number of different values of feature x\n",
    "    Outputs:\n",
    "    - prob: conditional probability\n",
    "    \"\"\"\n",
    "    return (count_x + alpha) / (count_all + N*alpha)\n",
    "\n",
    "    \n",
    "def prior_prob(y_train):\n",
    "    \"\"\"\n",
    "    compute the prior probability\n",
    "    Inputs:\n",
    "    - y_train: an array that stores ground true label for training data\n",
    "    Outputs:\n",
    "    - prior: a dictionary. key is the class label, value is the prior probability.\n",
    "    \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    prior = {}\n",
    "    num_train = len(y_train)\n",
    "    labels = np.unique(y_train)\n",
    "    for i in range(len(labels)):\n",
    "        prior[i] = len(y_train[y_train == i])/num_train\n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****   \n",
    "    \n",
    "    return prior\n",
    "\n",
    "def conditional_prob(X_train, y_train):\n",
    "    \"\"\"\n",
    "    compute the conditional probability for a document set\n",
    "    Inputs:\n",
    "    - X_train: an array of shape (num_train,) which stores SMS messages. each entity is a string type SMS message\n",
    "    - y_train: an array of shape (num_train,). the ground true label for each training data.\n",
    "    Ouputs:\n",
    "    - cond_prob: a dictionary. key is the class label, value is a dictionary in which the key is word, the value is the conditional probability of feature x_i given y.\n",
    "    \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    # docs with label '0' to ham, '1' to spam list\n",
    "    labels = np.unique(y_train)\n",
    "    X_train = pd.Series(X_train)\n",
    "    y_train = pd.Series(y_train)\n",
    "    num_train = len(X_train)\n",
    "    \n",
    "    #FIXME add general case for creating the nested dict\n",
    "    cond_prob = {0: {} , 1: {}} \n",
    "\n",
    "    # empty list with training examples of label 0\n",
    "    ham = []\n",
    "    # empty list with training examples of label 1\n",
    "    spam = []\n",
    "\n",
    "    # FIXME general case\n",
    "    for i, v in X_train.items():\n",
    "        if y_train[i] == 0:\n",
    "            ham.append(X_train[i])\n",
    "        else:\n",
    "            spam.append(X_train[i])\n",
    "     \n",
    "    # compute frequency in docs\n",
    "    freq_ham = count_frequency(ham)\n",
    "    \n",
    "    freq_spam = count_frequency(spam)\n",
    "    \n",
    "    # calculate conditional probability\n",
    "    \n",
    "    # for ham\n",
    "    count_ham = len(freq_ham) + 1\n",
    "    # add 0 labels\n",
    "    for key in freq_ham:\n",
    "        count_x = freq_ham[key]\n",
    "        cond_prob[0][key] = add_smooth(count_x, count_ham)\n",
    "    \n",
    "    # add dummy case for ham \n",
    "    cond_prob[0]['dummy'] = add_smooth(0, count_ham)\n",
    "    \n",
    "    # for spam\n",
    "    count_spam = len(freq_spam) + 1\n",
    "    # add 1 labels\n",
    "    for key in freq_spam:\n",
    "        count_x = freq_spam[key]\n",
    "        cond_prob[1][key] = add_smooth(count_x, count_spam)\n",
    "                                    \n",
    "    # add dummy case for spam \n",
    "    cond_prob[1]['dummy'] = add_smooth(0, count_spam)\n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****   \n",
    "    \n",
    "    return cond_prob\n",
    "\n",
    " #unit test case:\n",
    "x_train = ['Hello, how are you!',\n",
    "            'Win money, win from home.',\n",
    "           'Call me now.',\n",
    "           'Hello, Call hello you tomorrow?']\n",
    "#y_train = np.array([0,1,1,0,1])\n",
    "\n",
    "# sample outputs:\n",
    "# prior: {0: 0.5, 1: 0.5}\n",
    "# conditional: {0: {'hello': 0.0001999100404817832, 'how': 9.99550202408916e-05, \n",
    "#                   'are': 9.99550202408916e-05, 'you': 0.0001499325303613374, \n",
    "#                   'call': 9.99550202408916e-05, 'tomorrow': 9.99550202408916e-05,\n",
    "#                   'dummy': 4.99775101204458e-05}, \n",
    "#               1: {'win': 0.00014994002399040384, 'money': 9.996001599360256e-05, \n",
    "#                   'from': 9.996001599360256e-05, 'home': 9.996001599360256e-05, \n",
    "#                   'call': 9.996001599360256e-05, 'me': 9.996001599360256e-05, \n",
    "#                   'now': 9.996001599360256e-05}, 'dummy': 4.998000799680128e-05}}\n",
    "#x_train = np.array(['Hello, how are you!',\n",
    "#            'Win money, win from home.',\n",
    " #           'Call me now.',\n",
    "  #          'Hello, Call hello you tomorrow?'])\n",
    "y_train_mini = np.array([0,1,1,0])\n",
    "\n",
    "prior = prior_prob(y_train_mini)\n",
    "cond_prob = conditional_prob(x_train, y_train_mini)\n",
    "print(prior, cond_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdPoLF0-6Pr2"
   },
   "source": [
    "#### Step 2: predict label for test data **(25 points)** {-}\n",
    "\n",
    "Once we have the two models $P(Y)$ and $P(X_i|Y)$ from *training*, we can use them to predict the label for a given test message. To this end, we need to compute the probability of all possible labels, and then predict the one with maximum probability value:\n",
    "$$\n",
    "\\arg\\max_Y P(Y)\\prod_{i=1}^T P(X_i|Y). \\tag{6}\n",
    "$$\n",
    "\n",
    "**Avoid numerical underflow with log-trick.**\n",
    "As shown in the above equation, the calculation involves multiplying many probabilities together. Since probabilities lie in $(0,1]$, multiplying many of them together can lead to numerical underflow (i.e., a floating point number close to 0 gets rounded down to 0 by a computer), especially when $T$ is large, i.e., the test message is long.\n",
    "\n",
    "To overcome this problem, it is common to change the calculation from the product of probabilities to the sum of log probabilities. \n",
    "That is, take the natual logarithm of the right-hand side of Eq (6) as\n",
    "$$\n",
    "g_Y(X) = \\log P(Y) + \\sum_{i=1}^T \\log P(X_i|Y). \\tag{7}\n",
    "$$\n",
    "It is much more numerically stable to compute $g_Y(X)$ and to take $\\arg\\max_Y g_Y(X)$ to find the most likely class label (as the output prediction). \n",
    "\n",
    "With $g_Y(X)$, we can easily compute the posterior probability by\n",
    "$$\n",
    "P(Y|X) = \\frac{\\exp (g_Y(X) - m)}{\\sum_y \\exp (g_y(X)-m)},\n",
    "\\text{ where  }\n",
    "m = \\max_y g_y(X).\n",
    "$$\n",
    "In our message classification problem, the summation in the denominator is just over `positive` and `negative`.\n",
    "Note we subtract by $m$, which does not change the result because the numerator and denominator cancel.\n",
    "However it is numerically useful because sometimes all $g_Y(X)$ are overly small and can cause numerical underflow inside exponentiation.\n",
    "By subtracting $m$, $g_Y(X) - m$ will be 0 (properly scaled) for at least one value of $Y$, and be negative for the other.  And even if another $y$ still suffers underflow in exponentiating $g_y(X)-m$, the posterior probabilities will still be correct.\n",
    "\n",
    "Again, you are expected to implement Eq (7) and loop over all test examples by yourself with no pseudo-code given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "5iDZ9QpB6Pr4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] [[0.97959683 0.02040317]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "def predict_label(X_test, prior_prob, cond_prob):\n",
    "    \"\"\"\n",
    "    predict the class labels for the testing set\n",
    "    Inputs:\n",
    "    - X_test: an array of shape (num_test,) which stores test data. \n",
    "              Each entity is a string type SMS message.\n",
    "    - prior_prob: a dictionary which stores the prior probability for all categories\n",
    "              We previously used \"prior_prob\" as the name of function.  \n",
    "              Here it is used as a dictionary name.  No confusion should arise.\n",
    "    - cond_prob: a dictionary whose key is the class label y, and value is another dictionary.\n",
    "                   In the latter dictionary, the key is word w, and the value is the\n",
    "                   conditional probability P(X_i = w | y).\n",
    "    Outputs:\n",
    "    - predict: an array that stores predicted labels\n",
    "    - test_prob: an array of shape (num_test, num_classes) which stores the posterior probability of each class\n",
    "    \"\"\"\n",
    "\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    # initialize variables\n",
    "    num_test = len(X_test)\n",
    "    num_class = len(prior_prob)\n",
    "    # empty list to hold test probs calculated each index is a class\n",
    "    prob = np.empty((num_test, num_class))\n",
    "    predict = np.empty(num_test)\n",
    "    #word_count = {}\n",
    "    \n",
    "    # predict label\n",
    "    # iterate over all entries in X_test \n",
    "    for i in range(num_test):\n",
    "        # find word count of specific text example\n",
    "        #word_count = count_frequency([X_test[i]])\n",
    "        # compute conditional probability for j classes\n",
    "        for j in range(num_class):\n",
    "            # calculate sum of log probabilities\n",
    "            prob[i][j] = compute_test_prob(count_frequency([X_test[i]]), prior_prob[j],cond_prob[j])\n",
    "     \n",
    "    \n",
    "    # predict is argmax of each row of prob\n",
    "    predict = prob.argmax(axis=1)\n",
    "\n",
    "    # calculate posterior probability, subtract predict for computational ease\n",
    "    # create empty matrix for calculation\n",
    "    prob_minus_m = np.empty((num_test, num_class))\n",
    "    for i in range(num_test):\n",
    "        m = prob[i][predict[i]]\n",
    "        vm = np.full((num_class, ), m)\n",
    "        prob_minus_m[i] = vm\n",
    "    \n",
    "    test_prob = softmax(prob - vm)\n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return predict, test_prob\n",
    "\n",
    "def compute_test_prob(word_count, prior_cat, cond_cat):\n",
    "    \"\"\"\n",
    "    predict the class label for one test example\n",
    "    Inputs:\n",
    "    - word_count: a dictionary which stores the frequencies of each word in a SMS message. \n",
    "                  Key is the word, value is the number of its occurrence in that message\n",
    "    - prior_cat: a scalar. prior probability of a specific category\n",
    "    - cond_cat: a dictionary. conditional probability of a specific category\n",
    "    Outputs:\n",
    "    - prob: discriminant value g_y of a specific category for the test example \n",
    "                  (no need of normalization, i.e., not exactly the posterior probability)\n",
    "    \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    #initialize conditional probability  \n",
    "    log_cond_prob = 0\n",
    "    # fetch cond prob of each word in i of x_test from cond prob dict\n",
    "    for w, n in word_count.items():\n",
    "        if w in cond_cat:\n",
    "            log_cond_prob +=  n * np.log(cond_cat[w])\n",
    "        else:\n",
    "            log_cond_prob +=  n * np.log(cond_cat['dummy'])\n",
    "     \n",
    "    prob = np.log(prior_cat) + log_cond_prob\n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return prob\n",
    "\n",
    "# unit test case:\n",
    "# x_test = np.array(['Hello, how are you!'])\n",
    "# sample outputs:\n",
    "# y_pred: [0] \n",
    "# test_prob: [[0.97958684 0.02041316]]\n",
    "x_test = np.array(['Hello, how are you today!'])\n",
    "y_pred, test_prob = predict_label(x_test, prior, cond_prob)\n",
    "print(y_pred, test_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QINYf3KB6Pr-"
   },
   "source": [
    "#### Step 3: compute performance metrics **(9 points)** {-}\n",
    "You may have noticed that the classes are heavily imbalanced. There are only 747 `spam` messages, compared with 4827 `ham` messages. If a classifier simply predicts all messages as `ham`, it will get around 86% accuracy (pretty high). Therefore, accuracy is not a good metric in this case for evaluating the performance of the classifier. As we did before, we can use F-score metrics. But this time we will not implement it from the scratch. Instead, we will learn how to use the builtin methods from `scikit`. [Here](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) is a summary of well-implemented and commonly used metrics for evaluating the quality of a model's predictions. \n",
    "\n",
    "In this task, you need to **report** the testing accuracy, confusion matrix, and F1 score of the Naive Bayes method by choosing proper functions from `scikit` to compute those metrics with required arguments.\n",
    "\n",
    "Hint: you need to import methods from `sklearn.metrics` before using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Wb58oHwb6PsA",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 [[1 2]\n",
      " [1 2]] 0.5714285714285715\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    compute the performance metrics\n",
    "    Inputs:\n",
    "    - y_pred: an array of predictions\n",
    "    - y_true: an array of ground true labels\n",
    "    Outputs:\n",
    "    - acc: accuracy\n",
    "    - cm: confusion matrix\n",
    "    - f1: f1_score\n",
    "    \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    # compute accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # f1 score\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return acc, cm, f1\n",
    "\n",
    "# unit test case:\n",
    "# y_pred = np.array([0,1,1,1,0,1])\n",
    "# y_true = np.array([0,1,0,0,1,1])\n",
    "# \n",
    "# sample outputs:\n",
    "# acc: 0.5 \n",
    "# cm: [[1 2]\n",
    "#      [1 2]] \n",
    "# f1: 0.5714285714285715\n",
    "y_pred = np.array([0,1,1,1,0,1])\n",
    "y_true = np.array([0,1,0,0,1,1])\n",
    "acc, cm, f1 = compute_metrics(y_pred, y_true)\n",
    "print(acc, cm, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZLTG_uros32"
   },
   "source": [
    "#### Step 4: Plot ROC curve and print other results **(16 points)** {-}\n",
    "\n",
    "ROC (Receiver Operating Characteristics) curve is one of the most commonly used metrics for evaluating the performance of machine learning algorithms, especially when the classes are imbalanced.\n",
    "\n",
    "ROC is a probability curve for different classes. ROC tells us how good the model is for distinguishing the given classes, in term of the **predicted probability** (not the final hard label in pos/neg). A typical ROC curve has False Positive Rate (FPR) on the $x$-axis and True Positive Rate (TPR) on the $y$-axis. To obtain the FPR and TPR, you can use the `roc_curve` method from `scikit`. This `roc_curve` function takes two arguments: 1) the ground truth labels of the test examples, and 2) the predicted probability that each example is positive. It returns the FPR and TPR which can be used for plotting.\n",
    "\n",
    "You can even compute the area under the curve (AUC) by calling `roc_auc_score` which takes the same arguments as `roc_curve` required.\n",
    "\n",
    "In this task, **plot** the ROC curve and compute the AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "6DiYOIdIo7F7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score:  1.0 test accuracy:  0.9497757847533632 F-score:  0.7647058823529412 Confusion matrix:  [[968  56]\n",
      " [  0  91]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "  plt.plot(fpr, tpr, color='blue', label='ROC')\n",
    "  plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "  plt.xlabel('FPR')\n",
    "  plt.ylabel('TPR')\n",
    "  plt.title('ROC Curve')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "# We now compute the test performance.\n",
    "# X_train, X_test, y_train, y_test are the same as above\n",
    "\n",
    "# training naive Bayes model \n",
    "prior, cond = train_NB_model(X_train, y_train)\n",
    "\n",
    "\n",
    "# evaluate on test set\n",
    "y_pred, prob = predict_label(X_test, prior, cond)\n",
    "\n",
    "\n",
    "# Implement the following:\n",
    "#   1. compute the fpr and tpr for roc curve using the probability of being positive\n",
    "#   2. compute the auc score\n",
    "#   3. plot roc curve by calling the plot_roc_curve() method\n",
    "#   4. print AUC score, test accuracy, F-score, and Confusion matrix\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****  \n",
    "# 1\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "# 2\n",
    "auc = roc_auc_score(y_test, y_pred )\n",
    "# 3 \n",
    "plot_roc_curve(fpr, tpr)\n",
    "\n",
    "#4\n",
    "acc, cm, f1 = compute_metrics(y_test, y_pred)\n",
    "print(\"AUC score: \", roc_auc_score(fpr, tpr), \"test accuracy: \", acc, \"F-score: \", f1, \"Confusion matrix: \", cm)\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_si6Qa3wbuut"
   },
   "source": [
    "# Submission Instruction {-}\n",
    "\n",
    "You're almost done! Take the following steps to finally submit your work.\n",
    "\n",
    "1. After executing all commands and completing this notebook, save your `Lab_5.ipynb` as a PDF file, named as `X_Y_UIN.pdf`, where `X` is your first name, `Y` is your last name, and `UIN` is your UIN. Make sure that your PDF file includes all parts of your solution, including the plots. \n",
    "\n",
    "> * Print out all unit test case results before printing the notebook into a PDF.\n",
    "* If you use Colab, open this notebook in Chrome.  Then File -> Print -> set Destination to \"Save as PDF\".  If the web page freezes when printing, close Chrome and reopen the page. If Chrome doesn't work, try Firefox.\n",
    "* If you are working on your own computer, we recommend using the browser (not jupyter) for saving the PDF. For Chrome on a Mac, this is under *File->Print...->Open PDF in Preview*. When the PDF opens in Preview, you can use *Save...* to save it.\n",
    "* Sometimes, a figure that appears near the end of a page can get cut.  In this case, try to add some new lines in the preceding code block so that the figure is pushed to the beginning of the next page. Or insert some text blocks.\n",
    "\n",
    "2. Upload `X_Y_UIN.pdf` to Gradescope under `Lab_5_Written`.\n",
    "\n",
    "3. A template of `Lab_5.py` has been provided.  For all functions in `Lab_5.py`, copy the corresponding code snippets you have written into it, excluding the plot code.  **Do NOT** copy any code of plotting figures and do not import **matplotlib**.  This is because the auto-grader cannot work with plotting.  **Do NOT** change the function names.  \n",
    "\n",
    "4. Zip `Lab_5.py` and `Lab_5.ipynb` (**2 files**) into a zip file named `X_Y_UIN.zip`. Suppose the two files are in the folder `Lab_5`.  Then zip up the **two files inside the `Lab_5` folder**.  **Do NOT zip up the folder `Lab_5`** because the auto-grader cannot search inside a folder. Submit this zip file to Gradescope under `Lab_5_Code`. \n",
    "\n",
    "5. The autograder on Gradscope will be open all the time. We designed some simple test cases to help you check wehther your functions are executable. You will see the results of running autograder once you submit your code. Please follow the error messages to debug. Since those simple test cases are designed for debugging, it does not guaranttee your solution will work well on the real dataset. It is your responsibility to make your code logically correct. Since all functions are tested in batch, the autograder might take a few minutes to run after submission.\n",
    "\n",
    "<font color='red'>If you *only* try to get real-time feedback from auto-grader, it will be fine to just upload `Lab_5.py` to `Lab_5_Code`</font>.  However, the final submission for grading should still follow the above point 4.\n",
    "\n",
    "You can submit to Gradescope as often as you like. We will only consider your last submission before the deadline."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
