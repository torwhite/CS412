{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMM2e3OP6Kn4"
   },
   "source": [
    "# **Lab 7: Logistic Regression**\n",
    "\n",
    "CS 412\n",
    "\n",
    "***This lab can be conducted individually or in groups.***\n",
    "\n",
    "In this lab, you learn how to apply the logistic regression model to recognize images of hand-written digits.\n",
    "\n",
    "***Deadline:***\n",
    "**23:59, Nov 29**.\n",
    "\n",
    "\n",
    "<font color='red'> Please refer to `Lab_Guideline.pdf` in the same Google Drive folder as this Jupyter notebook; the guidelines there apply to all the labs.</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "o2jeOJg80h6N"
   },
   "outputs": [],
   "source": [
    "# Let's first import some modules for this experiment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS5vLRbndy7h"
   },
   "source": [
    "## Logistic Regression {-}\n",
    "\n",
    "In this problem, we will implement a Logistic Regression model with gradient descent from scratch. Logistic regression is a statistical model used for binary classification. We start with the fundamental mathematics and statistics behind logistic regression, and then extend it to multinomial logistic regression which can handle multi-class classification problems. To help you fully understand how they work, we will be working on a real dataset to recognize images of hand-written digits.\n",
    "\n",
    "## 1.1 From linear regression to logistic regression (24 points){-}\n",
    "\n",
    "In our previous lab assignment, we have learned how to use linear regression to predict the quality of wines. Actually linear regression is one of the most extensively used statistical technique for predictive modelling analysis thanks to its simplicity. Let us take a quick review of this method.\n",
    "\n",
    "### 1.1 Recap of linear regression {-}\n",
    "\n",
    "Linear regression assumes that the dependence of the target $y$ on the features $x_1, ..., x_m$ is linear, even if the true regression function is nonlinear. One benefit of making a linear dependence assumption is that the relationship between the target and features can be easily interpreted.\n",
    "\n",
    "Let's define $f_w(x)$ as the hypothesis for $y$ as a function of $x\\in\\mathbb{R}^m$, under the weight vector $w\\in\\mathbb{R}^m$.  This results in the following prediction function:\n",
    "$$f_w(x) = x^Tw.$$\n",
    "Our goal is to find the optimal $w$ that maps $f_w(x)$ to $y$ as accurately as possible. To achieve that, we use gradient descent to minimize the squared loss as the cost function:\n",
    "$$L(w) = \\frac{1}{2}||x^Tw - y||^2.$$\n",
    "Once we have learned the optimal $w$ from training data, we can use the learned model to predict the real value for test examples.\n",
    "\n",
    "In essence, linear regression is predicting continuous variables instead of binary variables. Then a natural question is whether linear regression can be used to solve classification problems. The answer is affirmative. Considering a binary classification problem, one can set up a threshold to distinguish different categories. Say if the predicted continuous value is greater than the threshold value, the data point will be classified as positive. Otherwise, it will be classified as negative. However, these predictions are not sensible for classification because the predicted values range from $-\\infty$ to $\\infty$ which can lead to adverse consequences in real-time. As a result, logistic regression comes to play.\n",
    "\n",
    "### 1.2 Logistic Regression - Modeling **(8 points)** {-}\n",
    "\n",
    "The basic idea of logistic regression is to produce probabilities out of linear regression. To achieve this, it feeds the learned score $x^Tw$ into a non-linear transformation, which is known as a sigmoid function:\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "Note that the sigmoid function $\\sigma(z)$ transforms an unbounded real number $z$ into the interval [0,1]:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\sigma(z) \\rightarrow 1, \\quad as \\quad z \\rightarrow \\infty\\\\\n",
    "&\\sigma(0) = \\frac{1}{2}, \\\\\n",
    "&\\sigma(z) \\rightarrow 0, \\quad as \\quad  z \\rightarrow -\\infty.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Moreover, $\\sigma(z)$ is differentiable and its derivative has a nice property\n",
    "for convenient computation\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma'(z) &= -\\frac{1}{(1+e^{-z})^2}(-e^{-z}) \\\\\n",
    "&= \\frac{1}{1+e^{-z}}\\frac{e^{-z}}{1+e^{-z}} \\\\\n",
    "&= \\frac{1}{1+e^{-z}}(1 - \\frac{1}{1+e^{-z}}) \\\\\n",
    "&=\\sigma(z)(1-\\sigma(z)).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "As we can see, if we modify $f_w$ to\n",
    "$$f_w(x) = \\sigma(x^Tw),$$\n",
    "then we have a model that outputs probabilities of an example $x$ belonging to the positive class, or in a mathematical form:\n",
    "$$P(y=1|x;w) = \\frac{1}{1+e^{-x^Tw}}$$\n",
    "For the negative class we have\n",
    "$$P(y=0|x;w) = \\frac{e^{-x^Tw}}{1+e^{-x^Tw}} = 1 - P(y=1|x;w)$$\n",
    "At training time, we learn the value of $w$ to yield high values for $P(y=1|x;w)$ when $x$ is a positive example, and to yield low values for $P(y=1|x;w)$ when $x$ is a negative example.\n",
    "\n",
    "In practice, a real dataset contains many training examples. To make the computation efficient, in this experiment, we will process all data points at once instead of one at a time. Let's assume the dataset contains $n$ examples  which allows us to assemble a feature matrix $X =[x^1, x^2, ..., x^n]^T \\in\\mathbb{R}^{n\\times m}$,\n",
    "where $x^i$ represents the $i$-th training example, and $^T$ is matrix transpose.\n",
    "Then the prediction can be written as\n",
    "$$\\begin{pmatrix} P(y=1|x^1;w) \\\\ \\vdots \\\\ P(y=1|x^n; w)\\end{pmatrix} = \\frac{1}{1+e^{-Xw}},$$\n",
    "where $Xw$ leads to an $n$-dimensional vector,\n",
    "and all other operations on the right-hand side (e.g., exponentiation and reciprocal) are performed elementwise on a vector.\n",
    "\n",
    "In the following code block, implement the functions `sigmoid` and `logistic_regression` that can handle *batch* inputs (see the header of the functions and the unit tests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QRJDVn2lF7Pv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.2479464629129513e-47, 0.5, 0.5793242521487495]\n",
      "[0.6721209941557822, 0.5481528998322192, 0.5871972032187779, 0.6217613090778221]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "  \"\"\"\n",
    "  sigmoid function that maps inputs into the interval [0,1]\n",
    "  Your implementation must be able to handle the case when z is a vector (see unit test)\n",
    "  Inputs:\n",
    "  - z: a scalar (real number) or a vector\n",
    "  Outputs:\n",
    "  - trans_z: the same shape as z, with sigmoid applied to each element of z\n",
    "  \"\"\"\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  # define trans_z\n",
    "  trans_z = []\n",
    "  for input in z: \n",
    "    trans_z.append(1 / (1 + np.exp(-input)))\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  return trans_z\n",
    "\n",
    "def logistic_regression(X, w):\n",
    "  \"\"\"\n",
    "  logistic regression model that outputs probabilities of positive examples\n",
    "  Inputs:\n",
    "  - X: an array of shape (num_sample, num_features)\n",
    "  - w: an array of shape (num_features,)\n",
    "  Outputs:\n",
    "  - logits: a vector of shape (num_samples,)\n",
    "  \"\"\"\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  # multiple X by w\n",
    "  z = np.dot(X, w)\n",
    "  # use sigmoid on results\n",
    "  logits = sigmoid(z)\n",
    "\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  return logits\n",
    "\n",
    "# unit test\n",
    "# sample inputs:\n",
    "# z = np.array([215, -108, 0, 0.32])\n",
    "# X = np.array([[4.17022005e-01, 7.20324493e-01, 1.14374817e-04],\n",
    "#               [3.02332573e-01, 1.46755891e-01, 9.23385948e-02],\n",
    "#               [1.86260211e-01, 3.45560727e-01, 3.96767474e-01],\n",
    "#               [5.38816734e-01, 4.19194514e-01, 6.85219500e-01]])\n",
    "# w = np.array([0.20445225, 0.87811744, 0.02738759])\n",
    "\n",
    "# sample outputs:\n",
    "# out1 = sigmoid(z)\n",
    "# out1 : [1.00000000e+00 1.24794646e-47 5.00000000e-01 5.79324252e-01]\n",
    "# out2 = logistic_regression(X, w)\n",
    "# out2 : [0.67212099 0.5481529  0.5871972  0.62176131]\n",
    "\n",
    "z = np.array([215, -108, 0, 0.32])\n",
    "X = np.array([[4.17022005e-01, 7.20324493e-01, 1.14374817e-04],\n",
    "              [3.02332573e-01, 1.46755891e-01, 9.23385948e-02],\n",
    "              [1.86260211e-01, 3.45560727e-01, 3.96767474e-01],\n",
    "              [5.38816734e-01, 4.19194514e-01, 6.85219500e-01]])\n",
    "w = np.array([0.20445225, 0.87811744, 0.02738759])\n",
    "out1 = sigmoid(z)\n",
    "out2 = logistic_regression(X, w)\n",
    "print (out1)\n",
    "print (out2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUWyMHakF7ve"
   },
   "source": [
    "### 1.3 Loss function **(16 points)** {-}\n",
    "Recall that in linear regression, we optimize the model by minimizing the square loss:\n",
    "$$L(w) = \\frac{1}{2}||x^Tw - y||^2$$\n",
    "This loss is a convex function w.r.t $w$, hence the local minimum is also the global minimum.\n",
    "\n",
    "A naive way to extend linear regression to classification would be to use the loss $\\frac{1}{2}||\\sigma(x^Tw) - y||^2$, where $y$ is either 1 or 0 (for positive or negative, respectively).  This loss turns out very hard to optimize with an algorithm like gradient descent, because the loss function is not convex in $w$. In other words, there can be more than one local minimum and we wouldn't be assured to find the global minimum that best optimizes the loss.\n",
    "\n",
    "Instead of minimizing the square error as in the linear regression, we can resort to maximizing the likelihood of the training set as in many other machine learning algorithms. By making the standard assumption that training examples are generated independently, the likelihood function is given by\n",
    "$$\n",
    "\\begin{align}\n",
    "L(w) &= P(y^1, ..., y^n|x^1, ..., x^n; w) \\\\\n",
    "&= \\prod_{i=1}^n P(y^i|x^i;w)\\\\\n",
    "&= \\prod_{i=1}^n (\\sigma(w^T x^i))^{y^i}(1-\\sigma(w^T x^i))^{1-y^i}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To see the last step, just enumerate the two cases of $y^i = 1$ or $0$ since we are considering a binary classification problem.\n",
    "To simplify the computation, let us maximize the logarithm of the likelihood,\n",
    "which is equivalent to minimizing $-\\frac{1}{n}$ times the log-likelihood:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ell(w) &= -\\frac{1}{n}\\log \\prod_{i=1}^n (\\sigma(w^T x^i))^{y^i}(1-\\sigma(w^T x^i))^{1-y^i} \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^n \\left[-{y^i}\\log(\\sigma(w^T x^i)) - (1-y^i)\\log(1-\\sigma(w^T x^i))\\right].\n",
    "\\tag{1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The term inside the square bracket is generally referred to as cross-entropy loss, or logistic loss.\n",
    "That is, for a given data point $\\{x, y\\}$,\n",
    "it can be written as two cases:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ell(x; w) &=\n",
    "-{y}\\log(\\sigma(w^T x)) - (1-y)\\log(1-\\sigma(w^T x)) \\\\\n",
    "\\tag{2}\n",
    "&= \\begin{cases}\n",
    "-\\log(1-\\sigma(x^Tw)) & \\text{if } y=0\\\\\n",
    "-\\log(\\sigma(x^Tw))    & \\text{if } y=1\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In practice, we surely implement Eq 2 based on the value of $y$.\n",
    "Eq 1, however, gives more convenience in mathematical derivation as it unifies two cases neatly.\n",
    "If we plot the curve of $\\ell(x; w)$ as a function of $w$, we will see it is a convex function and therefore the gradient descent algorithm can find its global minima. Just like in linear regression, we will use the derivative of the loss function to calculate a gradient descent step. Please derive\n",
    "\n",
    "1. The gradient of $\\ell(x; w)$ in Eq 2 with respect to (w.r.t.) $w$.\n",
    "<font color='red'> Fill your solution in the following line </font>:\n",
    "\n",
    "$$\\nabla_w\\ell(x;) = $$\n",
    "\n",
    "\n",
    "2. the gradient of $\\ell(w)$ in Eq 1 w.r.t. $w$.  For computational efficiency, we would like to express it in terms of two the feature matrix $X = (x^1, x^2, ..., x^n)^T \\in\\mathbb{R}^{n\\times m}$ and the label vector $Y = (y^1, y^2, ..., y^n)\\in\\mathbb{R}^n$).  <font color='red'> Fill your solution in the following line </font>:\n",
    "\n",
    "$$\\nabla_w\\ell(w) = $$\n",
    "\n",
    "Then, implement a function `logistic_loss` that computes $\\ell(w)$ and the gradient in $w$.  You are strongly recommended to use the matrix/vector implementation as introduced in the class, as opposed to Figure 10.6 of the textbook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyj_fVzBuo03"
   },
   "outputs": [],
   "source": [
    "def logistic_loss(X, w, y):\n",
    "  \"\"\"\n",
    "  a function that compute the loss value for the given dataset (X, y) and parameter w;\n",
    "  It also returns the gradient of loss function w.r.t w\n",
    "  Here (X, y) can be a set of examples, not just one example.\n",
    "  Inputs:\n",
    "  - X: an array of shape (num_sample, num_features)\n",
    "  - w: an array of shape (num_features,)\n",
    "  - y: an array of shape (num_sample,), it is the ground truth label of data X\n",
    "  Output:\n",
    "  - loss: a scalar which is the value of loss function for the given data and parameters\n",
    "  - grad: an array of shape (num_featues,), the gradient of loss\n",
    "  \"\"\"\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  return loss, grad\n",
    "#unit test\n",
    "# inputs:\n",
    "# X = np.array([[0.67046751, 0.41730480, 0.55868983],\n",
    "#               [0.14038694, 0.19810149, 0.80074457],\n",
    "#               [0.96826158, 0.31342418, 0.69232262],\n",
    "#               [0.87638915, 0.89460666, 0.08504421]])\n",
    "# w = np.array([0.03905478, 0.16983042, 0.8781425 ])\n",
    "# Y = np.array([1, 1, 0, 1] )\n",
    "\n",
    "# sample outputs:\n",
    "# loss, grad = logistic_loss(X, w, Y)\n",
    "# loss: 0.626238298577102\n",
    "# grad: [-0.00483685, -0.09821878, -0.0080873 ]\n",
    "\n",
    "X = np.array([[0.67046751, 0.41730480, 0.55868983],\n",
    "              [0.14038694, 0.19810149, 0.80074457],\n",
    "              [0.96826158, 0.31342418, 0.69232262],\n",
    "              [0.87638915, 0.89460666, 0.08504421]])\n",
    "w = np.array([0.03905478, 0.16983042, 0.8781425 ])\n",
    "Y = np.array([1, 1, 0, 1])\n",
    "loss, grad = logistic_loss(X, w, Y)\n",
    "print (loss, grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgDSd4ABKUM3"
   },
   "source": [
    "## 2 Recognizing hand-written digits with logistic regression (26 points){-}\n",
    "\n",
    "We have gone through all the theoretical concepts of the logistic regression model. It's time to put hands on a real problem in which we aim to recognize images of hand-written digits. The dataset we will use is the Optical Recognition of Handwritten Digits dataset, and the description of the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits).\n",
    "\n",
    "### 2.1 Data preprocessing (not for grading){-}\n",
    "The original dataset contains 10 classes (digits 0 to 9). Since for now we are concerned about logistic regression for binary classification, we will only use a subset of the dataset that contains 360 examples from 2 classes (digits 0 and 1).  Each example is a $8\\times 8$ matrix (image) where each element is an integer in the range $[0,16]$. Let's load the dataset by using the off-the-shell method from `sklearn` and print out some images to get a good understanding of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AX98obkFPLh"
   },
   "outputs": [],
   "source": [
    "# set up the code for this experiment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "\n",
    "# load the digits dataset\n",
    "digits = load_digits(n_class=2)\n",
    "# digits is a dictionary-like object that hold all the features and labels,\n",
    "# along with some metadata about the dataset.\n",
    "# The features are stored in the '.data' member, a (#sample, #feature) array.\n",
    "# The labels are stored in the '.target' member.\n",
    "\n",
    "print(f'There are {len(digits.target)} examples in total.')\n",
    "print(f'All examples are images of hand-written digit {list(set(digits.target))[0]} \\\\\n",
    "        or hand-written digits {list(set(digits.target))[1]}')\n",
    "print(f'Each example is an array of shape {digits.data[0].shape}')\n",
    "print(f'An example of data point:\\n{digits.data[0]}')\n",
    "\n",
    "# You may wondering why the shape of data is (64,) instead of (8, 8). Actually,\n",
    "# You can access to matrix shape of data through the '.images' member.\n",
    "print(f'The shape of image is {digits.images[0].shape}')\n",
    "print(f'An example of 2D array data:\\n {digits.images[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kazXeCBCXLee"
   },
   "outputs": [],
   "source": [
    "# The data we are interested in is made up of 8x8 images of digits.\n",
    "# Let's have a look at the first 6 images that are drawn from the dataset.\n",
    "# For these images,\n",
    "#   we know the digit they represented is given in the 'target' of the dataset.\n",
    "_, axes = plt.subplots(1, 6)\n",
    "img_label = list(zip(digits.images, digits.target))\n",
    "for ax, (img, target) in zip(axes, img_label[:6]):\n",
    "  ax.set_axis_off()\n",
    "  ax.imshow(img, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "  ax.set_title('Label: %i' % target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oveqe9GK526A"
   },
   "source": [
    "### 2.2 Padding features (not for grading) {-}\n",
    "As we did in Lab 2, to simplify the notation, we pad the input $x$ by inserting 1 to the **beginning** so that we can absorb the bias term into the parameter $w$.\n",
    "\n",
    "The following code morphs the variable `digits.data` by concatenating 1 and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_h4w6JIZ8Cpk"
   },
   "outputs": [],
   "source": [
    "ones = np.ones(digits.data.shape[0]).reshape(-1, 1)\n",
    "digits.data = np.concatenate((ones, digits.data), axis=1)\n",
    "print(digits.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSwnaHBAoQbt"
   },
   "source": [
    "### 2.3 Create training and test sets (not for grading) {-}\n",
    "As we have practiced in our previous lab assignment, we will use the `train_test_split()` method to partition the dataset into training and test sets. In this experiment, we use 80% data for training and the remaining 20% data for testing. To ensure your results are replicable, we will set the `random_state` argument of `train_test_split()` to **1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y89isTi9qLcV"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, train_size=0.8, random_state=1)\n",
    "print(f'The training set contains {X_train.shape[0]} examples.')\n",
    "print(f'The testing set contains {X_test.shape[0]} examples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n39CSTpbvpHR"
   },
   "source": [
    "### 2.4 Feature Normalization (not for grading) {-}\n",
    "In the previous lab assignment, we have implemented the function `featureNormalization()` to normalize the features that have different scale. In this lab, we will learn to use the built-in function `StandardScaler()` in `scikit-learn`. As we did in `featureNormalization()`, `StandardScaler()` returns standardized features by removing the mean and scaling to unit variance.\n",
    "Please read through the [API documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for detailed instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJV3UYO1z4Zk"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u69oMti4KhzJ"
   },
   "source": [
    "### 2.5 Training the model with gradient descent **(26 points)**{-}\n",
    "Now after all the pre-processing, we can train a logisitic regression model with the training data.  It is quite straightforward to make predictions on test data by using the learned model. To simplify the task, when the probability of being positive is greater than 0.5, we classify the sample to 1. Otherwise, we classify it to 0.\n",
    "\n",
    "In this part, we will train the model with gradient descent. After that, predict the label for test examples and compute the test accuracy. You may want to follow the procedures below to obtain the results:\n",
    "+ Randomly initialize the parameter $w$ by `np.random.rand`.\n",
    "+ Use gradient descent to update $w$ (number of iteration `num_iters` and learning rate `lr` are provided).\n",
    "+ Plot the curve of the $\\ell(w)$ value as a function of how many update steps have been taken (you need a variable to store the history of $\\ell(w)$ values).\n",
    "+ Compute and report the test accuracy on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYGPnvLAEvkO"
   },
   "outputs": [],
   "source": [
    "num_iters = 200\n",
    "lr = 0.1\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrsS04vQm-Tj"
   },
   "source": [
    "## 3. Multinomial (multi-class) Logistic Regression (MLR) (50 points){-}\n",
    "\n",
    "So far we have built a logisitic regression model for binary classification. In this section, we aim to extend it to multinomial logistic regression for solving multi-class classification. More specifically, we expect the MLR model can predict one out of $k$ possible classes, where $k$ is the total number of classes.\n",
    "\n",
    "Recall that in binary logisitic regression, the output of the model is the probability of the positive class. Analogously, the MLR model should perform a series of mathematical operations to produce a vector encoding the probability that an example $x$ belongs to each class:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{pmatrix}\n",
    "  P(y=1|x; W) \\\\\n",
    "  \\vdots \\\\\n",
    "  p(y=k|x; W)\n",
    "\\end{pmatrix},\n",
    "\\quad where \\quad\n",
    "W = (w_1, \\ldots, w_k).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here $w_1, \\ldots, w_k$ are all $m$-dimensional vectors, one for each class.\n",
    "$W$ is an $m$-by-$k$ matrix.\n",
    "The class with the highest probability will be adopted as the prediction outcome for the given data $x$.\n",
    "Now the question is, how does the MLR model covert features to probability values? In binary logistic regression, we used the sigmoid function.\n",
    "In MLR, we can use the `softmax` to covert $(w_1^T x, \\ldots, w_k^T x)$ (which are often called logits) to probability values.\n",
    "For a $k$-class problem, this conversion formula is\n",
    "$$\n",
    "P(y=i|x; W) = \\frac{e^{w_i^Tx}}{\\sum_{j=1}^k e^{w_j^Tx}}.\n",
    "$$\n",
    "We will simply write\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{pmatrix}\n",
    "  P(y=1|x; W) \\\\\n",
    "  \\vdots \\\\\n",
    "  p(y=k|x; W)\n",
    "\\end{pmatrix}\n",
    "=\n",
    "softmax (W^\\top x).\n",
    "\\end{aligned}\n",
    "$$\n",
    "To summarize, a weight vector $w_i$ is learned for each class,\n",
    "which produces $k$ logits $\\{w_i^T x\\}_{i=1}^k$ for each example. Then `softmax` is subsequently applied to these logits to derive the probabilities for different classes.\n",
    "\n",
    "**(8 points)** Please implement `softmax()` and `MLR()` functions in the following code block.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcrM2A3iO7qW"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  \"\"\"\n",
    "  Convert logits for each possible outcomes to probability values.\n",
    "  In this function, we assume the input x is a 2D matrix of shape (num_sample, num_classes).\n",
    "  So we need to normalize each row by applying the softmax function.\n",
    "  Inputs:\n",
    "  - x: an array of shape (num_sample, num_classse) which contains the logits for each input\n",
    "  Outputs:\n",
    "  - probability: an array of shape (num_sample, num_classes) which contains the\n",
    "                 probability values of each class for each input\n",
    "  \"\"\"\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  #For numerical stability, you can consider to subtract the maximium of X to avoid overflow: all elements in x - maxvalue in x\n",
    "\n",
    "\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  return probability\n",
    "\n",
    "def MLR(X, W):\n",
    "  \"\"\"\n",
    "  performs logistic regression on given inputs X\n",
    "  Inputs:\n",
    "  - X: an array of shape (num_sample, num_feature)\n",
    "  - W: an array of shape (num_feature, num_class)\n",
    "  Outputs:\n",
    "  - probability: an array of shape (num_sample, num_classes)\n",
    "  \"\"\"\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  return probability\n",
    "\n",
    "# unit test\n",
    "# sample inputs:\n",
    "# X = np.array([[0.49157316, 0.05336255, 0.57411761],\n",
    "#               [0.14672857, 0.58930554, 0.69975836],\n",
    "#               [0.10233443, 0.41405599, 0.69440016],\n",
    "#               [0.41417927, 0.04995346, 0.53589641],\n",
    "#               [0.66379465, 0.51488911, 0.94459476]])\n",
    "# W = np.array([[0.58655504, 0.90340192, 0.1374747, 0.13927635],\n",
    "#               [0.80739129, 0.39767684, 0.1653542, 0.92750858],\n",
    "#               [0.34776586, 0.75081210, 0.7259979, 0.88330609]])\n",
    "\n",
    "# sample outputs:\n",
    "# out1 = softmax(X)\n",
    "# out1: [[0.36613449 0.23622627 0.39763924]\n",
    "#        [0.23281662 0.36242881 0.40475457]\n",
    "#        [0.23960744 0.32724969 0.43314287]\n",
    "#        [0.35408647 0.24599602 0.39991751]\n",
    "#        [0.31388902 0.27046263 0.41564835]]\n",
    "# out2 = MLR(X, W)\n",
    "# out2:\n",
    "# [[0.22210723 0.32004009 0.21385397 0.24399871]\n",
    "#  [0.2278552  0.24858598 0.19040101 0.33315781]\n",
    "#  [0.21922197 0.25283567 0.20870744 0.31923492]\n",
    "#  [0.22296738 0.30913599 0.2195647  0.24833193]\n",
    "#  [0.22047099 0.32241683 0.16806773 0.28904445]]\n",
    "\n",
    "X = np.array([[0.49157316, 0.05336255, 0.57411761],\n",
    "              [0.14672857, 0.58930554, 0.69975836],\n",
    "              [0.10233443, 0.41405599, 0.69440016],\n",
    "              [0.41417927, 0.04995346, 0.53589641],\n",
    "              [0.66379465, 0.51488911, 0.94459476]])\n",
    "W = np.array([[0.58655504, 0.90340192, 0.1374747, 0.13927635],\n",
    "              [0.80739129, 0.39767684, 0.1653542, 0.92750858],\n",
    "              [0.34776586, 0.75081210, 0.7259979, 0.88330609]])\n",
    "out1 = softmax(X)\n",
    "print (out1)\n",
    "out2 = MLR(X, W)\n",
    "print (out2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYEhL39TR0jC"
   },
   "source": [
    "### 3.1 Cross entropy loss **(16 points)**{-}\n",
    "\n",
    "For the MLR model, generally, we use the cross-entropy loss which generalizes the the loss function we used in binary logistic regression\n",
    "$$\n",
    "\\ell(W) = -\\frac{1}{n}\\sum_{i=1}^n y_i^T\\log(p_i),\n",
    "\\quad \\text{where} \\quad  \n",
    "p_i =\n",
    "\\begin{pmatrix}\n",
    "P(y_i = 1 | x_i; W) \\\\\n",
    "\\vdots \\\\\n",
    "P(y_i = k | x_i; W)\n",
    "\\end{pmatrix}\n",
    "= softmax(W^T x_i).\n",
    "$$\n",
    "Here $p_i\\in\\mathbb{R}^k$ is a probabiltiy vector of sample $x_i$. Then we apply element-wise logarithm on $p_i$ to obtain $\\log(p_i)\\in\\mathbb{R}^k$.\n",
    "In addition, $y_i$ is a one-hot vector, where the component corresponding to the correct label is 1, and all the other components are 0.\n",
    "For instance, in a 5-class classification problem (say, digits 0-4), when the ground truth label for a data point is digit 3 (i.e., the fourth class because of the digit 0), we have $y = (0, 0, 0, 1, 0)^T$.\n",
    "As a result if $y_i$ encodes the fact that the true class for the $i$-th example is $c$, then $y_i^T\\log(p_i)$ simply returns\n",
    "$\\log P(y_i = c | x_i; W)$.\n",
    "\n",
    "Now we need to derive the gradient of $\\ell(W)$ w.r.t. $W$,\n",
    "and express it in terms of $X = (x_1, x_2, ..., x_n)^T \\in \\mathbb{R}^{n\\times m}$ and $Y=(y_1, y_2, ..., y_n)^T \\in\\mathbb{R}^{n\\times k}$. <font color='red'> Please fill you solution in the following line </font>\n",
    "$$\\nabla_W\\ell(W)= $$\n",
    "\n",
    "Recall that if $\\ell$ is a function that maps a matrix $W \\in \\mathbb{R}^{m \\times k}$ to a real number, then $\\nabla_W \\ell(W)$ is also an $m$-by-$k$ matrix, and its $(r,s)$-th element is $\\frac{\\partial \\ell(W)}{\\partial W_{rs}}$.\n",
    "\n",
    "Now implement the function `cross_entropy_loss` that returns $\\ell(W)$ and its gradient.   You are strongly recommended to use the matrix/vector implementation as introduced in the class, as opposed to Figure 10.8 of the textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLq-0JNwTq9I"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(X, W, y):\n",
    "  \"\"\"\n",
    "  Inputs:\n",
    "  - X: an array of shape (num_sample, num_feature)\n",
    "  - W: an array of shape (num_feature, num_class)\n",
    "  - y: an array of shape (num_sample,)\n",
    "  Ouputs:\n",
    "  - loss: a scalar which is the value of loss function for the given data and parameters\n",
    "  - grad: an array of shape (num_featues, num_class), the gradient of the loss function\n",
    "  \"\"\"\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  return loss, grad\n",
    "\n",
    "# unit test\n",
    "# sample inputs:\n",
    "# X = np.array([[0.49157316, 0.05336255, 0.57411761],\n",
    "#               [0.14672857, 0.58930554, 0.69975836],\n",
    "#               [0.10233443, 0.41405599, 0.69440016],\n",
    "#               [0.41417927, 0.04995346, 0.53589641],\n",
    "#               [0.66379465, 0.51488911, 0.94459476]])\n",
    "# W = np.array([[0.58655504, 0.90340192, 0.1374747, 0.13927635],\n",
    "#               [0.80739129, 0.39767684, 0.1653542, 0.92750858],\n",
    "#               [0.34776586, 0.75081210, 0.7259979, 0.88330609]])\n",
    "# y = np.array([0, 1, 1, 0, 1])\n",
    "\n",
    "# sample outputs:\n",
    "# loss, grad = cross_entropy_loss(X, W, y)\n",
    "# loss:   1.3808433676397016\n",
    "# grad:[[-0.10040155, -0.07022596,  0.07138434,  0.09924316],\n",
    "#       [ 0.05164776, -0.21370799,  0.0615074 ,  0.10055283],\n",
    "#       [-0.06861677, -0.26705505,  0.13547167,  0.20020015]]\n",
    "\n",
    "X = np.array([[0.49157316, 0.05336255, 0.57411761],\n",
    "              [0.14672857, 0.58930554, 0.69975836],\n",
    "              [0.10233443, 0.41405599, 0.69440016],\n",
    "              [0.41417927, 0.04995346, 0.53589641],\n",
    "              [0.66379465, 0.51488911, 0.94459476]])\n",
    "W = np.array([[0.58655504, 0.90340192, 0.1374747, 0.13927635],\n",
    "              [0.80739129, 0.39767684, 0.1653542, 0.92750858],\n",
    "              [0.34776586, 0.75081210, 0.7259979, 0.88330609]])\n",
    "y = np.array([0, 1, 1, 0, 1])\n",
    "loss, grad = cross_entropy_loss(X, W, y)\n",
    "print (loss)\n",
    "print (grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9r3WiknkyMW"
   },
   "source": [
    "### 3.2 Learning the model on real dataset **(26 points)**{-}\n",
    "In this last section, we will experiment on a subset of the hand-written digits dataset, and the task is a 10-class (also known as 10-way) classification. Compared with binary classification, the procedure of doing 10-ways classification is pretty much the same. Hence, in the following code block, you will need to train a MLR model and test it on test data.\n",
    "\n",
    "You can perform the following main steps to obtain the results:\n",
    "+ Load the whole dataset that contains 10 classes\n",
    "+ Normalize the features\n",
    "+ Create training and test sets (80% for training and 20% for testing)\n",
    "+ Randomly initialize the weight matrix $W$ by `np.random.rand`\n",
    "+ Update $W$ with gradient descent\n",
    "+ Plot the value of $\\ell(W)$ as a function of the number of gradient descent steps taken\n",
    "+ Predict the label for all test examples\n",
    "+ Report the test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rejkRwJKkw3-"
   },
   "outputs": [],
   "source": [
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "# Loading dataset, you can take 1.2.1 as reference\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_si6Qa3wbuut"
   },
   "source": [
    "# Submission Instruction {-}\n",
    "\n",
    "You're almost done! Take the following steps to finally submit your work.\n",
    "\n",
    "1. After executing all commands and completing this notebook, save your `Lab_7.ipynb` as a PDF file, named as `X_Y_UIN.pdf`, where `X` is your first name, `Y` is your last name, and `UIN` is your UIN. Make sure that your PDF file includes all parts of your solution, including the plots.\n",
    "\n",
    "> * Print out all unit test case results before printing the notebook into a PDF.\n",
    "* If you use Colab, open this notebook in Chrome.  Then File -> Print -> set Destination to \"Save as PDF\".  If the web page freezes when printing, close Chrome and reopen the page. If Chrome doesn't work, try Firefox.\n",
    "* If you are working on your own computer, we recommend using the browser (not jupyter) for saving the PDF. For Chrome on a Mac, this is under *File->Print...->Open PDF in Preview*. When the PDF opens in Preview, you can use *Save...* to save it.\n",
    "* Sometimes, a figure that appears near the end of a page can get cut.  In this case, try to add some new lines in the preceding code block so that the figure is pushed to the beginning of the next page. Or insert some text blocks.\n",
    "\n",
    "2. Upload `X_Y_UIN.pdf` to Gradescope under `Lab_7_Written`.\n",
    "\n",
    "3. A template of `Lab_7.py` has been provided.  For all functions in `Lab_7.py`, copy the corresponding code snippets you have written into it, excluding the plot code.  **Do NOT** copy any code of plotting figures and do not import **matplotlib**.  This is because the auto-grader cannot work with plotting.  **Do NOT** change the function names.  \n",
    "\n",
    "4. Zip `Lab_7.py` and `Lab_7.ipynb` (**2 files**) into a zip file named `X_Y_UIN.zip`. Suppose the two files are in the folder `Lab_7`.  Then zip up the **two files inside the `Lab_7` folder**.  **Do NOT zip up the folder `Lab_7`** because the auto-grader cannot search inside a folder. Submit this zip file to Gradescope under `Lab_7_Code`.\n",
    "\n",
    "5. The autograder on Gradscope will be open all the time. We designed some simple test cases to help you check wehther your functions are executable. You will see the results of running autograder once you submit your code. Please follow the error messages to debug. Since those simple test cases are designed for debugging, it does not guaranttee your solution will work well on the real dataset. It is your responsibility to make your code logically correct. Since all functions are tested in batch, the autograder might take a few minutes to run after submission.\n",
    "\n",
    "<font color='red'>If you *only* try to get real-time feedback from auto-grader, it will be fine to just upload `Lab_7.py` to `Lab_7_Code`</font>.  However, the final submission for grading should still follow the above point 4.\n",
    "\n",
    "You can submit to Gradescope as often as you like. We will only consider your last submission before the deadline."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
