{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4w4C9qMucYv"
   },
   "source": [
    "## **Lab 2: k-Nearest Neighbour**\n",
    "\n",
    "CS 412\n",
    "\n",
    "***This lab is for group work.***\n",
    "\n",
    "In this lab, we will see how to implement and use k-Nearest Neighbour for classification tasks _step by step_.\n",
    "\n",
    "***Deadline:***\n",
    "**5 PM, Monday of Week 5 (Feb 6)**.\n",
    "\n",
    "\n",
    "## <font color='red'> Please refer to `Lab_Guideline.pdf` in the same Google Drive folder as this Jupyter notebook; the guidelines there apply to all the labs.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hutIH7_uyvjZ"
   },
   "source": [
    "## Problem 1: Implementation of the k-Nearest Neighbours (kNN) classifier **(65 points)**\n",
    "\n",
    "In Problem 1, you will implement kNN from scratch, which is a good exercise to make sure that you fully understand the algorithm.\n",
    "Do not use any library such as scikit-learn that already has kNN implemented.\n",
    "But you can use general libraries for array and matrix operations such as numpy.\n",
    "\n",
    "\n",
    "**Step 1. (20 points)** The kNN classifier mainly consists of two stages:\n",
    "\n",
    "1.   During training, the classifier takes the training data and simply stores it.\n",
    "2.   During testing, kNN classifies every test example $x$ by \n",
    "\n",
    "> i) finding the $k$ training examples that are most similar to $x$;\n",
    "\n",
    "> ii) outputing the most common label among these $k$ examples.\n",
    "\n",
    "To measure the similarity between samples, we commonly compute the Euclidean distance. The Euclidean distance (a.k.a. $L_2$ distance) between two examples $p$ and $q$ in an $n$-dimensional space is defined as the square root of:\n",
    "\n",
    "\\begin{equation}\n",
    "(p_1-q_1)^2 + (p_2-q_2)^2 + ... + (p_n-q_n)^2. \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "This term is equal to \n",
    "\\begin{equation}\n",
    "\\sum_i p_i^2 + \\sum_i q_i^2 - 2 \\sum_i p_i q_i. \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "With Euclidean distance, the smaller the value, the more similar the two examples are. Actually, there are many different ways to measure the similarity, such as cosine distance, Manhattan, Chebyshev, and Hamming distance. In practice, you can choose the one that suits your problem. For this lab, we will implement Equation (2) with a function `my_euclidean_dist` that  computes the Euclidean distances.\n",
    "\n",
    "**DO NOT use np.linalg.norm() or function from scipy.**  Make sure your implementation is generic, i.e., not hard coding the number of feature to 2, or the number of training example to 10.\n",
    "\n",
    "**Unit test:** to unit test `my_euclidean_dist`, you can construct two matrices by yourself, e.g., `X_train` being 3-by-2 and `X_test` being 2-by-2. Then you can compute the squared Euclidean distances by hand, and compare it with the result of your code.  See the last four lines of the following code block, which lie outside the definition of `my_euclidean_dist`.  You can uncomment them for testing, but comment them back when you finish the entire lab.\n",
    "\n",
    "`euclidean_dist` will be called eventually by the `knn_predict` function in Step 3 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "FWDHxYKZGV1B"
   },
   "outputs": [],
   "source": [
    "# set up code for this experiment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "ukpJSEIfBzt6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8. 10.  1.]\n",
      " [ 2.  8.  9.]]\n"
     ]
    }
   ],
   "source": [
    "def my_euclidean_dist(X_test, X_train):\n",
    "  \"\"\"\n",
    "  Compute the *squared* distance between each test example and each training example\n",
    "\n",
    "  Input:\n",
    "  - X_test: A numpy array of shape (num_test, dim_feat) containing test data\n",
    "  - X_train: A numpy array of shape (num_train, dim_feat) containing training data\n",
    "\n",
    "  Output:\n",
    "  - dists: A numpy array of shape (num_test, num_train) where \n",
    "           dist[i, j] is the squared Euclidean distance between \n",
    "           the i-th test example and the j-th training example\n",
    "  \"\"\"\n",
    "  num_test = X_test.shape[0]\n",
    "  num_train = X_train.shape[0]\n",
    "  dists = np.zeros((num_test, num_train))\n",
    "  # TODO:\n",
    "  # Compute the squared L2 distance between all test and training examples.\n",
    "  #\n",
    "  # One most straightforward way is to use nested for loop\n",
    "  # to iterate over all test and training samples.\n",
    "  # Here is the pseudo-code:\n",
    "  # for i = 0 ... num_test - 1\n",
    "  #    a[i] = square of the norm of the i-th test example\n",
    "  # for j = 0 ... num_train - 1\n",
    "  #    b[j] = square of the norm of the j-th training example\n",
    "  # for i = 0 ... num_test - 1\n",
    "  #    for j = 0 ... num_train - 1\n",
    "  #        dists[i, j] = a[i] + b[j] - 2 * np.dot(i-th test example, j-th training example)\n",
    "  # return dists\n",
    "  \n",
    "  \n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "  # calculate square of the norm of the i-th test example\n",
    "  for i in range(num_test):\n",
    "    a = sum(np.square(X_test[i]))\n",
    "    # calculate square of the norm of the i-th train example\n",
    "    for j in range(num_train):\n",
    "      b = sum(np.square(X_train[j]))\n",
    "      # calculate distance for i-th test example & i-th train example\n",
    "      dists[i,j] = a + b - 2* np.dot(X_test[i], X_train[j])\n",
    "\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)***** \n",
    "\n",
    "  return dists\n",
    "\n",
    "# Unit test code here (you can uncomment the four lines below to test)\n",
    "# Compute by hand to check if the result is correct.\n",
    "# The right matrix of squared distance should be\n",
    "# [[ 8 10  1]\n",
    "#  [ 2  8  9]]\n",
    "X_train = np.array([[1, 2], [0, 3], [-1, 1]])\n",
    "X_test = np.array([[-1, 0], [2, 1]])\n",
    "my_dists = my_euclidean_dist(X_test, X_train)\n",
    "print(my_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aEEdL4Oww8O"
   },
   "source": [
    "However, you can entirely avoid using loops by reformulating Equation (2) with linear algebra.  The trick is to reformulate the L2 distance as two broadcast sums and matrix multiplication.  \n",
    "\n",
    "**Task:** Try the following implementation and feel the speedup!  Understand the following implementation.  You do not need to write down your understanding or submit anything for it, but it will be helpful to understand it.\n",
    "\n",
    "**Note:**  Since Euclidean distance computation underlies all the subsequent experiments, its efficiency is highly important. Therefore, in the sequel, we will NOT use `my_euclidean_dist` that you just implemented.  Instead, we will use `euclidean_dist`.  However, your implementation of `my_euclidean_dist` will still be graded based on unit test; it will need to be copied to `Lab_1.py` (see submission instruction at the bottom of the page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "6XsyQxBYxBvp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8 10  1]\n",
      " [ 2  8  9]]\n"
     ]
    }
   ],
   "source": [
    "def euclidean_dist(X_test, X_train):\n",
    "  dists = np.add(np.sum(X_test ** 2, axis=1, keepdims=True), np.sum(X_train ** 2, axis=1, keepdims=True).T) - 2* X_test @ X_train.T\n",
    "  return dists\n",
    "\n",
    "# Unit test code here (you can uncomment the four lines below to test)\n",
    "X_train = np.array([[1, 2], [0, 3], [-1, 1]])\n",
    "X_test = np.array([[-1, 0], [2, 1]])\n",
    "dists = euclidean_dist(X_test, X_train)\n",
    "print(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_aUcYK-KINy"
   },
   "source": [
    "**Step 2. (20 points)**  Once distances are calculated, we can find the top $k$ nearest neighbors for each test example by retrieving from the dists matrix. \n",
    "In particular, for each test example $x$, we can sort all the training examples by their distance to $x$ then find the $k$ most nearest neighbors.  \n",
    "\n",
    "**HINT**: Recall from the lecture that `argsort` is useful for this purpose.\n",
    "\n",
    "**Note**: to run the unit test, you need to uncomment the unit test in the previous code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "lveUni0IT2G4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1.]\n",
      " [0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "def find_k_neighbors(dists, Y_train, k):\n",
    "  \"\"\"\n",
    "  find the labels of the top k nearest neighbors\n",
    "\n",
    "  Inputs:\n",
    "  - dists: distance matrix of shape (num_test, num_train)\n",
    "  - Y_train: A numpy array of shape (num_train) containing ground truth labels for training data\n",
    "  - k: An integer, k nearest neighbors\n",
    "\n",
    "  Output:\n",
    "  - neighbors: A numpy array of shape (num_test, k), where each row containts the \n",
    "               labels of the k nearest neighbors for each test example\n",
    "  \"\"\"\n",
    "  # TODO:\n",
    "  # find the top k nearest neighbors for each test sample.\n",
    "  # retrieve the corresponding labels of those neighbors.\n",
    "  # Here is the pseudo-code:\n",
    "  # for i = 0 ... num_test-1\n",
    "  #     idx = numpy.argsort(i-th row of dists)\n",
    "  #     neighbors[i] = Y_train(idx[0]), ..., Y_train(idx[k-1])\n",
    "  # return neighbors\n",
    "  # Advanced: You can accelerate the code by, e.g., argsort on the `dists` matrix directly\n",
    "\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  num_test = dists.shape[0]\n",
    "  neighbors = np.zeros((num_test, k))\n",
    "  # sort indices of dists by shortest to longest distance\n",
    "  idx = np.argsort(dists, axis=1)\n",
    "  \n",
    "  # return label for k closest neighbors\n",
    "  for i in range(num_test):\n",
    "    for j in range(k):\n",
    "      neighbors[i][j] = Y_train[idx[i][j]]\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  return neighbors\n",
    "\n",
    "# Unit test code here (you can uncomment the lines below to test)\n",
    "# Compute by hand to check if the result is correct.\n",
    "\"\"\"\n",
    "[[1. 0. 1.]\n",
    " [0. 1. 1.]]\n",
    "\"\"\"\n",
    "k = 3   # you can vary it as 1 or 3\n",
    "Y_train = np.array([0, 1, 1])\n",
    "neighbors = find_k_neighbors(dists, Y_train, k)\n",
    "print(neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISP74Eh2rr4i"
   },
   "source": [
    "**Step 3. (20 points)** Finally, we can put together `euclidean_dist` and `find_k_neighbors`, so that labels can be predicted for test examples.  In kNN, we take the labels of the $k$ nearest neighbors and find the most common one and assign it to the test sample.\n",
    "\n",
    "**Hint:** You may find [`np.unique`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html#numpy-unique) and `argmax` useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "LXqx94yVt41e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "def knn_predict(X_test, X_train, Y_train, k):\n",
    "  \"\"\"\n",
    "  predict labels for test data.\n",
    "\n",
    "  Inputs:\n",
    "  - X_test: A numpy array of shape (num_test, dim_feat) containing test data.\n",
    "  - X_train: A numpy array of shape (num_train, dim_feat) containing training data.\n",
    "  - Y_train: A numpy array of shape (num_train) containing ground truth labels for training data\n",
    "  - k: An integer, k nearest neighbors\n",
    "\n",
    "  Output:\n",
    "  - Y_pred: A numpy array of shape (num_test). Predicted labels for the test data.\n",
    "  \"\"\"\n",
    "  # TODO:\n",
    "  # find the labels of k nearest neighbors for each test example,\n",
    "  # and then find the majority label out of the k labels\n",
    "  #\n",
    "  # Here is the pseudo-code:\n",
    "  # dists = euclidean_dist(X_test, X_train)\n",
    "  # neighbors = find_k_neighbors(dists, Y_train, k)\n",
    "  # Y_pred = np.zeros(num_test, dtype=int)  # force dtype=int in case the dataset\n",
    "  #                                         # stores labels as float-point numbers\n",
    "  # for i = 0 ... num_test-1\n",
    "  #     Y_pred[i] = # the most common/frequent label in neighbors[i], you can\n",
    "  #                 # implement it by using np.unique\n",
    "  # return Y_pred\n",
    "\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  # find euclidean distance of x_test vals to x_train vals\n",
    "  dists = euclidean_dist(X_test, X_train)\n",
    "  # return the labels of the k nearest training vals\n",
    "  neighbors = find_k_neighbors(dists, Y_train, k)\n",
    "  \n",
    "  # calculate num_test\n",
    "  num_test = X_test.shape[0]\n",
    "  # initialize empty array for Y_pred of length num_test\n",
    "  Y_pred = np.zeros(num_test, dtype=int)\n",
    "  \n",
    "  # calculate most frequent label in neigbors add to Y_pred\n",
    "  for i in range(num_test):\n",
    "    labels, counts = np.unique(neighbors[i], return_counts=True)\n",
    "    Y_pred[i] = labels[np.argmax(counts)]\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  return Y_pred\n",
    "\n",
    "# Unit test code here (you can uncomment the lines below to test)\n",
    "# Compute by hand to check if the result is correct.\n",
    "\"\"\"\n",
    "[1 1]\n",
    "\"\"\"\n",
    "Y_pred = knn_predict(X_test, X_train, Y_train, k)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EAaUOyC4AY"
   },
   "source": [
    "**Step 4. (5 points)** Once we obtain the predicted labels, we need to implement a function to compare them against the true label and compute the error rate in percentage (i.e., a number between 0 and 100). In the following code block, implement the `compute_error_rate` function by following the specified inputs and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "Vr3PVo1_C7_r"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_error_rate(ypred, ytrue):\n",
    "  \"\"\"\n",
    "  Compute error rate given the predicted results and true label.\n",
    "  Inputs:\n",
    "  - ypred: array of prediction results.\n",
    "  - ytrue: array of true labels.\n",
    "    ypred and ytrue should be of same length.\n",
    "  Output:\n",
    "  - error rate: float number indicating the error in percentage\n",
    "                (i.e., a number between 0 and 100).\n",
    "  \"\"\"\n",
    "  # Here is the pseudo-code:\n",
    "  # err = 0\n",
    "  # for i = 0 ... num_test - 1\n",
    "  #     err = err + (ypred[i] != ytrue[i])  # generalizes to multiple classes\n",
    "  # error_rate = err / num_test * 100\n",
    "  # return error_rate\n",
    "  #\n",
    "  # Advanced (optional): \n",
    "  #   implement it in one line by using vector operation and the `mean` function\n",
    "\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  num_test = ypred.shape[0]\n",
    "  err = 0\n",
    "  for i in range(num_test):\n",
    "    err = err + (ypred[i] != ytrue[i])\n",
    "\n",
    "  error_rate = err / num_test * 100\n",
    "\n",
    "\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  return error_rate\n",
    "\n",
    "ypred = np.array([1,0,0,1,0])\n",
    "ytrue = np.array([1,0,0,1,1])\n",
    "\n",
    "compute_error_rate(ypred, ytrue)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNJ62NKozii3"
   },
   "source": [
    "## Problem 2: Optical character recognition (OCR) **(35 points)**{-}\n",
    "\n",
    "We will now apply the above developed function to a real world problem of optical character recognition (OCR).\n",
    "\n",
    "**Load the MNIST dataset.** In the following code block, we have downloaded the MNIST dataset and split the data into trainning and test sets. This part has already been done, and you can directly run it with no need of modifying the code.  But do make sure that you understand the code as it will be useful for future labs.\n",
    "\n",
    "**Note:** after running the code, the training data (Xtrain, ytrain) has 10,000 examples, and the test data (Xtest, ytest) also has 10,000 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "sgWufXl41uJX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "\n",
    "DATA_URL = ' http://www.cs.uic.edu/~zhangx/teaching/'\n",
    "\n",
    "# Download and import the MNIST dataset from Yann LeCun's website.\n",
    "# Each image is an array of 784 (28x28) float values  from 0 (white) to 1 (black).\n",
    "def load_data():\n",
    "    x_tr = load_images('train-images-idx3-ubyte.gz')\n",
    "    y_tr = load_labels('train-labels-idx1-ubyte.gz')\n",
    "    x_te = load_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_te = load_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    return x_tr, y_tr, x_te, y_te\n",
    "\n",
    "def load_images(filename):\n",
    "    maybe_download(filename)\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    return data.reshape(-1, 28 * 28) / np.float32(256)\n",
    "\n",
    "def load_labels(filename):\n",
    "    maybe_download(filename)\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    return data\n",
    "\n",
    "# Download the file, unless it's already here.\n",
    "def maybe_download(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        from urllib.request import urlretrieve\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(DATA_URL + filename, filename)\n",
    "\n",
    "Xtrain, ytrain, Xtest, ytest = load_data()\n",
    "\n",
    "train_size = 10000\n",
    "test_size  = 10000\n",
    "\n",
    "Xtrain = Xtrain[0:train_size]\n",
    "ytrain = ytrain[0:train_size]\n",
    "\n",
    "Xtest = Xtest[0:test_size]\n",
    "ytest = ytest[0:test_size]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-KNutaT130w"
   },
   "source": [
    "## 2.1 Effect of different numbers of training examples\n",
    "\n",
    "**(35 points)** In the following code block, we will compute the classification error of the 1-NN ($k=1$) for the MNIST dataset by calling the `knn_predict` function. We will study does the error change with different number of training examples.\n",
    "\n",
    "**Tasks**: train on the **first** $ntr$ number of training examples in (Xtrain, ytrain) that is produced by the above data-loading code, where $ntr$ is varied in $\\{100, 1000, 2500, 5000, 7500, 10000\\}$.\n",
    "1. Print the test error rate for each of these values of $ntr$.  Note that the above data-loading code produces 10,000 test examples stored in (Xtest, ytest). Just use all of them for testing, i.e., fixing the test set size to 10000.\n",
    "2. Plot a figure where the $x$-axis is the above values of $ntr$, and the $y$-axis is the test error rate.\n",
    "\n",
    "Directly calling `knn_predict` with the training and test set may cost too much memory.  So we will classify the test examples in batches, i.e., divide the test set into `nbtaches` number of subsets/batches, and predict for the first batch, then second batch, etc. For example, with 30 test examples and 5 batches, we first use `knn_predict` to classify test examples 0...5, then 6...11, ..., and finally 26...29.\n",
    "\n",
    "**Hint:** you may refer [here](https://matplotlib.org/tutorials/introductory/pyplot.html) for how to plot in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "lbnD99tN3WIG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial size =  100 error rate =  32.06 %\n",
      "trial size =  1000 error rate =  13.100000000000001 %\n",
      "trial size =  2500 error rate =  8.64 %\n",
      "trial size =  5000 error rate =  6.569999999999999 %\n",
      "trial size =  7500 error rate =  5.91 %\n",
      "trial size =  10000 error rate =  5.37 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  nbatches must be an even divisor of test_size. Increase if you run out of memory \n",
    "if test_size > 1000:\n",
    "  nbatches = 50\n",
    "else:\n",
    "  nbatches = 5\n",
    "\n",
    "# Let us first set up the index of each batch. \n",
    "# After running the next line, 'batches' will be a 2D array sized nbatches-by-m,\n",
    "# where m = test_size / nbatches.\n",
    "# batches[i] stores the indices (out of 0...test_size-1) for the i-th batch\n",
    "# You can run 'print(batches[3])' etc to witness the value of 'batches'.\n",
    "batches = np.array_split(np.arange(test_size), nbatches)\n",
    "ypred = np.zeros_like(ytest)\n",
    "trial_sizes = [100, 1000, 2500, 5000, 7500, 10000]\n",
    "trials = len(trial_sizes)\n",
    "error_rates = [0]*trials\n",
    "k = 1\n",
    "\n",
    "# Here is the pseudo code:\n",
    "# \n",
    "# for t = 0 ... trials-1  # loop over different number of training examples\n",
    "# \ttrial_size = trial_sizes[t]\n",
    "# \ttrial_X = Xtrain[...] # extract trial_size number of training examples from the whole training set\n",
    "# \ttrial_Y = Ytrain[...] # extract the corresponding labels\n",
    "# \tfor i = 0…nbatches—1\n",
    "# \t\typred[...] = # call knn_predict to classify the i-th batch of test examples.\n",
    "#                  # You should use 'batches' to get the indices for batch i.\n",
    "#                  # Then store the predicted labels also in the corresponding\n",
    "#                  # elements of ypred, so that after the loop over i completes,\n",
    "#                  # ypred will hold exactly the predicted labels of all test examples.\n",
    "# \terror_rates[t] = # call compute_error_rate to compute the error rate by \n",
    "#                     comparing ypred against ytest\n",
    "#   print a line like '#tr = 100, error rate = 50.3%'\n",
    "# plot the figure:\n",
    "# f = plt.figure()\n",
    "# plt.plot(...)\n",
    "# plt.xlabel(...)\n",
    "# plt.ylabel(...)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "for t in range(trials):\n",
    "  trial_size = trial_sizes[t]\n",
    "  trial_X = Xtrain[:trial_size]\n",
    "  trial_Y = ytrain[:trial_size]\n",
    "\n",
    "  # predict label for each batch\n",
    "  for i in range(nbatches):\n",
    "    ypred[:][batches[i]] = knn_predict(Xtest[batches[i]], trial_X, trial_Y, k)\n",
    "  # calculate error rate for each trial size\n",
    "  error_rates[t] = compute_error_rate(ypred, ytest)\n",
    "  print(\"trial size = \", trial_sizes[t], \"error rate = \", error_rates[t], \"%\")\n",
    "\n",
    "\n",
    "plt.plot(trial_sizes, error_rates)\n",
    "plt.title(\"How trial size affects error rate for KNN prediction\")\n",
    "plt.xlabel(\"trial size\")\n",
    "plt.ylabel(\"error rate\")\n",
    "plt.show()\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w0TZVbzIQ3Z"
   },
   "source": [
    "# Submission Instruction {-}\n",
    "\n",
    "You're almost done! Take the following steps to finally submit your work.\n",
    "\n",
    "1. After executing all commands and completing this notebook, save your `Lab_2.ipynb` as a PDF file, named as `X_Y_UIN.pdf`, where `X` is your first name, `Y` is your last name, and `UIN` is your UIN. Make sure that your PDF file includes all parts of your solution, including the plots. \n",
    "\n",
    "> * Print out all unit test case results before printing the notebook into a PDF.\n",
    "* If you use Colab, open this notebook in Chrome.  Then File -> Print -> set Destination to \"Save as PDF\".  If the web page freezes when printing, close Chrome and reopen the page. If Chrome doesn't work, try Firefox.\n",
    "* If you are working on your own computer, we recommend using the browser (not jupyter) for saving the PDF. For Chrome on a Mac, this is under *File->Print...->Open PDF in Preview*. When the PDF opens in Preview, you can use *Save...* to save it.\n",
    "* Sometimes, a figure that appears near the end of a page can get cut.  In this case, try to add some new lines in the preceding code block so that the figure is pushed to the beginning of the next page. Or insert some text blocks.\n",
    "\n",
    "2. Upload `X_Y_UIN.pdf` to Gradescope under `Lab_2_Written`.\n",
    "\n",
    "3. A template of `Lab_2.py` has been provided.  For all functions in `Lab_2.py`, copy the corresponding code snippets you have written into it, excluding the plot code.  **Do NOT** copy any code of plotting figures and do not import **matplotlib**.  This is because the auto-grader cannot work with plotting.  **Do NOT** change the function names.  \n",
    "\n",
    "4. Zip `Lab_2.py` and `Lab_2.ipynb` (**2 files**) into a zip file named `X_Y_UIN.zip`. Suppose the two files are in the folder `Lab_2`.  Then zip up the **two files inside the `Lab_2` folder**.  **Do NOT zip up the folder `Lab_2`** because the auto-grader cannot search inside a folder. Submit this zip file to Gradescope under `Lab_2_Code`. \n",
    "\n",
    "5. The autograder on Gradscope will be open all the time. We designed some simple test cases to help you check wehther your functions are executable. You will see the results of running autograder once you submit your code. Please follow the error messages to debug. Since those simple test cases are designed for debugging, it does not guaranttee your solution will work well on the real dataset. It is your responsibility to make your code logically correct. Since all functions are tested in batch, the autograder might take a few minutes to run after submission.\n",
    "\n",
    "If you *only* try to get real-time feedback from auto-grader, it will be fine to just upload `Lab_2.py` to `Lab_2_Code`.  However, the final submission for grading should still follow the above point 4.\n",
    "\n",
    "You can submit to Gradescope as often as you like. We will only consider your last submission before the deadline."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Z4w4C9qMucYv",
    "0w0TZVbzIQ3Z"
   ],
   "provenance": []
  },
  "interpreter": {
   "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
