{"cells":[{"cell_type":"markdown","metadata":{"id":"h-C8omlh6Pq4"},"source":["# **Lab 1: Basic Sampling and Linear Regression**\n","\n","CS 412\n","\n","***This is an individual lab, i.e., NOT for group work.***\n","\n","This is your first lab. You will see how to do multinomial sampling efficiently and how to do linear regression. You will also learn how to make real-valued predictions using a linear regression model. In particular, we will help you to get started with these algorithms _step by step_.\n","\n","***Deadline:***\n","**23:59, Wednesday of Week 3**.\n","\n","\n","## <font color='red'> Please refer to `Lab_Guideline.pdf` in the same Google Drive folder as this Jupyter notebook; the rules there apply to all the labs.</font>\n","\n"]},{"cell_type":"markdown","source":["# Problem 1: Basic Sampling **(24 points)** \n","\n","Say that we cast a die and want to know what the chance is of seeing a 1 rather than another digit. If the die is fair, all the six outcomes $\\{1, \\ldots, 6\\}$ are equally likely to occur, and thus we would see a $1$ in one out of six cases. Formally we state that $1$ occurs with probability $\\frac{1}{6}$.\n","\n","For a real die that we receive from a factory, we might not know those proportions and we would need to check whether it is tainted. The only way to investigate the die is by casting it many times and recording the outcomes. For each cast of the die, we will observe a value in $\\{1, \\ldots, 6\\}$. Given these outcomes, we want to investigate the probability of observing each outcome.\n","\n","One natural approach for each value is to take the\n","individual count for that value and to divide it by the total number of tosses.\n","This gives us an *estimate* of the probability of a given *event*. The *law of\n","large numbers* tell us that as the number of tosses grows this estimate will draw closer and closer to the true underlying probability. Before going into the details of what is going here, let us try it out.\n","\n","To start, let us import the necessary packages.\n"],"metadata":{"id":"4R9Wx1nBO3Ta"}},{"cell_type":"code","source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","np.random.seed(1)"],"metadata":{"id":"N6bxPYxKPD7a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.1 Basic Sampling for multinomial distribution (7 points) \n","\n","Next, we will want to be able to cast the die. In statistics we call this process of drawing examples from probability distributions *sampling*.\n","The distribution that assigns probabilities to a number of discrete choices is called the *multinomial distribution*. We will give a more formal definition of\n","*distribution* later, but at a high level, think of it as just an assignment of\n","probabilities to events.\n","\n","Given a vector of probabilities (e.g., $p=(p_1, \\ldots, p_6) =$ (0.05, 0.1, 0.15, 0.2, 0.25, 0.25)), `np.random.multinomial`($n,p$) will simulate the cast for $n$ times independently, and it returns a $k$-dimensional vector, whose $i$-th entry encodes count of outcome $i$ ($i \\in \\{1,2,\\ldots,k\\}$). In this example, $k=6$.  Based on it, we can compute the fraction of occurrences for each of the $k$ outcomes.\n","\n","In the code below, make sure that you do not hard code $k$ into 6.  Leave it as a variable that can be readily read from the dimensionality of the input argument.\n"],"metadata":{"id":"jIRdiK2jPiQp"}},{"cell_type":"code","source":["def estimate(p, nSample):\n","  \"\"\"\n","  Estimate the value of p by drawing samples with varying number of cast\n","  Input:\n","    - p:    a 1-D numpy array of size k (number of events), \n","            encoding the probability of each of the k outcomes\n","    - nSample: a 1-D numpy array of size m specifying the sample sizes.\n","            Effectively, we run m number of experiments,\n","            with each experiment drawing nSample[j] number of samples (j=0,...,m-1)\n","  Outputs:\n","    - ProbEst: an m-by-k matrix (2-D numpy array), \n","                whose j-th row (j >= 0) encodes the probability estimate \n","                for the k events based on nSample[j] number of samples\n","  \"\"\"  \n","\n","  np.random.seed(1)\n","  k = p.size\n","  m = nSample.size\n","  ProbEst = np.empty([m, k], dtype=float)\n","  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","  # for n = nSample[0], nSample[1], ..., nSample[-1] (btw, -1 mean?)\n","  #    Throw the dice for n times by calling np.random.multinomial(n, p)\n","  #    Estimate p_1, ..., p_k and record it in a row of the ProbEst matrix\n"," \n","  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","  return ProbEst\n","\n","\"\"\"\n","Unit test case below\n","You should get the following results:\n","[[0.         0.2        0.         0.2        0.4        0.2       ]\n"," [0.         0.05       0.15       0.2        0.3        0.3       ]\n"," [0.03333333 0.13333333 0.1        0.3        0.33333333 0.1       ]]\n","\"\"\"\n","p = np.array([0.05, 0.1, 0.15, 0.2, 0.25, 0.25])\n","nSample = np.arange(10, 40, 10)\n","ProbEst = estimate(p, nSample)\n","print(ProbEst)"],"metadata":{"id":"DIzefoWsSFQn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.2  Plot the estimates (7 points)\n","\n","Now use `matplotlib.pyplot.plot` to plot a figure whose horizontal $x$-axis is the number of samples drawn ($n$), and $n$ increments from 10 to 1000 at the step size of 10.  The plot has 6 curves in six different colors, each corresponding to the estimation of $p_i$ ($y$-axis) based on the $n$ casts.  A good plot should\n","\n","1. Properly label the two axes by meaningful text.\n","2. Properly label the [ticks](https://www.tutorialspoint.com/matplotlib/matplotlib_setting_ticks_and_tick_labels.htm) of the two axes.  This includes the decision of the range of their values.  Most time, you can leave it automatically set by Python.  But if it does not look good, you can manually set it. \n","3. Properly set the range of the $y$-axis.  That for the $x$-axis is given.\n","4. Provide legend. If the location that Python automatically chooses clouds some important parts of the plot, then manually set the legend location.\n","\n","Although it is not required for this lab, a figure often needs to be included in a paper or report.  In such cases, you should also pay attention to the line width, line style (which can help distinguish curves when color information is lost in black-white printing), font size of the legend, axis label, and tick labels."],"metadata":{"id":"D_eE_8HspNlg"}},{"cell_type":"code","source":["# Since this question is on plotting, no auto-grading can be done.\n","# Your answer will be graded manually based on the part between\n","#     *****START OF YOUR CODE  and   *****END OF YOUR CODE\n","\n","p = np.array([0.05, 0.1, 0.15, 0.2, 0.25, 0.25])\n","\n","nSample = np.arange(10, 1000+1, 10)  # think why we add 1 to 1000?\n","\n","ProbEst = estimate(p, nSample) # Call the estimate function you just wrote\n","\n","plt.figure(figsize=(10, 5))\n","\n","# Plot using the ProbEst matrix\n","# Hint: it suffices to call plt.plot just once (no penalty if you call it multiple times)\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n"],"metadata":{"id":"wYe70KZUo_Oa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.3 Efficient sampling (10 points) \n","\n","The experiment in the previous sub-question needs to call `np.random.multinomial`($n$,$p$) many times.  This is not efficient.  Fortunately, this function provides a third argument that allows one to repeat the $n$-cast for $s$ times. np.random.multinomial($n, p, s$) returns an $s$-by-$n$ matrix, where each row is as before, but different rows represent different experiments.  See the manual of the multinomial function [here](https://numpy.org/doc/stable/reference/random/generated/numpy.random.multinomial.html).\n","\n","Now redo the task in Section 1.1, but now you are only allowed to call np.random.multinomial **once**. The function you will write takes two input parameters `nSample_start` and `nSample_end`. **The latter is required to be a multiple of the former**.  So in the sense of section 1.1, the  function you will implement below will be equivalent to calling the *estimate* function (from Section 1.1) as follows:\n","\n","    nSample = np.arange(nSample_start, nSample_end+1, nSample_start)\n","    estimate(p, nSample)\n","\n","**Hint**: You may find the function `numpy.cumsum` useful. Also decide on the  value of $s$ carefully."],"metadata":{"id":"5Wz87F9XsnG4"}},{"cell_type":"code","source":["\n","def estimate_efficient(p, nSample_start, nSample_end):\n","  \"\"\"\n","  Estimate the value of p by drawing samples with varying number of nSample\n","  This function will call np.random.multinomial only once.\n","\n","  Input:\n","  - p: a 1-D numpy array of size k (number of events), \n","            encoding the probability of each of the k outcomes\n","  - nSample_start: an integer specifying the starting/minimum number of samples\n","  - nSample_end: an integer specifying the ending/maximum number of samples (inclusive)\n","    We require that nSample_end must be a multiple of nSample_start\n","\n","  In the sense of section 1.1, the estimate function there will equivalently call\n","    nSample = np.arange(nSample_start, nSample_end+1, nSample_start)\n","    estimate(p, nSample)\n","\n","  Outputs:\n","  - ProbEst: an m-by-k matrix (2-D numpy array), where m = nSample_end/nSample_start.\n","              The j-th (j >= 0) row encodes the probability estimate \n","                for the k events based on nSample_start*(j+1) number of samples\n","  \"\"\"\n","\n","  assert(nSample_end % nSample_start == 0)\n","\n","  np.random.seed(1)\n","\n","  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","  # Some processing to aggregate the results\n"," \n","\n","  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","  return ProbEst\n","\n","\"\"\"\n","Unit test case below.\n","You should get the following results:\n","[[0.         0.2        0.         0.2        0.4        0.2       ]\n"," [0.         0.1        0.05       0.2        0.35       0.3       ]\n"," [0.         0.1        0.06666667 0.26666667 0.36666667 0.2       ]]\n","\"\"\"\n","\n","p = np.array([0.05, 0.1, 0.15, 0.2, 0.25, 0.25])\n","probEst_eff = estimate_efficient(p, 10, 30)\n","print(probEst_eff)\n"],"metadata":{"id":"MsqOR0niu79o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.4 Plot the result from Efficient sampling (0 points) \n","\n","Now use `matplotlib.pyplot.plot` to plot the result from **efficient** sampling using *estimate_efficient*. \n","\n","Everything will be exactly the same as in Section 1.2, except that `estimate` is replaced by `estimate_efficient`.  So there is no point associated with this question.  It is only meant for yourself to appreciate the result."],"metadata":{"id":"kkAz3qLQvfpV"}},{"cell_type":"code","source":["p = np.array([0.05, 0.1, 0.15, 0.2, 0.25, 0.25])\n","ProbEst_eff = estimate_efficient(p, 10, 1000) \n","\n","plt.figure(figsize=(10, 5))\n","\n","# Copy your code from Section 1.2 to plot using the ProbEst_eff matrix\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n"],"metadata":{"id":"fd2i6AkHvdsn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iJoS8gz6ZXfV"},"source":["# Problem 2: Linear regression **(76 points)** \n","\n","In this section, we will explore linear regression models.  The dataset we will use for this section is Wine Qualuty, whose description can be found [here](http://archive.ics.uci.edu/ml/datasets/Wine+Quality). This dataset contains **4898** examples, each containing **11** features (the first 11 columns), and the **last** (12-th) column is the value we want to predict. The dataset can be downloaded here [`winequality-white.csv`](https://www.cs.uic.edu/~zhangx/teaching/winequality-white.csv) (our code will download it directly). \n","\n","Different from classification models, a regression model is used to predict real values rather than the category an example belongs to. Linear regression is a linear approach to modeling the relationship between features and real value target. To perform supervised learning, we represent the hypothesis as a linear function of features ($x$) to predict the output ($y$).\n","\n","\\begin{equation}\n","f(x) = \\theta_0 + \\theta_1x_1 + ... + \\theta_nx_n    \\tag{8}\n","\\end{equation}\n","\n","Here $\\theta_i$'s are the **parameters** parameterizing the space of linear functions mapping from $\\mathcal{X}$ to $\\mathcal{Y}$. Our goal is to **learn** these parameters so that we can find a linear function in this hypothesis space to estimate the output $y$.\n","\n","To simplify the notation and ease the computation, we **pad** the input $x$ by letting $x_0=1$. That is, for an example with three features $x=[x_1, x_2, x_3]^\\top$, the padded feature vector will be $x=[1,x_1, x_2, x_3]^\\top$. Then, the linear function can be written as:\n","\n","\\begin{equation}\n","f_\\theta(x) = \\sum_{j=0}^n \\theta_j x_j = \\theta^\\top x    \\tag{9}\n","\\end{equation}\n","where on the right-hand side above we are viewing $\\theta := [\\theta_0, \\theta_1, \\ldots, \\theta_n]^\\top$ and $x$ both as vectors, and here $n = 11$ is the number of features. \n","\n","Given a training set, the way to learn these parameters is to make $f_\\theta(x)$ close to $y$. To measure the closeness, we use Mean-Squared-Error (MSE) here. The loss function can therefore be defined as:\n","\n","\\begin{equation}\n","L(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m(f_\\theta(x^{(i)})-y^{(i)})^2 = \\frac{1}{2m}\\sum_{i=1}^m(\\theta^\\top x^{(i)}-y^{(i)})^2,  \\tag{10}\n","\\end{equation}\n","where the superscript $(i)$ denotes the $i$-th example, \n","and $m$ is the total number of training samples. To learn the parameter $\\theta$, our goal is to **minimize** the above loss function. In this lab, we will explore two different methods to learn the parameter: \n","\n","1. Gradient descent\n","2. Closed-form solution (root of the gradient)"]},{"cell_type":"markdown","source":["## 2.1 Data preprocessing **(12 points)**\n","\n","Once we have received the dataset, we first need to preprocess it.  Very often, the features in a dataset are of very different scale, which can slow down the optimization for Eq (10). To accelerate it, we need to normalize each feature by substracting its mean value, and then dividing by its standard deviation (std). Assuming $X_i = [x_i^{(1)}, ... , x_i^{(m)}]^\\top$ is the $i$-th feature in the training set (across the $m$ examples), the normalized feature $i$ for the $j$-th training example can be computed by:\n","\\begin{equation}\n","\\hat{x}^j_i = \\frac{x^j_i - m_i}{s_i},\n","\\text{ where } m_i = mean(X_i), \\text{ and } s_i = std(X_i).\n","  \\tag{11}\n","\\end{equation}"],"metadata":{"id":"bg-AAL0qmAB5"}},{"cell_type":"markdown","metadata":{"id":"sQ3aIqTn4QSt"},"source":["**Step 1: normalize the training set (6 points)** {-}\n","\n","In the following code block, implement a function `featureNormalization`. The input is the training set. The output is the normalized training set, along with the mean and std of each features. You will need the mean and std to apply to the test set later."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eyuuNjr8KITJ"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","def featureNormalization(X):\n","  \"\"\"\n","  Normalize each feature for the input set\n","  Input:\n","  - X: a 2-D numpy array of shape (num_train, num_features)\n","  Outputs:\n","  - X_normalized: a 2-D numpy array of shape (num_train, num_features)\n","  - X_mean: a 1-D numpy array of length (num_features)\n","  - X_std: a 1-D numpy array of length (num_features)\n","  \"\"\"\n","  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","  \n","  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","  \n","  return X_normalized, X_mean, X_std\n","\n","\"\"\"\n"," Unit test case below.\n"," Should print\n","[[ 0.49109521 -0.40242644 -0.92156595 -0.1052653 ]\n"," [ 0.90298151  1.37532553  1.3897809   1.27398003]\n"," [-1.39407672 -0.97289909 -0.46821496 -1.16871473]]\n","[0.99 3.12 4.47 4.51]\n","[0.63124216 2.26128282 1.34553583 3.70492465]\n","\"\"\"\n","X = np.array([[1.30,2.21,3.23,4.12], [1.56, 6.23, 6.34, 9.23], [0.11, 0.92, 3.84, 0.18]])\n","X_normalized, X_mean, X_std = featureNormalization(X)\n","print(X_normalized)\n","print(X_mean)\n","print(X_std)"]},{"cell_type":"markdown","metadata":{"id":"op0kAryrPYgU"},"source":["**Step 2: normalize the test set (6 points)** {-}\n","\n","The above normalization function will be used for the training set. At test time, we will need to normalize the test data in the same way. However, we shouldn't compute new mean and std from the test set itself, because it may be inconsistent with the training data.  Instead, we will apply the mean $m_i$ and std $s_i$ computed from the training set.  Given a text example $[x_1, \\ldots, x_m]^\\top$, we just transform $x_i$ into $(x_i - m_i)/s_i$,\n","where $m_i$ and $s_i$ are computed from the training data as in the *where* clause of Eq (11).\n","\n","In the following code block, implement a function `applyNormalization`, which normalizes the test set for each feature using the provided mean and std."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NWuTPf6yTSaC"},"outputs":[],"source":["def applyNormalization(X, X_mean, X_std):\n","  \"\"\"\n","  Normalize each feature for the input set X\n","  Input:\n","  - X: a 2-D numpy array of shape (num_test, num_features)\n","  - X_mean: a 1-D numpy array of length (num_features)\n","  - X_std: a 1-D numpy array of length (num_features)\n","\n","  Output:\n","  - X_normalized: a 2-D numpy array of shape (num_test, num_features)  \n","  \"\"\"\n","\n","  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","          \n","  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","  return X_normalized\n","\n","\"\"\"\n","  Unit test case\n","  Should print\n","  [[ 3.00e-01  1.21e+00  6.15e-01  4.02e+01]\n","   [ 5.60e-01  5.23e+00  2.17e+00  9.13e+01]\n","   [-8.90e-01 -8.00e-02  9.20e-01  8.00e-01]]\n","\"\"\"\n","X =  np.array([[1.30,2.21,3.23,4.12], [1.56, 6.23, 6.34, 9.23], \n","               [0.11, 0.92, 3.84, 0.18]])\n","X_mean = np.array([1.0, 1.0, 2.0, 0.1])\n","X_std = np.array([1.0, 1.0, 2.0, 0.1])\n","X_normalized = applyNormalization(X, X_mean, X_std)\n","print(X_normalized)\n"]},{"cell_type":"markdown","metadata":{"id":"oVAlHwT_Ts_l"},"source":["##2.2 Gradient Descent **(28 points)**{-}\n","\n","In this section, you will need to implement the gradient descent algorithm that trains the linear regression model. Some introductions to gradient descent can be found [here](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html)."]},{"cell_type":"markdown","metadata":{"id":"qP3V48YIt_yo"},"source":["**Step 1: implement the loss function (6 points)** {-} \n","\n","As introduced at the begining of this problem, we will use MSE to measure the loss. In the following code block, implement a function `computeMSE`. Follow Equation (10), and the function should compute the MSE for the input set with the given $\\theta$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_I9hUncTPMU"},"outputs":[],"source":["def computeMSE(X, y, theta):\n","  \"\"\"\n","  Compute MSE for the input set (X,y) with theta\n","  Inputs:\n","  - X: a 2-D numpy array of shape (num_samples, num_features+1)\n","  - y: a 1-D numpy array of length (num_samples)\n","  - theta: a 1-D numpy array of length (num_features+1)\n","  Output:\n","  - error: MSE, a real number\n","  \"\"\"\n","  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","  \n","  return error\n","\n","\"\"\"\n","  Unit test case:\n","  Should print 73.0\n","\"\"\"\n","X = np.array([[2.0, 1.0, 3.0], [3.0, 6.0, 2.0]])\n","y =  np.array([1.0, 1.0])\n","theta = np.array([1.0, 2.0,1.0])\n","error = computeMSE(X, y, theta)\n","print(error)\n"]},{"cell_type":"markdown","metadata":{"id":"eXvZOOiCVyaS"},"source":["**Step 2: compute the gradient of the loss function (14 points)** {-} \n","\n","Recall that our goal is to find the parameter $\\theta$ that can minimize the loss $L(\\theta)$. To find the $\\theta$ with gradient descent method, we start from some initial $\\theta$, and then repeatedly perform the update:\n","\\begin{equation}\n","\\theta = \\theta - \\alpha\\nabla_{\\theta}L(\\theta).    \\tag{12}\n","\\end{equation}\n","\n","Here $\\alpha > 0$ is a step size, a.k.a., learning rate.\n","To enable this update rule, we first need to compute the gardient in $\\theta$. \n","\n","1. Derive the gradient of $\\theta$ from Eq (10), and \n","<font color='red'> type the result in the following line:</font>\n","\n","$$\n","\\nabla_{\\theta}L(\\theta) = \n","$$\n","\n","Note for computational efficiency, your expression is not allowed to have $\\sum_{i=1}^m$, and cannot have any multiplication of two matrices.  Multiplications of a matrix and a vector is allowed. Here is a hint.  Let $a_i$ and $b_i$ be vectors.  Then\n","$$\n","  \\sum_i (\\theta^\\top b_i) a_i = \\sum_i a_i (b_i^\\top \\theta) = \\left(\\sum_i a_i b_i^\\top \\right) \\theta = (A B^\\top) \\theta = A (B^\\top \\theta),\n","$$\n","where $A = [a_1, ..., a_m]$ and $B = [b_1, ..., b_m]$.\n","\n","2. Then, implement a function `computeGradient` to compute the gradient $\\nabla_{\\theta}L(\\theta)$ by following the expression you just derived above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9nTxP7YJaCo6"},"outputs":[],"source":["def computeGradient(X, y, theta):\n","  \"\"\"\n","  Compute the gradient of theta\n","  Inputs:\n","  - X: A 2-D numpy array of shape (num_train, num_features+1)\n","  - y: A 1-D numpy array of length (num_train)\n","  - theta: A 1-D numpy array of length (num_features+1)\n","  Output:\n","  - gradient: A 1-D numpy array of length (num_features+1)\n","  \"\"\"\n","\n","  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","  \n","  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","  return gradient\n","\n","\"\"\" \n","  Unit test case:\n","  Should return\n","  [30. 51. 25.]\n","\"\"\" \n","X = np.array([[2.0, 1.0, 3.0], [3.0, 6.0, 2.0]])\n","y =  np.array([1.0, 1.0])\n","theta = np.array([1.0, 2.0,1.0])\n","gradient = computeGradient(X, y, theta)\n","print(gradient)\n"]},{"cell_type":"markdown","metadata":{"id":"8BCElx6TayKV"},"source":["**Step 3: implement the gradient descent algorithm (8 points)** {-} \n","\n","Now we can use the update rule in Equation (12) to find the $\\theta$ that minimizes $L(\\theta)$. We start from some initial $\\theta_0$, then repeatedly take a step in the direction of steepest decrease of $L$. The $\\alpha$ in Equation (12) indicates how large the step we want to take at every update. We repeat the updates for a certain number of iterations, and the last updated $\\theta$ will be the $\\theta$ we find.\n","In the following code block, implement a function `gradientDescent`, which updates $\\theta$ for `num_iters` times and records the loss value (MSE) at every iteration."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lcmwpVi3fOoA"},"outputs":[],"source":["def gradientDescent(X, y, theta, alpha, num_iters):\n","  \"\"\"\n","  Update theta using equation (12) for num_iters times.\n","  Input: \n","  - X: a numpy array of shape (num_train, num_features+1)\n","  - y: a numpy array of shape (num_train, 1)\n","  - theta: a 1-D numpy array of length (num_features+1)\n","  - alpha: learning rate, a scalar\n","  - num_iters: an integer specifying how many steps to run the gradient descent\n","  Outputs:\n","  - theta: the final theta, a 1-D numpy array of length (num_features+1). \n","           You can directly overwrite the theta in the input argument, and return it.\n","  - Loss_record: a 1-D numpy array of length (num_iters), \n","          recording the loss value of Eq (10) at every iteration, \n","  \"\"\"\n","  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","  \n","  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","  return theta, Loss_record\n","\n","\"\"\"\n","  Unit test case:\n","  Should return\n","  [[0.3322825 0.858839 0.446925 ]]\n","  [37.5778     19.36559064 10.00046345]\n","\"\"\"\n","X = np.array([[2.0, 1.0, 3.0], [3.0, 6.0, 2.0]])\n","y =  np.array([1.0, 1.0])\n","theta = np.array([1.0, 2.0, 1.0])\n","alpha = 0.01\n","num_iters = 3\n","theta, Loss_record = gradientDescent(X, y, theta, alpha, num_iters)\n","print(theta)\n","print(Loss_record)"]},{"cell_type":"markdown","metadata":{"id":"lvegb8zigFpe"},"source":["## 2.3 Train the linear regression model with gradient descent **(14 points)**{-}\n","\n","Now we are ready to chain all the above functions together to perform the linear regression training on the Wine Quality dataset. \n","\n","**Step 1: load the data (not for grading)**\n","\n","The last column of the resulting \"data\" variable is the label.  Read the first row of the downloaded winequality-white.csv for more descriptions of the columns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0kiPKQ9XHDfg"},"outputs":[],"source":["# First load the data (this code block is not for grading)\n","\n","import urllib.request\n","import shutil\n","\n","url = 'https://www.cs.uic.edu/~zhangx/teaching/winequality-white.csv'\n","file_name = 'winequality-white.csv'\n","with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n","    shutil.copyfileobj(response, out_file)\n","\n","data = np.genfromtxt(file_name, delimiter=\";\", skip_header=1)\n","print(data.shape)"]},{"cell_type":"markdown","metadata":{"id":"u-0vYx8MJX7c"},"source":["**Step 2: training and testing (14 points)**\n","\n","After loading the dataset, split the dataset into training and test sets. Please split the first **4000** samples as training set and the rest as test set. Then perform the following:\n","\n","*   Normalize training set features\n","*   Pad the normalized training features by a constant 1, as the new first feature\n","*   Initialize $\\theta$ as a zero vector\n","*   Update $\\theta$ using gradient descent (`num_iters` and `alpha` are provided)\n","*   **Plot** a figure where $x$-axis is the number of iterations, $y$-axis is the loss value (MSE).\n","*   Apply normalization to test set features, pad the features\n","*   Compute the test error (MSE) and **print** out the test error"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYbVsdEbj2By"},"outputs":[],"source":["\n","num_train = 4000\n","alpha = 0.01\n","num_iters = 500\n","\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"]},{"cell_type":"markdown","metadata":{"id":"EePmmYxrkOo4"},"source":["## 2.4 Effect of different learning rate (8 points) {-}\n","\n","To investigate the effect of learning rate, repeat the learning process (gradient descent) with different learning rate in $[1.0, 0.1, 0.01, 0.001]$. \n","**Plot** 4 figures corresponding to different learning rates, where the $x$-axis is the number of iterations, \n","and the $y$-axis is the loss value (MSE). **Print** the test error (MSE) respectively in the format \"test MSE for using learning rate __ is __\" (four lines in total)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8ANzIIam0NV"},"outputs":[],"source":["learning_rates = [1.0, 0.1, 0.01, 0.001]\n","Loss_record = np.zeros((len(learning_rates), num_iters))\n","test_error = np.zeros(len(learning_rates))\n","\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"]},{"cell_type":"markdown","metadata":{"id":"X2Ox89manO-n"},"source":["## 2.5 Closed-form solution **(14 points)** {-}\n","\n","\n","Gradient descent minimizes $L$ by updating $\\theta$ iteratively. There is another way to find the $\\theta$ explicitly. Indeed, by finding the root of the gradient $\\nabla_\\theta L(\\theta)$ (i.e., the $\\theta$ such that $\\nabla_\\theta L(\\theta) = 0$), we can obtain a closed-form solution of $\\theta$ that minimizes the loss $L$."]},{"cell_type":"markdown","metadata":{"id":"zw9FrxOB7KE_"},"source":["**Step 1: find the root of the gradient to obtain the closed-form solution of $\\theta$  (10 points)**\n","\n","<font color='red'> Type your result in the following lines:</font>\n","\n","$$\n","\\nabla_{\\theta}L(\\theta) = 0 \\quad \\Rightarrow \\quad \n","$$\n","Then, implement a function `closeForm` to compute the closed-form solution of $\\theta$ using the expression you have derived above. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HU3ltzQ1r9U3"},"outputs":[],"source":["def closeForm(X, y):\n","  \"\"\"\n","  Compute close form solution for theta\n","  Inputs:\n","  - X: a numpy array of shape (num_train, num_features+1)\n","  - y: a 1-D numpy array of length (num_train)\n","  Output:\n","  - theta: a 1-D numpy array of length (num_features+1)\n","  \"\"\"\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","  \n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","  return theta\n","\n","\"\"\"\n","  Unit test case:\n","  Should return\n","  [ 0.76470588 -0.17647059 -0.11764706]\n","\"\"\"\n","X = np.array([[2.0, 1.0, 3.0], [3.0, 6.0, 2.0], [-1, 0, 2.0]])\n","y = np.array([1.0, 1.0, -1.0])\n","theta = closeForm(X, y)\n","print(theta)"]},{"cell_type":"markdown","metadata":{"id":"RUIWxmR0sY4J"},"source":["**Step 2: evaluate the test error using closed-form solution (4 points)** {-} \n","\n","Compute a new $\\theta$ using the closed-form solution. Evaluate the new $\\theta$ on test set by **printing** the test error (MSE) in the format: \"test MSE using close form solution is : __\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHtEAKdXtRoD"},"outputs":[],"source":["# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","print('test MSE using close form solution is : ', test_error)\n"]},{"cell_type":"markdown","metadata":{"id":"_si6Qa3wbuut"},"source":["# Submission Instruction {-}\n","\n","You're almost done! Take the following steps to finally submit your work.\n","\n","1. After executing all commands and completing this notebook, save your `Lab_1.ipynb` as a PDF file, named as `X_Y_UIN.pdf`, where `X` is your first name, `Y` is your last name, and `UIN` is your UIN. Make sure that your PDF file includes all parts of your solution, including the plots. \n","\n","> * Print out all unit test case results before printing the notebook into a PDF.\n","* If you use Colab, open this notebook in Chrome.  Then File -> Print -> set Destination to \"Save as PDF\".  If the web page freezes when printing, close Chrome and reopen the page. If Chrome doesn't work, try Firefox.\n","* If you are working on your own computer, we recommend using the browser (not jupyter) for saving the PDF. For Chrome on a Mac, this is under *File->Print...->Open PDF in Preview*. When the PDF opens in Preview, you can use *Save...* to save it.\n","* Sometimes, a figure that appears near the end of a page can get cut.  In this case, try to add some new lines in the preceding code block so that the figure is pushed to the beginning of the next page. Or insert some text blocks.\n","\n","2. Upload `X_Y_UIN.pdf` to Gradescope under `Lab_1_Written`.\n","\n","3. A template of `Lab_1.py` has been provided.  For all functions in `Lab_1.py`, copy the corresponding code snippets you have written into it, excluding the plot code.  **Do NOT** copy any code of plotting figures and do not import **matplotlib**.  This is because the auto-grader cannot work with plotting.  **Do NOT** change the function names.  \n","\n","4. Zip `Lab_1.py` and `Lab_1.ipynb` (**2 files**) into a zip file named `X_Y_UIN.zip`. Suppose the two files are in the folder `Lab_1`.  Then zip up the **two files inside the `Lab_1` folder**.  **Do NOT zip up the folder `Lab_1`** because the auto-grader cannot search inside a folder. Submit this zip file to Gradescope under `Lab_1_Code`. \n","\n","5. The autograder on Gradscope will be open all the time. We designed some simple test cases to help you check wehther your functions are executable. You will see the results of running autograder once you submit your code. Please follow the error messages to debug. Since those simple test cases are designed for debugging, it does not guaranttee your solution will work well on the real dataset. It is your responsibility to make your code logically correct. Since all functions are tested in batch, the autograder might take a few minutes to run after submission.  Also note that some questions are not covered by the auto-grader, and so the total score you see from the auto-grader is not 100.\n","\n","6. Sections 1.1 and 1.3 involve random functions, which makes auto-grading very difficult.  We are using the following code to tolerate small difference between your function's result and the reference result:\n","\n","    `self.assertIsNone(np.testing.assert_almost_equal(your_res, ref_res, decimal=2))`\n","\n","    So if the auto-test fails, try to add something like np.random.seed(x) (x = 1, 2, ...) in your function implementation, so that randomness luckily attains an error below 2 decimal points.  If it keeps failing and you are really confident, then ignore it and we will check your code manually anyway.\n","\n","\n","\n","<font color='red'> **If you only try to get real-time feedback from auto-grader, it will be fine to just upload** `Lab_1.py` to `Lab_1_Code`</font>.  However, the final submission for grading should still follow the above point 4.\n","\n","You can submit to Gradescope as often as you like. We will only consider your last submission before the deadline."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}