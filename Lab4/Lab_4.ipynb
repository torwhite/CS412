{"cells":[{"cell_type":"markdown","metadata":{"id":"Z4w4C9qMucYv"},"source":["## **Lab 4: Multivariate Methods**\n","\n","CS 412\n","\n","***This lab can be conducted in groups or individually.***\n","\n","In this lab, we will delve into multi-variate Gaussian distribution, including its inference, sampling, and visualization.  There are a lot of plotting in this lab, and they cannot be auto-graded.\n","\n","***Deadline:***\n","**23:59, Oct 17**.\n","\n","\n","<font color='red'> Please refer to `Lab_Guideline.pdf` in the same Google Drive folder as this Jupyter notebook; the guidelines there apply to all the labs.</font>"]},{"cell_type":"markdown","source":["To start, let us import the necessary packages."],"metadata":{"id":"AcNY-fDvaHro"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dMB3YGWRELPD"},"outputs":[],"source":["# set up code for this experiment\n","import numpy as np\n","import scipy.linalg\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","np.random.seed(1)"]},{"cell_type":"markdown","metadata":{"id":"hutIH7_uyvjZ"},"source":["## Problem 1: Exercise 4.1 of Alpaydin (4 pt){-}\n","\n","Write the code that generates a Bernoulli sample with given parameter $p$, and\n","the code that calculates $\\hat{p}$ from the sample.\n","\n","The sampler will be based on the function `np.random.random_sample`, which returns (an array of) uniformly distributed samples from [0.0, 1.0).  To turn them into Bernoulli samples, just threshold with $p$.  Then use the resulting binary value to compute the estimate $\\hat{p}$.  So we are done in three steps.\n","\n","**Important:** for efficiency, you are **not** allowed to use loops in your implementation, meaning that you should not call `np.random.random_sample` to just return a single random number."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XsyQxBYxBvp"},"outputs":[],"source":["def ex4_1(p, nSample):\n","  \"\"\"\n","  Inputs:\n","  - p: a real number, which specifies the parameter of Bernoulli\n","  - nSample: an integer which is the  number of samples to draw\n","\n","  Output:\n","  - phat: the estimate of p from the samples\n","  \"\"\"\n","  np.random.seed(1)\n","  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","  return phat\n","\n","\"\"\"\n","Unit test\n","Should print: some value close to 0.2 (it's random)\n","\"\"\"\n","p = 0.2\n","nSample = 1000\n","phat = ex4_1(p, nSample)\n","print (phat)"]},{"cell_type":"markdown","metadata":{"id":"T_aUcYK-KINy"},"source":["## Problem 2: Exercise 4.3 of Alpaydin (23 pt){-}\n","\n","Write the code that generates a real-valued normal sample with given $\\mu$ and $\\sigma$, and the code that calculates $m$ and $s$ from the sample. Do the same using the Bayes’ estimator assuming a Gaussian prior distribution for $μ$."]},{"cell_type":"markdown","metadata":{"id":"GvSUGyL_eKWG"},"source":["### 2.1 Implement the density function of a Gaussian distribution (4 pt) {-}\n","\n","Write a function that, given the value of mean $\\mu$ and standard deviation $\\sigma$, computes the density of the univariate Gaussian distribution at $x$.  Here $x$ can be a vector, and the result should be a vector of the same size, with the $i$-th element being the density of x[i]."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oxctxyQclcsY"},"outputs":[],"source":["def nrmf(x, mu, sigma):\n","  '''\n","  Given mean mu and standard deviation sigma,\n","  compute the density of the univariate Gaussian distribution at x.\n","  Here x can be a vector, and the result should be a vector of the same size,\n","  with the i-th element being the density of x[i].\n","  Input:\n","    - x:    a 1-D numpy array specifying where to query the probability density\n","    - mu:   a real number specifying the mean of the Gaussian\n","    - sigma: a real number specifying the standard deviation of the Gaussian\n","  Outputs:\n","    - p:  a 1-D numpy array specifying the pdf at each element of x\n","  '''\n","  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","  return p\n","\n","'''\n","Unit test\n","We compare our result with the density computed by scipy\n","'''\n","import scipy.stats\n","\n","x = np.random.random_sample(3)\n","mu = 1\n","sigma = 2\n","density = nrmf(x, mu, sigma)\n","ref_density = scipy.stats.norm(mu, sigma).pdf(x)\n","print(density)\n","print(ref_density)\n"]},{"cell_type":"markdown","metadata":{"id":"RiEJgtBVhBpq"},"source":["### 2.2 Implement the Bayes’ estimator assuming a Gaussian prior distribution for $μ$. (6 pt) {-}\n","\n","In a Gaussian distribution $N(\\mu, \\sigma^2)$, assume $\\mu$ has a Gaussian prior $N$(priorMean, priorStd^2).  Then compute $\\mu_{Bayes}$: the Bayes' estimate of mu by using the equation on slide 8 of Parametric Methods lecture slides: $E[\\theta | X ] = ...$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xoAYQt7hhEG2"},"outputs":[],"source":["def bayes_estimator(x, sigma, priorMean, priorStd):\n","  '''\n","    Compute mu_bayes: the Bayes' estimate of mu by using the equation on slide 8\n","  Input:\n","    - x:    a 1-D numpy array specifying the samples from the Gaussian distribution\n","    - sigma: a real number specifying the standard deviation of the Gaussian\n","    - priorMean: a real number specifying the mean of the Gaussian prior on mu\n","    - priorStd: a real number specifying the standard deviation of the Gaussian prior on mu\n","  Outputs:\n","    - mu_post: a real number specifying the Bayes' estimate of mu by using the equation on slide 8\n","  '''\n","\n","  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","  return mu_post\n","\n","'''\n","Unit test\n","You should get 1.25\n","'''\n","np.random.seed(1)\n","sigma = 2\n","priorMean, priorStd = (-1, 2)\n","x = np.array([1, 2, 3])\n","bayes_estimator(x, sigma, priorMean, priorStd)"]},{"cell_type":"markdown","metadata":{"id":"d6xKSsNOeIKL"},"source":["### 2.3  Do the following computation and plotting (8 pt) {-}\n","\n","1. Plot the density function of $N(\\mu, \\sigma^2)$ with the $x$-axis ranging in `xrange` (an input argument)\n","\n","2. Draw $N$ samples from $N(\\mu, \\sigma^2)$ using `np.random.normal`.  In the same figure from item 1, plot the $N$ samples in by `plt.scatter`. Each sample $x_i$ will lead to a red point at coordinate $(x_i, 0)$ ($y$-axis is $0$).\n","\n","3. Apply maximum-likelihood estimation (MLE) on these $N$ samples to estimate the $\\mu$ as $\\mu_{MLE}$ and $\\sigma$ as $\\sigma_{MLE}$.  Then plot the density function of $N(\\mu_{MLE}, \\sigma_{MLE}^2)$ in the same plot as item 1.\n","\n","4. Call `bayes_estimator` you implemented before by using the $N$ samples and a prior Gaussian with mean `priorMean` and standard deviation `priorStd` (both are input arguments).  Denote the Bayes estimate of $\\mu$ as $\\mu_{Bayes}$.\n","\n","5. Plot the density function of $N(\\mu_{Bayes}, \\sigma^2)$ in the same figure as item 1.  Note we do not consider a Bayes estimate of $\\sigma$, and will just directly use the given $\\sigma$ for plotting.  This is different from item 3 above.\n","\n","Make sure that the three curves in the plot use different colors and line styles.  At a good position, put the legend as \"Actual\", \"MLE\", and \"Bayes\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ruVAheRljIdn"},"outputs":[],"source":["def ex4_3(xrange, mu, sigma, priorMean, priorStd, N):\n","  '''\n","  Inputs:\n","    - xrange: a 1-D numpy array specifying the range of x to make plots\n","    - mu: a real number as the mean of the Gaussian\n","    - sigma: a real number as the standard deviation of the Gaussian\n","    - N: an integer specifying how many samples to draw\n","  Outputs: nil\n","  '''\n","  np.random.seed(1)\n","\n","  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","\n","\n","\n","  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","'''\n","This is the real test you will be graded on:\n","'''\n","mu, sigma = (0, 5)\n","priorMean, priorStd = (mu, 3)\n","x_start, x_end, step= (mu - 3*sigma, mu + 3*sigma, 0.1)\n","xrange = np.arange(x_start, x_end, step)\n","N = 3\n","ex4_3(xrange, mu, sigma, priorMean, priorStd, N)\n"]},{"cell_type":"markdown","metadata":{"id":"PDS8Z-EU6f7z"},"source":["### 2.4 Call `ex4_3` with the same input arguments as before, but now set $N$ to $1000$  (5 pt) {-}\n","\n","Discuss briefly how the new plot changes from  the previous one by writing in the paragraph below.  Pay attention to $\\mu$, $\\mu_{MLE}$, and $\\mu_{Bayes}$.\n","\n","<font color='red'> My observation is: (type the result here)</font>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ALbPX_5qDeby"},"outputs":[],"source":["'''\n","This is the real test you will be graded on:\n","'''\n","mu, sigma = (0, 5)\n","priorMean, priorStd = (mu, 3)\n","x_start, x_end, step= (mu - 3*sigma, mu + 3*sigma, 0.1)\n","xrange = np.arange(x_start, x_end, step)\n","N = 1000\n","ex4_3(xrange, mu, sigma, priorMean, priorStd, N)"]},{"cell_type":"markdown","metadata":{"id":"ISP74Eh2rr4i"},"source":["## Problem 3: Exercise 4.6 of Alpaydin (26 pt){-}\n","\n","For a two-class problem, suppose the likelihood models for both classes are Gaussians, and their covariances are different.  Visualize the posterior probability, and use parametric classification to estimate the discriminant points."]},{"cell_type":"markdown","metadata":{"id":"ysW73JwOpBBQ"},"source":["### 3.1 Compute the likelihood and posterior probability (8 pt) {-}\n","\n","Let $p(x|C_1) = N(\\mu_1, \\sigma_1^2)$ and $p(x|C_2) = N(\\mu_2, \\sigma_1^2)$.  On the points specified by `xrange`, compute likelihood $p(x|C_1)$ and posterior $p(C_1|x)$, along with $p(x|C_2)$ and $p(C_2|x)$.\n","\n","Note, your code should not have any for loop because the function `nrmf` you implemented before can take an array as the first input argument."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LXqx94yVt41e"},"outputs":[],"source":["def comp_posterior(mu1, sigma1, p1, mu2, sigma2, xrange):\n","  \"\"\"\n","  Compute the likelihood and posterior proability for x values in xrange\n","  Inputs:\n","  - mu1: a real number specifying the mean of Gaussian distribution p(x|C1)\n","  - sigma1: a real number specifying the standard deviation of Gaussian distribution p(x|C1)\n","  - p1: a real number in [0, 1] specifying the prior probability of C1, i.e., P(C1)\n","        P(C2) will be automatically inferred by 1 - p1.\n","  - mu2: a real number specifying the mean of Gaussian distribution p(x|C2)\n","  - sigma2: a real number specifying the standard deviation of Gaussian distribution p(x|C2)\n","  - xrange: a 1-D numpy array specifying the range of x to evaluate likelihood and posterior\n","\n","  Outputs:\n","  - l1: a 1-D numpy array in the same size as xrange, recording p(x|C1)\n","  - post1: a 1-D numpy array in the same size as xrange, recording p(C1|x)\n","  - l2: a 1-D numpy array in the same size as xrange, recording p(x|C2)\n","  - post2: a 1-D numpy array in the same size as xrange, recording p(C2|x)\n","  \"\"\"\n","  np.random.seed(1)\n","\n","  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","  # p(x|C1)\n","\n","  # p(x|C2)\n","\n","  # p(C1|x)\n","\n","  # p(C2|x)\n","\n","  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","  return l1, post1, l2, post2\n","\n","\"\"\"\n","Unit test below.\n","Should print:\n","[0.00443185 0.05399097 0.24197072]\n","[0.9999999  0.87502269 0.10818288]\n","[2.97030006e-10 5.14092999e-03 1.32980760e+00]\n","[1.00532537e-07 1.24977307e-01 8.91817115e-01]\n","\"\"\"\n","mu1 = 3.0\n","sigma1 = 1.0\n","p1 = 0.4\n","mu2 = 2.0\n","sigma2 = 0.3\n","xrange = np.arange(0,3,1)\n","\n","l1, post1, l2, post2 = comp_posterior(mu1, sigma1, p1, mu2, sigma2, xrange)\n","print(l1)\n","print(post1)\n","print(l2)\n","print(post2)"]},{"cell_type":"markdown","metadata":{"id":"tKRzQiJAo5nd"},"source":["### 3.2 Compute the two discriminant points (8 pt) {-}\n","\n","Use the results in Exercise 4.4 of Alpaydin's book to compute, analytically, the two values of $x$ such that  $p(C_1|x) = p(C_2|x)$.  Denote them as $x_1$ and $x_2$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"blR7_ZkjksSS"},"outputs":[],"source":["def find_dpoint(mu1, sigma1, p1, mu2, sigma2):\n","  '''\n","  Find the discriminant points of two Gaussians\n","  Inputs:\n","  - mu1: a real number specifying the mean of Gaussian distribution p(x|C1)\n","  - sigma1: a real number specifying the standard deviation of Gaussian distribution p(x|C1)\n","  - p1: a real number in [0, 1] specifying the prior probability of C1, i.e., P(C1)\n","        P(C2) will be automatically inferred by 1 - p1.\n","  - mu2: a real number specifying the mean of Gaussian distribution p(x|C2)\n","  - sigma2: a real number specifying the standard deviation of Gaussian distribution p(x|C2)\n","  Output:\n","  - x: a 1-D numpy array with two elements, recording the discriminant points\n","  '''\n","\n","  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","\n","\n","  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","  return np.array([x1, x2])\n","\n","\"\"\"\n","Unit test below.\n","Should print:\n","[2.55457643 1.24762137]\n","\"\"\"\n","mu1 = 3.0\n","sigma1 = 1.0\n","p1 = 0.4\n","mu2 = 2.0\n","sigma2 = 0.3\n","\n","dpoint = find_dpoint(mu1, sigma1, p1, mu2, sigma2)\n","print(dpoint)"]},{"cell_type":"markdown","metadata":{"id":"GAC9lATBo2ok"},"source":["### 3.3  Plot using the previous functions (10 pt) {-}\n","\n","1. Plot the density of $p(x|C_1)$ and $p(x|C_2)$ in one plot, with the $x$-axis range being the input argument `xrange`.  Use **dashed** line style ('--'), and color $p(x|C_1)$ as red, and $p(x|C_2)$ as black.\n","\n","2. In the previous figure, plot two more curves $p(C_1|x)$ and $p(C_2|x)$, both as a function of $x$ as given by `xrange`.  Use **solid** line style ('-'), and color $p(C_1|x)$ as red, and $p(C_2|x)$ as black.\n","\n","3. Use `find_dpoint` to find the two discriminant points $x_1$ and $x_2$.  Put a small solid blue circle at $(x_1, p(C_1|x_1))$ and $(x_2, p(C_1|x_2))$.  Hint: you may want to reuse `comp_posterior` here.\n","\n","Note that 1 and 2 can be accomplished by using the function `comp_posterior`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nSkwfNu1kHHP"},"outputs":[],"source":["def ex4_6(mu1, sigma1, p1, mu2, sigma2, xrange):\n","  '''\n","  Plot the likelihood and posterior probabilities for two classes\n","    and plot the discrminant points.\n","  Inputs:\n","  - mu1: a real number specifying the mean of Gaussian distribution p(x|C1)\n","  - sigma1: a real number specifying the standard deviation of Gaussian distribution p(x|C1)\n","  - p1: a real number in [0, 1] specifying the prior probability of C1, i.e., P(C1)\n","        P(C2) will be automatically inferred by 1 - p1.\n","  - mu2: a real number specifying the mean of Gaussian distribution p(x|C2)\n","  - sigma2: a real number specifying the standard deviation of Gaussian distribution p(x|C2)\n","  - xrange: a 1-D numpy array specifying the range of x to evaluate likelihood and posterior\n","  Outputs:\n","    Nil\n","  '''\n","\n","  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","\n","\n","\n","  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","mu1 = 3.0\n","sigma1 = 1.0\n","p1 = 0.4\n","mu2 = 2.0\n","sigma2 = 0.3\n","xrange = np.arange(0,6,0.05)\n","\n","ex4_6(mu1, sigma1, p1, mu2, sigma2,  xrange)"]},{"cell_type":"markdown","metadata":{"id":"cZGZBBt6j7I1"},"source":["## Problem 4: Exercise 4.7 of Alpaydin (0 pts) {-}\n","\n","Assume a linear model and then add 0-mean Gaussian noise to generate a\n","sample. Divide your sample into two as training and validation sets. Use\n","linear regression using the training half. Compute error on the validation set.\n","Do the same for polynomials of degrees 2 and 3 as well. You may find **np.polyfit()** is useful.\n","\n","The code has been fully proivded and you are expected to digest it fully.  Also play with the code to try anything you like.  There is nothing to submit for this question."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o7_WK5lykGjO"},"outputs":[],"source":["def ex4_7(nSample, n_train, degree):\n","  \"\"\"\n","  Inputs:\n","  - degree: List, degree list of polynoimial function.\n","  - nSample: Integer, number of samples you want to draw\n","  - n_train: Integer, number of samples in the training set\n","\n","  Output:\n","  - error: List, error of different degrees on validation set\n","  \"\"\"\n","\n","  #generate samples\n","  np.random.seed(1)\n","  # X = np.arange(nSample)\n","  X = np.random.rand(nSample)\n","  Y = X + np.random.standard_normal(nSample)\n","\n","  #split samples into training set and validation set\n","  order = np.random.permutation(len(X))\n","  X_train = X[order[:n_train]]\n","  Y_train = Y[order[:n_train]]\n","  X_test = X[order[n_train:]]\n","  Y_test = Y[order[n_train:]]\n","\n","  error = np.zeros_like(degree)\n","\n","  for d in degree:\n","  #train fit\n","    A = np.polyfit(X_train, Y_train, d)\n","    Y_predict = np.zeros_like(Y_test)\n","\n","    #compute the error\n","    for i in range(len(A)):\n","      Y_predict += A[i]*np.power(X_test, d-i)\n","\n","    error = np.power((Y_predict-Y_test),2)\n","    error[d] = (1.0/(2*nSample)) * error.sum(axis=0)\n","    print ('The error of polynoimial degree {degree} : {error}'.format(degree = d, error= error[d]))\n","  return error\n","\n","nSample = 100\n","n_train = 50\n","degree = [1, 2, 3]\n","error = ex4_7(nSample, n_train, degree)"]},{"cell_type":"markdown","metadata":{"id":"VdQoqFW9khl9"},"source":["## Problem 5: Exercise 5.2 of Alpaydin (32 pt) {-}\n","\n","Generate a sample from a 2-dimensional normal density $N(\\mu,\\Sigma)$, calculate $m$ and $S$, and compare them with $\\mu$ and $\\Sigma$. Check how your estimates change as the sample size changes.\n","\n","Specificially, draw $N = 10, 50, 500$ samples. For each value of $N$, generate a scatter plot of the samples drawn.  On each plot, also include a contour of the original density $N(\\mu,\\Sigma)$."]},{"cell_type":"markdown","metadata":{"id":"QVH8s8kjqheC"},"source":["### 5.1 Implement the function that computes the density of a multi-variate Gaussian distribution (9 pt)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tPcckj7vyByO"},"outputs":[],"source":["def mvar(x, mu, Sigma):\n","  '''\n","    Computes the density function of a multi-variate Gaussian distribution\n","      with mean mu and covariance matrix Sigma, evaluated at x.\n","    This is the multi-variate version of nrmf.\n","    Different from nrmf, x encodes a single location only.\n","\n","    Inputs:\n","    - x: 1-D numpy array specifying the single position where the probability density is queried (num_dimension)\n","    - mu: 1-D numpy array specifying the mean of 2-dimensional normal density (num_dimension)\n","    - Sigma: 2-D numpy array specifying the covariance of 2-dimensional normal density (num_dimension, num_dimension)\n","    Outputs:\n","    - p: a real number specifying the probability density of x\n","  '''\n","\n","  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","\n","  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","  return p\n","\n","\"\"\"\n","Unit test:\n","should print: p = 0.011828135195080136\n","\"\"\"\n","Sigma = np.array([[1, -1],\n","           [-1, 5]])\n","\n","mu = np.array([1.5, 2])\n","x = np.array([3, 3])\n","p = mvar(x, mu, Sigma)\n","print(p)"]},{"cell_type":"markdown","metadata":{"id":"JzLn4axHu9MB"},"source":["### 5.2 Create the meshgrid to plot the contour for the density function (9 pt) {-}\n","\n","Given a range of $x$ and $y$ values (both as a vector), create a mesh grid as their cross product.  Most parts of the function have been done for you, and you will only need to implement one line of code that evaluates the Gaussian density using the `mvar` function.  Read the code carefully and digest the `meshgrid` function.  It will be very important for plotting 2-variable functions.\n","\n","You may look at the code in Section 5.4 below and see how to make `comp_density_grid` compatible with its application there.  As such, we are not providing a test case here, but you should make sure that the results in Section 5.4 look reasonable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WYRTMUuBR2fo"},"outputs":[],"source":["def comp_density_grid(x, y, mu, Sigma):\n","  '''\n","  Generate the meshgrid of plotting the 2-variable Gaussian density function\n","  Inputs:\n","    - x: 1-D numpy array specifying the ticks of x-axis\n","    - y: 1-D numpy array specifying the ticks of y-axis\n","    - mu: 1-D numpy array specifying the mean of 2-dimensional normal density (num_dimension)\n","    - Sigma: 2-D numpy array specifying the covariance of 2-dimensional normal density (num_dimension, num_dimension)\n","  Outputs:\n","    - X: shaped x by using np.meshgrid (len(y), len(x))\n","    - Y: shaped y by using np.meshgrid (len(y), len(x))\n","    - f: 2-D numpy array specifying the probability density of 2-variable Gaussian density (len(y), len(x))\n","  '''\n","  X, Y = np.meshgrid(x, y)  # Both X and Y are shaped (len(y), len(x))\n","  len_x = np.size(x)\n","  len_y = np.size(y)\n","  f = np.empty([len_y, len_x])\n","\n","  for i in range(len_x):\n","    for j in range(len_y):\n","      # Next we need to assign the values for the f matrix\n","      # It is quite tricky. Check out\n","      # Manual: https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html\n","      # Pay attention to the 'notes' section in the manual page.\n","      # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","\n","      # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","  return X, Y, f"]},{"cell_type":"markdown","metadata":{"id":"pEqPWgggu6eA"},"source":["### 5.3  Draw samples from a multivariate Gaussian and re-estimate its parameters (9 pt) {-}\n","\n","To draw a sample from an $n$-dimensional Gaussian $N(\\mu, \\Sigma)$, one needs to take the following steps:\n","\n","1. Perform a Cholesky decomposition on $\\Sigma$, i.e., find an $n$-by-$n$ upper-triangular matrix $R$ such that $R^T R = \\Sigma$.  Since $\\Sigma$ is positive semi-definite, there must be such a real-valued matrix $R$.\n","\n","2. Draw $n$ number of iid samples from $N(0,1)$ using `np.random.standard_normal`. Stack them together into an $n$-dimensional vecotr $x$.\n","\n","3. Output a sample computed by $R^\\top x + \\mu$.\n","\n","The function `sample_multivariate_normal` should first loop over steps 1-3 to draw $N$ samples, and then use these samples to estimate the mean and covariance matrix of the Gaussian.\n","\n","Now implement this procedure.  You code should not import any module and can directly call `scipy.linalg.cholesky`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5IZ_VmmWjLue"},"outputs":[],"source":["def sample_multivariate_normal(mu, Sigma, N):\n","  '''\n","  Draw samples from a multivariate Gaussian and then re-estimate its mean and covariance\n","  Inputs:\n","    - mu: 1-D numpy array specifying the mean of 2-dimensional normal density (num_dimension)\n","    - Sigma: 2-D numpy array specifying the covariance of 2-dimensional normal density (num_dimension, num_dimension)\n","    - N: Integer, the number of samples (num_sample)\n","    ...\n","  Outputs:\n","    - sample: 2-D numpy array of drawn samples from a multivariate Gaussian (num_sample, num_dimension)\n","    - mean: 1-D numpy array specifying the mean of drawn samples (num_dimension)\n","    - cov: 2-D numpy array specifying the covariance of drawn samples (num_dimension, num_dimension)\n","  '''\n","  np.random.seed(1)\n","  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","\n","\n","  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","  return sample, mean, cov\n","\n","  '''\n","  Unit test:\n","  It is slightly difficult to test the implementation because the result is random.\n","  Depending on your implementation, it is hard to specify a \"correct\" answer.\n","  So one way to check is to draw a large number of samples (e.g., N = 10000),\n","  then estimate the mean and covariance from it.\n","  Finally, compare them with the input arguments mu and Sigma.\n","  You can implement this test yourself, and the auto-grader will do exactly this.\n","  '''"]},{"cell_type":"markdown","metadata":{"id":"RJede6qaGCJC"},"source":["### 5.4  Generate contours of Gaussian densities and samples (5 pt){-}\n","\n","1. Generate contours of Gaussian densities,\n","2. Draw $N$ samples from it,\n","3. Estimate the mean and covariance based on the $N$ samples,\n","4. Generate the contour of the estimated Gaussian.\n","\n","We will vary $N = 10, 50, 500$.  The code has been done for you, and you should carefully and thoroughly comprehend it.  In particular, see how the `contour` function is used.  Your implementation in the previous steps should make the plots here reasonable.  There is no code to submit for this step.\n","\n","Discuss briefly what observations you can make from the plots.\n","\n","<font color='red'> My observation is : (type the result here)</font>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i29eEONB0n-O"},"outputs":[],"source":["def ex5_2(Nlist):\n","  np.random.seed(1)\n","  Sigma = np.array([[0.1, -0.1],\n","           [-0.1, 0.5]])\n","\n","  m = np.array([1.5, 2])\n","  x = np.linspace(0, 3, 40)\n","  y = np.linspace(-0.5, 4, 50)\n","\n","  for N in Nlist:\n","\n","    # Generate contours of Gaussian densities\n","    # This subplot is the same for different values of N.\n","    X, Y, f = comp_density_grid(x, y, m, Sigma)\n","    fig = plt.figure()\n","    fig.suptitle(f\"N = {N}\", va='bottom', fontsize=16)\n","    ax1 = fig.add_subplot(121)    # Digest the syntax here\n","    ax1.title.set_text('Population')\n","    ax1.contour(X, Y, f)\n","    ax1.set_xlabel('x')\n","    ax1.set_ylabel('y')\n","    ax1.set_ylim(-0.5,4.5)\n","\n","    ax2 = fig.add_subplot(122)\n","    ax2.title.set_text('Data and fitted Gaussian')\n","    # Draw N samples from the Gaussian,\n","    # and estimate the mean and covariance based on these samples.\n","    dta, mean, cov = sample_multivariate_normal(m, Sigma, N)\n","\n","    # Generate the contour of the estimated Gaussian.\n","    X, Y, f = comp_density_grid(x, y, mean, cov)\n","    ax2.contour(X, Y, f)\n","\n","    ax2.plot(dta[:,0], dta[:,1], 'x')\n","    ax2.set_xlabel('x')\n","    ax2.set_ylabel('y')\n","    ax2.set_ylim(-0.5,4.5)\n","    plt.tight_layout()\n","\n","N = [10, 50, 500]\n","ex5_2(N)"]},{"cell_type":"markdown","metadata":{"id":"ZSH9Nnh_zO2y"},"source":["## Problem 6: Exercise 5.6 of Alpaydin (0 pt) {-}\n","\n","Let us say in two dimensions, we have two classes with exactly the same mean. What type of boundaries can be defined?\n","\n","Digest the code below and appreciate the four cases.\n","We plot in black the decision boundary where $p_1 - p_2 = 0$.\n","We assume $p(C_1) = p(C_2) = 0.5$.  We generate the contour plot of the likelihood $p(x|C_1)$ and $p(x|C_2)$ at the level of 1, in red and blue respectively.  There is no code to submit for this question.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-hjonlY00X_9"},"outputs":[],"source":["def cal_likli_pos(x, y, m1, m2, sigma1, sigma2):\n","  '''\n","  Generate the meshgrid of plotting\n","      1) two Gaussian density functions of two variables (output f1, f2).\n","      2) the posterior probability of the two classes (p1, p2)\n","  Inputs:\n","    - x, y: 1-D numpy arrays to specify the x and y coordinates for plotting\n","    - m1, m2, sigma1, sigma2: the mean and covariance of the two Gaussians.\n","  '''\n","  X, Y = np.meshgrid(y, x)\n","  len_x = np.size(x)\n","  len_y = np.size(y)\n","\n","  f1 = np.zeros((len_y, len_x))\n","  f2 = np.zeros((len_y, len_x))\n","  p1 = np.zeros((len_y, len_x))\n","  p2 = np.zeros((len_y, len_x))\n","  for i in range(len_y):\n","    for j in range(len_x):\n","        f1[i][j] = mvar(np.array([x[j], y[i]]), m1, sigma1)\n","        f2[i][j] = mvar(np.array([x[j], y[i]]), m2, sigma2)\n","        p1[i][j] = f1[i][j]/(f1[i][j]+f2[i][j])\n","        p2[i][j] = 1 - p1[i][j]\n","\n","  return f1, f2, p1, p2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7h8q9ef7zJwp"},"outputs":[],"source":["def ex5_6(mean, sg1, sg2):\n","  if (sg1.shape != sg2.shape):\n","    return 0\n","  x = np.arange(0, 4, 0.1)\n","  y = np.arange(0, 4, 0.1)\n","  n = sg1.shape[0]\n","  nrow = np.rint(np.sqrt(n)).astype(int)\n","  ncol = np.rint(np.sqrt(n)).astype(int)\n","  for i in range(n):\n","    f1, f2, p1, p2 = cal_likli_pos(x, y, mean, mean, sg1[i], sg2[i]) # cal_likli_pos as implemented in Problem 6\n","    plt.subplot(nrow,ncol, i+1)\n","    plt.title('({0:2d})'.format(i+1))\n","    plt.contour(x, y, f1, 1, colors='r')\n","    plt.contour(x, y, f2, 1, colors='b')\n","    plt.contour(x, y, p1-p2, 0, colors='k')   # plot the boundary of the two classes\n","    plt.xticks(np.arange(0, 5, step=1))\n","    plt.yticks(np.arange(0, 5, step=1))\n","    plt.gca().set_aspect('equal')\n","\n","  plt.tight_layout()\n","\n","mean = np.array([2.0,2.0])\n","sg1 = np.array([[[0.1,0],[0,0.1]],[[0.1,0],[0,0.1]],[[0.1,0],[0,0.1]],[[0.5,-0.25],[-0.25,0.5]]])\n","sg2 = np.array([[[1,0],[0,1]], [[1,0],[0,0.1]], [[1,0.25],[0.25,0.1]],[[1,0.25],[0.25,0.1]]])\n","ex5_6(mean, sg1, sg2)"]},{"cell_type":"markdown","metadata":{"id":"-LWlt0cd8S4n"},"source":["## Problem 7: Based on Exercise 5.3 of Alpaydin  (15 pt){-}\n","\n","Generate samples from two multivariate normal densities $N(\\mu_i, \\Sigma_i)$ ($i = 1, 2$), and use these samples to calculate the Bayes’ optimal discriminant for the four cases in Table 5.1. For simplicity, assume $N(\\mu_i, \\Sigma_i)$ is a 2-dimensional Gaussian density for both $i = 1$ and $2$.\n","\n","We can consider $N(\\mu_i, \\Sigma_i)$ as the likelihood $p(x|C_i)$.  We will assume a uniform prior $p(C_1) = p(C_2) = 0.5$.  Our overall experiment is set up as follows.\n","\n","1. Generate $N = 5$ samples from the given mean $N(\\mu_i, \\Sigma_i)$.\n","\n","2. Estimate $\\mu_i$ from the samples and denote the estimates as $mean_i$.\n","\n","3. For $k$ = 4, 3, 2, 1 **(the construction of covariance estimate in the four cases will be covered by Section 7.1)**\n","  \n","     * If $k$ == 4, estimate the covariances $\\Sigma_i$ as \"Different, Hyperellipsoidal\".  Denote the result as $cov_i$.  Refer to the last row of Table 5.1 of Alpaydin.\n","\n","     * If $k$ == 3, estimate the covariances $\\Sigma_i$ as \"Shared, Hyperellipsoidal\".  This shared covariance matrix can be computed as $(cov_1 + cov_2)/2$ based on the result of $k$ == 4.\n","\n","     * If $k$ == 2, estimate the covariances $\\Sigma_i$ as \"Shared, Axis-aligned\".  Denote the shared covariance matrix from $k=3$ as $S$. Then the new shared covariance matrix is just a diagonal matrix whose $(i,i)$-th entry is $S_{ii}$.  In other words, just set the off-diagonal entries of $S$ to 0.\n","\n","     * If $k$ == 1, estimate the covariances $\\Sigma_i$ as \"Shared, Hyperspheric\".  Denote the shared covariance matrix from $k=2$ as $S$. Then the new shared covariance matrix is just $a \\cdot I$, where $I$ is the identity matrix and $a$ is the mean of the diagonal entries of $S$.\n","\n","4. For $k$ = 4, 3, 2, 1\n","\n","     * Plot the contour of the density of the two Gaussians with the estimated mean ($mean_i$) and covariance ($cov_i$ for $k = 4$, etc).\n","\n","     * Plot the optimal discriminant (i.e., decision boundary) for the two classes, i.e., where $p(C_1 | x) = p(C_2 | x) = 0.5$.  You can use the same technique or code from Problem 6."]},{"cell_type":"markdown","metadata":{"id":"6rCeCXeC1hGM"},"source":["### 7.1 Compute the covariance estimation for the four different assumptions (10 pt){-}\n","\n","We first implement the covariance matrix for the four cases in the function `comp_CovList`.  It takes $cov_i$ from the standard estimation of the two Gaussians assuming they are different and hyperellipsoidal (see the case $k=4$ above); you can see how they are computed from the code in Section 7.2.  The output is two lists: `cov1List` and `cov2List`.  `cov1List` is a list of four entries, where `cov1List[k-1]` is the estimate of $\\Sigma_1$ in the above case $k$.  Similarly for `cov2List`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QECo-CKwD3Gp"},"outputs":[],"source":["def comp_CovList(cov1, cov2):\n","  '''\n","  Implement the covariance matrix for the four cases\n","  Inputs:\n","  - cov1: the estimate of covariance matrix for the first Gaussian based on samples from it\n","  - cov2: the estimate of covariance matrix for the second Gaussian based on samples from it\n","         Note cov1 and cov2 are estimated separately, just like in the standard setting.\n","  Outputs:\n","    - cov1List: a list of four entries, where cov1List[k-1] is the estimate of Sigma_1 in the case k\n","    - cov2List: a list of four entries, where cov2List[k-1] is the estimate of Sigma_2 in the case k\n","  '''\n","  cov1List = list()\n","  cov2List = list()\n","\n","  for k in range(4):\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","  return cov1List, cov2List"]},{"cell_type":"markdown","metadata":{"id":"OeVde4Y5HnYM"},"source":["### 7.2 Use `compCovList` to plot Gaussian likelihood and optimal discriminant. (5 pt){-}\n","\n","The code has been provided. You should read and digest the code.  There is no code to submit.\n","\n","Now consider which one of the four assumptions in Table 5.1 of Alpaydin is the best for our samples from the two Gaussians.  If it is not \"Different, Hyperellipsoidal\" ($k=4$), explain why we cannot benefit from such a high capacity model.  If it is not \"Shared, Hyperspheric\" ($k=1$), explain why it does not work well.\n","\n","<font color='red'> My observation is: (type the result here)</font>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rrLSkl-uAhg4"},"outputs":[],"source":["def ex5_3():\n","  np.random.seed(1)\n","\n","  mu1 = np.array([1, 2])\n","  sigma1 = np.array([[0.1, -0.1],\n","          [-0.1, 0.5]])\n","  mu2 = np.array([2.5, 1])\n","  sigma2 = sigma1\n","\n","  min_x, max_x = (0, 4)\n","  min_y, max_y = (-2, 4)\n","\n","  #xrange and yrange are the range for plotting\n","  xrange = np.arange(min_x, max_x, step=1)\n","  yrange = np.arange(min_y, max_y, step=1)\n","  x = np.arange(min_x, max_x, 0.05)\n","  y = np.arange(min_y, max_y, 0.05)\n","\n","  f1, f2, p1, p2 = cal_likli_pos(x, y, mu1, mu2, sigma1, sigma2)\n","  plt.figure(figsize=(7,7))\n","  plt.subplot(3,2,1)\n","  plt.contour(x, y, f1)\n","  plt.contour(x, y, f2)\n","  plt.xlabel('x')\n","  plt.ylabel('y')\n","  plt.title('Likelihoods and Posteriors')\n","  plt.contour(x, y, p1-p2, levels=[0])\n","  plt.gca().set_aspect('equal')\n","\n","  #Sample from populations\n","  N = 5\n","  dta1, mean1, cov1 = sample_multivariate_normal(mu1, sigma1, N)\n","  dta2, mean2, cov2 = sample_multivariate_normal(mu2, sigma2, N)\n","\n","  cov1List, cov2List = comp_CovList(cov1, cov2)\n","\n","  for k in range(4):\n","    if k == 3:\n","      plt.subplot(3,2,6)\n","      title = 'Arbitrary cov'\n","    elif k == 2:\n","      plt.subplot(3,2,5)\n","      title = 'Shared cov'\n","    elif k == 1:\n","      plt.subplot(3,2,4)\n","      title = 'Diag cov'\n","    else:\n","      plt.subplot(3,2,3)\n","      title = 'Equal Diag cov'\n","\n","    plt.xlabel('x')\n","    plt.ylabel('y')\n","\n","    # Plot the samples dta1 using 'x'\n","    plt.plot(dta1[:,0], dta1[:,1], 'x')\n","    # Plot the samples dta1 using '+r'\n","    plt.plot(dta2[:,0], dta2[:,1], '+r')\n","\n","    f1, f2, p1, p2 = cal_likli_pos(x, y, mean1, mean2, cov1List[k], cov2List[k])\n","    # Plot the contour of f1 and f2\n","    plt.contour(x, y, f1)\n","    plt.contour(x, y, f2)\n","    # Plot the decision boundary\n","    plt.contour(x, y, p1-p2, levels=[0])\n","\n","    plt.gca().set_aspect('equal')\n","    plt.title(title)\n","\n","  plt.tight_layout()\n","\n","ex5_3()\n"]},{"cell_type":"markdown","metadata":{"id":"EzjBtICTlkRQ"},"source":["# Submission Instruction {-}\n","\n","You're almost done! Take the following steps to finally submit your work.\n","\n","1. After executing all commands and completing this notebook, save your `Lab_4.ipynb` as a PDF file, named as `X_Y_UIN.pdf`, where `X` is your first name, `Y` is your last name, and `UIN` is your UIN. Make sure that your PDF file includes all parts of your solution, including the plots.\n","\n","> * Print out all unit test case results before printing the notebook into a PDF.\n","* If you use Colab, open this notebook in Chrome.  Then File -> Print -> set Destination to \"Save as PDF\".  If the web page freezes when printing, close Chrome and reopen the page. If Chrome doesn't work, try Firefox.\n","* If you are working on your own computer, we recommend using the browser (not jupyter) for saving the PDF. For Chrome on a Mac, this is under *File->Print...->Open PDF in Preview*. When the PDF opens in Preview, you can use *Save...* to save it.\n","* Sometimes, a figure that appears near the end of a page can get cut.  In this case, try to add some new lines in the preceding code block so that the figure is pushed to the beginning of the next page. Or insert some text blocks.\n","\n","2. Upload `X_Y_UIN.pdf` to Gradescope under `Lab_4_Written`.\n","\n","3. A template of `Lab_4.py` has been provided.  For all functions in `Lab_4.py`, copy the corresponding code snippets you have written into it, excluding the plot code.  **Do NOT** copy any code of plotting figures and do not import **matplotlib**.  This is because the auto-grader cannot work with plotting.  **Do NOT** change the function names.  \n","\n","4. Zip `Lab_4.py` and `Lab_4.ipynb` (**2 files**) into a zip file named `X_Y_UIN.zip`. Suppose the two files are in the folder `Lab_4`.  Then zip up the **two files inside the `Lab_4` folder**.  **Do NOT zip up the folder `Lab_4`** because the auto-grader cannot search inside a folder. Submit this zip file to Gradescope under `Lab_4_Code`.\n","\n","5. The autograder on Gradscope will be open all the time. We designed some simple test cases to help you check wehther your functions are executable. You will see the results of running autograder once you submit your code. Please follow the error messages to debug. Since those simple test cases are designed for debugging, it does not guaranttee your solution will work well on the real dataset. It is your responsibility to make your code logically correct. Since all functions are tested in batch, the autograder might take a few minutes to run after submission.\n","\n","<font color='red'>If you *only* try to get real-time feedback from auto-grader, it will be fine to just upload `Lab_4.py` to `Lab_4_Code`</font>.  However, the final submission for grading should still follow the above point 4.\n","\n","You can submit to Gradescope as often as you like. We will only consider your last submission before the deadline."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}